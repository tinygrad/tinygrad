logdir: null
traindir: null
evaldir: null
offline_traindir: ''
offline_evaldir: ''
seed: 0
deterministic_run: False
steps: 400000
parallel: False
eval_every: 1000
eval_episode_num: 100
log_every: 1000
reset_every: 0
device: 'cuda:0'
video_pred_log: True
expl_until: 0

# Environment
env: 'ALE/Pong-v5'
size: [64, 64]
num_envs: 4
action_repeat: 4
time_limit: 108000
grayscale: False
prefill: 2500
reward_EMA: True

# Model
dyn_hidden: 512
dyn_deter: 512
dyn_stoch: 32
dyn_discrete: 32
dyn_rec_depth: 1
dyn_mean_act: 'none'
dyn_std_act: 'sigmoid'
dyn_min_std: 0.1
grad_heads: ['decoder', 'reward', 'cont']
units: 512
act: 'silu'
norm: True
encoder:
  {mlp_keys: '$^', cnn_keys: 'image', act: 'silu', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 2, mlp_units: 512, symlog_inputs: True}
decoder:
  {mlp_keys: '$^', cnn_keys: 'image', act: 'silu', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 2, mlp_units: 512, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}
actor:
  {layers: 2, dist: 'onehot', entropy: 0.0003, unimix_ratio: 0.01, min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
critic:
  {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
reward_head:
  {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 0.0}
cont_head:
  {layers: 2, loss_scale: 1.0, outscale: 1.0}
dyn_scale: 0.5
rep_scale: 0.1
kl_free: 1.0
weight_decay: 0.0
unimix_ratio: 0.01
initial: 'learned'

# Training
batch_size: 16
batch_length: 64
train_ratio: 1024
pretrain: 100
model_lr: 1e-4
opt_eps: 1e-8
grad_clip: 1000
dataset_size: 1000000
opt: 'adam'

# Behavior.
discount: 0.997
discount_lambda: 0.95
imag_horizon: 15
imag_gradient: 'reinforce'
imag_gradient_mix: 0.0