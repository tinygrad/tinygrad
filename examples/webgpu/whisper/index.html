<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>whisper tinygrad WebGPU</title>
    <script>
        // Promise the external script can await
        window.moduleReady = new Promise(resolve => window._resolveModule = resolve);
    </script>
    <script type="module">
        import mel from "./mel.js"
        window.mel = mel;
        import encoder from "./encoder.js"
        window.encoder = encoder;
        import decoder from "./decoder.js"
        window.decoder = decoder;
        window._resolveModule();
    </script>
    <style>
        body {
            background-color: #607d8b;
            text-align: center;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            overflow: hidden;
        }

        h1 {
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <h2>whisper tinygrad WebGPU</h2>
    <h2 id="wgpu-error" style="display: none; color: red;">Error: WebGPU is not supported in this browser</h2>
    <h2 id="output"></h2>

    <script>
        let net_encoder = null;
        const wgpuError = document.getElementById('wgpu-error');
        const SAMPLES_PER_SEGMENT = 480000;

        async function processFrame() {
            if (video.videoWidth == 0 || video.videoHeight == 0) {
                requestAnimationFrame(processFrame);
                return;
            }
            // const boxes = await detectObjectsOnFrame(offscreenContext);
        }
        // requestAnimationFrame(processFrame);

        async function fetchMonoFloat32Array(url) {
            const response = await fetch(url);
            const arrayBuffer = await response.arrayBuffer();

            const audioCtx = new window.AudioContext({ sampleRate: 16000 });
            const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);

            // Downmix to mono by averaging channels
            const channels = audioBuffer.numberOfChannels;
            const length = audioBuffer.length;
            const mono = new Float32Array(length);

            for (let c = 0; c < channels; c++) {
                const data = audioBuffer.getChannelData(c);
                for (let i = 0; i < length; i++) mono[i] += data[i] / channels;
            }

            return {
                sampleRate: audioBuffer.sampleRate,
                samples: mono
            };
        }

        async function transcribeAudio() {
            if (!net_encoder) {
                await window.moduleReady;

                let device = await getDevice();
                if (!device) {
                    wgpuError.style.display = "block";
                    // loadingContainer.style.display = "none";
                }
                net_mel = await mel.load(device, "./mel.safetensors");
                net_encoder = await encoder.load(device, "./encoder.safetensors");
                net_decoder = await decoder.load(device, "./decoder.safetensors");
                // loadingContainer.style.display = "none";
            }

            // let log_spec = await fetch('log_spec.bin')
            //     .then(res => res.arrayBuffer())
            //     .then(buf => {
            //         const arr = new Float32Array(buf);
            //         return arr;
            //     });

            let loaded = await fetchMonoFloat32Array('./test2.wav');
            let samples = loaded.samples;
            if (samples.length > SAMPLES_PER_SEGMENT) {
                samples = samples.slice(0, SAMPLES_PER_SEGMENT);
            } else if (samples.length < SAMPLES_PER_SEGMENT) {
                let padded = new Float32Array(SAMPLES_PER_SEGMENT);
                for (var i = 0; i < samples.length; ++i) {
                    padded[i] = samples[i];
                }
                samples = padded;
            }

            let log_spec = await net_mel(samples);

            // const input = await prepareInput(offscreenContext);
            const output = await net_encoder(log_spec[0]);
            let context = [50257, 50362];

            // Load JSON mapping
            const mapping = await fetch('./vocab.json').then(res => res.json());;

            // Input list of indices
            // const indices = [2, 0, 1];

            // Translate and join
            // const result = indices.map(i => mapping[i]).join('');
            // console.log(result); // "fox cat dog"


            async function doOneToken(context, output, i) {
                if (i < 384*2)
                {
                    var decoded_0 = await net_decoder(context, output[0], [i]);
                    context.push(decoded_0[0]);
                    // document.getElementById("output").innerText = context.toString();
                    document.getElementById("output").innerText = context.map(j => mapping[j]).join('');
                    if (decoded_0[0] == 50256) {

                    } else {
                        // requestAnimationFrame(lambda => doOneToken(context, output, i+1));
                        setTimeout(async function() {await doOneToken(context, output, i+1);});
                    }
                }
            }

            setTimeout(async function() {await doOneToken(context, output, 2);});
            // for (var i = 2; i < 384*2; i += 2)
            // {
            //     var decoded_0 = await net_decoder(context, output[0], [i]);
            //     context.push(decoded_0[0]);
            //     document.getElementById("output").innerText = context.toString();
            //     if (decoded_0[0] === 50256) {
            //         break;
            //     }
            // }
            // return context;
        }


        function downloadArrayAsTxt(arr, filename) {
            let text = [...arr].map(x => x.toFixed(6)).join('\n');
            let blob = new Blob([text], { type: 'text/plain' });

            let a = document.createElement('a');
            a.href = URL.createObjectURL(blob);
            a.download = filename;
            a.click();
        }

        const getDevice = async () => {
            if (!navigator.gpu) return false;
            const adapter = await navigator.gpu.requestAdapter();
            return await adapter.requestDevice({
                requiredFeatures: ["shader-f16"],
                powerPreference: "high-performance"
		    });
        };

        const final = transcribeAudio();
        // final.then((response) => alert(response));
        //final.then((response) => downloadArrayAsTxt(response, "whisper_encoder_output.txt"));
    </script>
</body>
</html>
