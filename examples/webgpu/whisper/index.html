<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>whisper tinygrad WebGPU</title>
    <script>
        // Promise the external script can await
        window.moduleReady = new Promise(resolve => window._resolveModule = resolve);
    </script>
    <script type="module">
        import mel from "./mel.js"
        window.mel = mel;
        import encoder from "./encoder.js"
        window.encoder = encoder;
        import decoder from "./decoder.js"
        window.decoder = decoder;
        window._resolveModule();
    </script>
    <style>
        body {
            background-color: #607d8b;
            text-align: center;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            overflow: hidden;
        }

        .loader {
            width: 48px;
            height: 48px;
            border: 5px solid #FFF;
            border-bottom-color: transparent;
            border-radius: 50%;
            display: inline-block;
            box-sizing: border-box;
            animation: rotation 1s linear infinite;
        }

        @keyframes rotation {
            0% {
                transform: rotate(0deg);
            }

            100% {
                transform: rotate(360deg);
            }
        }

        .loading-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.6);
            z-index: 10;
        }

        .loading-text {
            font-size: 24px;
            color: white;
            margin-bottom: 20px;
        }

        h1 {
            margin-top: 20px;
        }
    </style>
</head>

<body>
    <h2>whisper tinygrad WebGPU</h2>
    <h2 id="wgpu-error" style="display: none; color: red;">Error: WebGPU is not supported in this browser</h2>
    <div id="output"></div>
    <div id="div-loading" class="loading-container">
        <p class="loading-text">Loading model</p>
        <span class="loader"></span>
    </div>

    <script>
        let net_encoder = null;
        const wgpuError = document.getElementById('wgpu-error');
        const loadingContainer = document.getElementById('div-loading');

        const SAMPLES_PER_SEGMENT = 480000;
        const BASE_URL = '.'; // for LFS-stored weights
        const AUDIO_PATH = 'test2.wav';

        async function fetchMonoFloat32Array(url) {
            const response = await fetch(url);
            const arrayBuffer = await response.arrayBuffer();

            const audioCtx = new window.AudioContext({ sampleRate: 16000 });
            const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);

            // Downmix to mono by averaging channels
            const channels = audioBuffer.numberOfChannels;
            const length = audioBuffer.length;
            const mono = new Float32Array(length);

            for (let c = 0; c < channels; c++) {
                const data = audioBuffer.getChannelData(c);
                for (let i = 0; i < length; i++) mono[i] += data[i] / channels;
            }

            return {
                sampleRate: audioBuffer.sampleRate,
                samples: mono
            };
        }

        // NOTE(irwin): copied from stable diffusion example
        const getProgressDlForPart = async (part, progressCallback) => {
            const response = await fetch(part);
            const contentLength = response.headers.get('content-length');
            const total = parseInt(contentLength, 10);

            const res = new Response(new ReadableStream({
                async start(controller) {
                    const reader = response.body.getReader();
                    for (;;) {
                        const { done, value } = await reader.read();
                        if (done) break;
                        progressCallback(part, value.byteLength, total);
                        controller.enqueue(value);
                    }

                    controller.close();
                },
            }));

            return res.arrayBuffer();
        };

        let totalLoaded = 0;
        let totalSize = 0;
        let partSize = {};
        const loadingText = document.querySelector("#div-loading > p");
        const progressCallback = (part, loaded, total) => {
            totalLoaded += loaded;

            if (!partSize[part]) {
                totalSize += total;
                partSize[part] = true;
            }

            loadingText.innerHTML = `Loading model ${part.split('/').at(-1).split('.')[0]} ${Math.trunc((totalLoaded/totalSize) * 100)}%`;
        };

        async function transcribeAudio() {
            if (!net_encoder) {
                await window.moduleReady;

                let device = await getDevice();
                if (!device) {
                    wgpuError.style.display = "block";
                    loadingContainer.style.display = "none";
                }
                loadingText.innerHTML = 'Loading model mel';
                net_mel = await mel.setupNet(device, new Uint8Array(await getProgressDlForPart(`${BASE_URL}/mel.safetensors`, progressCallback)));
                loadingText.innerHTML = 'Loading model encoder';
                net_encoder = await encoder.setupNet(device, new Uint8Array(await getProgressDlForPart(`${BASE_URL}/encoder.safetensors`, progressCallback)));
                loadingText.innerHTML = 'Loading model decoder';
                net_decoder = await decoder.setupNet(device, new Uint8Array(await getProgressDlForPart(`${BASE_URL}/decoder.safetensors`, progressCallback)));
                loadingContainer.style.display = "none";
            }

            let loaded = await fetchMonoFloat32Array(`./${AUDIO_PATH}`);
            let samples = loaded.samples;

            let log_specs = [];
            for (var i = 0; i < samples.length; i+=SAMPLES_PER_SEGMENT) {
                let chunk = samples.slice(i, i+SAMPLES_PER_SEGMENT);
                if (chunk.length < SAMPLES_PER_SEGMENT) {
                    let padded = new Float32Array(SAMPLES_PER_SEGMENT);
                    for (var i = 0; i < chunk.length; ++i) {
                        padded[i] = chunk[i];
                    }
                    chunk = padded;
                }
                let log_spec = await net_mel(chunk);
                log_specs.push(log_spec[0]);
            }
            console.log(log_specs.length);

            const mapping = await fetch('./vocab.json').then(res => res.json());

            let pendingText = null;
            let lastDisplayed = '';
            let lastUpdateTime = 0;
            let inferenceDone = false;

            const ELEMENT_UPDATE_FPS = 30;
            const MAX_TOKENS_TO_DECODE = 384;
            function updateLoop(now) {
                if (pendingText !== null && pendingText !== lastDisplayed) {
                    if (now - lastUpdateTime >= 1000.0 / ELEMENT_UPDATE_FPS) {
                        document.getElementById("output").innerText = pendingText;
                        lastDisplayed = pendingText;
                        lastUpdateTime = now;
                        if (inferenceDone) return;
                    }
                }
                requestAnimationFrame(updateLoop);
            }
            requestAnimationFrame(updateLoop);

            for (var index = 0; index < log_specs.length; ++index) {
                const output = await net_encoder(log_specs[index]);

                const TOK_EOS = 50256;
                const TOK_BEGIN_TRANSCRIPTION = 50257;
                const TOK_TRANSCRIBE = 50358;
                const TOK_NO_TIMESTAMPS = 50362;
                let context = [TOK_BEGIN_TRANSCRIPTION, TOK_NO_TIMESTAMPS];

                const offset = context.length;
                if (offset >= MAX_TOKENS_TO_DECODE) {
                    console.error("Context length exceeds 384");
                    inferenceDone = true;
                    return;
                }
                for(let i = offset; i < (offset+MAX_TOKENS_TO_DECODE); ++i) {
                    if (i < MAX_TOKENS_TO_DECODE * 2) {
                        var decoded_0 = await net_decoder(context, output[0], [i]);
                        context.push(decoded_0[0]);
                        pendingText = context.map(j => mapping[j]).join('');
                        if (decoded_0[0] == TOK_EOS) {
                            break;
                        }
                    } else {
                        break;
                    }
                }
                const out = document.getElementById("output");
                var dupe = out.cloneNode(true);
                dupe.id += index;
                dupe.innerText = pendingText;
                pendingText = '';
                out.insertAdjacentElement('beforebegin', dupe);
            }
            inferenceDone = true;
        }

        const getDevice = async () => {
            if (!navigator.gpu) return false;
            const adapter = await navigator.gpu.requestAdapter();
            return await adapter.requestDevice({
                requiredFeatures: ["shader-f16"],
                powerPreference: "high-performance"
            });
        };

        const final = transcribeAudio();
    </script>
</body>

</html>