<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>whisper tinygrad WebGPU</title>
    <script>
        window.moduleReady = new Promise(resolve => window._resolveModule = resolve);
    </script>
    <script type="module">
        const modules = [
            ["mel", "./mel.js"],
            ["encoder", "./encoder.js"],
            ["decoder", "./decoder.js"],
        ];
        window.modules = modules;

        const nets = {};
        for (const [name, path] of modules) {
            const mod = await import(path);
            window[name] = mod.default;
        }
        window.nets = nets;
        window._resolveModule();
    </script>
    <style>
        body {
            background-color: #333;
            color: #f0f0f0;
            font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
            margin: 0;
            padding: 2rem;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        .container { max-width: 800px; width: 100%; }
        h2 { text-align: center; color: #fff; margin-bottom: 2rem; }
        #wgpu-error { color: #ff6b6b; }

        #transcription-container {
            background-color: #444;
            border-radius: 8px;
            padding: 1.5rem;
            margin-top: 1rem;
            height: 60vh;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
        }
        #transcription-log { flex-grow: 1; }
        .transcription-chunk { padding: 0.75rem 0; border-bottom: 1px solid #555; }
        .transcription-chunk:empty { display: none; }
        #current-transcription {
            color: #fff;
            /* min-height: 1.5em; */
            padding: 0.75rem 0;
        }
        #current-transcription::after {
            content: 'â–‹';
            animation: blink 1s step-end infinite;
        }
        @keyframes blink { 50% { opacity: 0; } }

        .loading-container {
            display: flex; flex-direction: column; align-items: center; justify-content: center;
            position: fixed; top: 0; left: 0; width: 100%; height: 100%;
            background-color: rgba(0, 0, 0, 0.8); z-index: 10;
        }
        .loader {
            width: 48px; height: 48px; border: 5px solid #FFF; border-bottom-color: #888;
            border-radius: 50%; display: inline-block; animation: rotation 1s linear infinite;
        }
        @keyframes rotation { from { transform: rotate(0deg); } to { transform: rotate(360deg); } }
        .loading-text { font-size: 1.5rem; color: white; margin-bottom: 1.5rem; }

        #overlay {
            position: fixed;
            top: 0; left: 0; right: 0; bottom: 0;
            background: rgba(0, 0, 0, 0.5);
            color: white;
            font-size: 2em;
            display: none;
            justify-content: center;
            align-items: center;
            z-index: 1000;
        }
    </style>
</head>

<body>
    <div class="container">
        <h2 id="whisper-title">Whisper Tinygrad WebGPU (tiny.en)</h2>
        <h2 id="wgpu-error" style="display: none;">Error: WebGPU is not supported in this browser. See <a href="https://developer.chrome.com/docs/web-platform/webgpu/troubleshooting-tips">more</a>.</h2>

        <form id="fetchForm">
            <input size="100%" list="testFiles" id="urlInput" type="text" placeholder="Put media url here...">
            <button type="submit">Fetch</button>
        </form>
        <datalist id="testFiles">
            <option value="https://raw.githubusercontent.com/tinygrad/tinygrad/master/test/models/whisper/test.wav">test.wav</option>
            <option value="https://raw.githubusercontent.com/tinygrad/tinygrad/master/test/models/whisper/test2.wav">test2.wav</option>
            <option value="https://raw.githubusercontent.com/openai/whisper/master/tests/jfk.flac">jfk.flac</option>
        </datalist>

        <div id="progressbar">Transcription status: idle</div>
        <div id="transcription-container">
            <div id="transcription-log"></div>
            <div id="current-transcription"></div>
        </div>
        <div id="drop-zone">Drop files here</div>
        <div id="overlay">Drop your file</div>
    </div>
    <div id="div-loading" class="loading-container">
        <p class="loading-text">Loading model</p>
        <span class="loader"></span>
    </div>

    <script type="module">
        import {
                tensorStore,
                initDb,
                getDevice,

                fetchMonoFloat32Array,
                fetchMonoFloat32ArrayFile,
                getProgressDlForPart,

                transcribeAudio
            } from "./whisper.js";

        function setModelName(model_name) {
            const h = id => document.getElementById(id);
            h('whisper-title').textContent = `Whisper Tinygrad WebGPU (${model_name})`;
        }
        const model_metadata = await fetch('./model_metadata.json', {cache: 'no-cache'}).then(x => x.json());
        setModelName(model_metadata.model_name);

        await window.moduleReady;
        nets.model_metadata = model_metadata;

        let pendingText = null, lastDisplayed = '', lastUpdateTime = 0, inferenceDone = false;
        let pendingTexts = [];
        const updateLoop = (now) => {
            if (pendingText !== null /*&& pendingText !== lastDisplayed*/ && now - lastUpdateTime >= 1000.0 / 30) {
                let updateScroll = Math.ceil(transcriptionContainer.scrollTop + transcriptionContainer.clientHeight) >= transcriptionContainer.scrollHeight;

                let current = Array.from(document.querySelectorAll("div#current-transcription"));
                for (let i = 0; i < pendingTexts.length; ++i) {
                    current[i].innerText = pendingTexts[i];
                    // currentTranscription.innerText = pendingText;
                }
                lastDisplayed = pendingText;
                lastUpdateTime = now;
                // let ignored = current.at(-1).offsetHeight;
                // console.log(ignored);
                // if (updateScroll) {
                //     transcriptionContainer.scrollTop = transcriptionContainer.scrollHeight;
                // }
                if (updateScroll) {
                    // transcriptionContainer.scrollTop = transcriptionContainer.scrollHeight;
                    transcriptionContainer.scrollTo({ top: transcriptionContainer.scrollHeight, behavior: "instant" });
                }
            }
            if (!inferenceDone) requestAnimationFrame(updateLoop);
        };

        let chunkCount = 0;
        let chunksRemaining = 0;

        const STATUS_IDLE = 0;
        const STATUS_AUDIO_DECODING = 1;
        const STATUS_AUDIO_ENCODING = 2;
        const STATUS_DECODING = 3;

        let transcriptionStatus = STATUS_IDLE;

        function updateProgress(data) {
            let progress = document.getElementById("progressbar");
            let chunksDecoded = chunkCount - chunksRemaining;
            let currentToken = 0;
            if (data) {
                chunksDecoded += data.sequenceStatus.filter(x => x == "done").length;
                currentToken = data.currentTokenIndex;
            }
            let percent = (chunksDecoded / Math.max(chunkCount, 1.0)) * 100;
            let text = `chunks decoded: ${chunksDecoded} / ${chunkCount} (${percent.toFixed(0).padStart(3, ' ')}%), current token: ${currentToken} / 224`;
            if (transcriptionStatus === STATUS_AUDIO_ENCODING) {
                text += " encoding audio features...";
            } else if (transcriptionStatus === STATUS_AUDIO_DECODING) {
                text += " fetching and decoding audio...";
            }
            progress.innerText = text;
        }

        function onTranscriptionEvent(event, data) {
            if (event === "cancel") {
                currentTranscription.style.display = 'none';
                inferenceDone = true;
                transcriptionStatus = STATUS_IDLE;
            } else if (event === "inferenceDone") {
                inferenceDone = true;
                currentTranscription.style.display = 'none';
                transcriptionStatus = STATUS_IDLE;
            } else if (event === "inferenceBegin") {
                chunkCount = data.chunkCount;
                chunksRemaining = chunkCount;
                inferenceDone = false;
                pendingText = '';
                currentTranscription.style.display = 'block';
                requestAnimationFrame(updateLoop);
                updateProgress();
            } else if (event === "audioDecode") {
                transcriptionStatus = STATUS_AUDIO_DECODING;
                updateProgress();
            } else if (event === "audioEncode") {
                transcriptionStatus = STATUS_AUDIO_ENCODING;
                updateProgress();
            } else if (event === "chunkUpdate") {
                transcriptionStatus = STATUS_DECODING;
                pendingTexts = data.pendingTexts;
                pendingText = data.pendingTexts[0];

                let height = transcriptionContainer.scrollHeight;
                let current = Array.from(document.querySelectorAll("div#current-transcription"));
                for (let i = 0; i < current.length && i < pendingTexts.length; ++i) {
                    current[i].style.display = 'block';
                }
                for (let i = 0; i < pendingTexts.length-current.length; ++i) {
                    let extraTranscription = document.createElement('div');
                    extraTranscription.id = "current-transcription";
                    extraTranscription.style.display = 'block';
                    transcriptionContainer.appendChild(extraTranscription);
                }
                if (transcriptionContainer.scrollHeight > height) {
                    transcriptionContainer.scrollTo({ top: transcriptionContainer.scrollHeight, behavior: "instant" });
                }
                updateProgress(data);
            } else if (event === "chunkDone") {
                let current = Array.from(document.querySelectorAll("div#current-transcription"));
                current[data.index].style.display = 'none';
                const newChunk = document.createElement('div');
                newChunk.className = 'transcription-chunk';
                newChunk.innerText = data.pendingText;
                transcriptionLog.appendChild(newChunk);
                currentTranscription.innerText = '';
                pendingText = '';
                --chunksRemaining;
            }
        }

        document.getElementById("fetchForm").addEventListener("submit", async (e) => {
            // prevent page refresh
            e.preventDefault();
            const url = document.getElementById("urlInput").value;
            try {
                currentCancel = { cancelled: false };
                await transcribeAudio(nets, async () => await fetchMonoFloat32Array(url, OfflineAudioContext), currentCancel, onTranscriptionEvent, loadAndInitializeModels);
            } catch (err) {
                console.error("Fetch failed:", err);
            }
        });

        const wgpuError = document.getElementById('wgpu-error');
        const loadingContainer = document.getElementById('div-loading');
        const transcriptionLog = document.getElementById('transcription-log');
        const currentTranscription = document.getElementById('current-transcription');
        const transcriptionContainer = document.getElementById('transcription-container');


        const BASE_URL = '.';
        // const AUDIO_PATH = 'RED_16k.wav';
        // const AUDIO_PATH = 'RED_60s.wav';
        const AUDIO_PATH = 'test.wav';
        // const AUDIO_PATH = 'test2.wav';
        // const AUDIO_PATH = 'TINYCORP_MEETING_2025-07-07-DSWQCT9mypQ.mp3';
        // const AUDIO_PATH = `${BASE_URL}/TINYCORP_MEETING_2025-08-25-KA0h9zmJtcs.mp3`;

        var db;

        let totalLoaded = 0, totalSize = 0, partSize = {};
        const loadingText = document.querySelector("#div-loading > p");
        const progressCallback = (part, loaded, total) => {
            totalLoaded += loaded;
            if (!partSize[part]) { totalSize += total; partSize[part] = true; }
            const name = part.split('/').pop().split('.')[0];
            const percent = Math.trunc((totalLoaded/totalSize) * 100);
            loadingText.textContent = `Loading model ${name} ${percent}%`;
        };

        const getPart = async(key) => {
            let full_url = `${BASE_URL}/${key}.safetensors`;
            const cached = await tensorStore(db).get(key);

            const download = await getProgressDlForPart(full_url, progressCallback, cached?.lastModified);

            if (download === null && cached) {
                console.log(`Cache hit: ${key}`);
                return cached.content;
            }
            {
                console.log(`Cache refresh: ${key}`);
                await tensorStore(db).put(key, download.buffer, download.lastModified);
                return download.buffer;
            }
        }

        async function loadAndInitializeModels() {
            await window.moduleReady;
            if (!nets.encoder) {
                const device = await getDevice(navigator.gpu);
                if (!device) {
                    wgpuError.style.display = "block";
                    loadingContainer.style.display = "none";
                    return;
                }
                if (!db) {
                    db = await initDb();
                }

                for (const [name, path] of modules) {
                    nets[name] = await window[name].setupNet(device, new Uint8Array(await getPart(name)));
                }

                loadingContainer.style.display = "none";
            }
        }

        const mapping = await fetch('./vocab.json').then(res => res.json());
        nets.mapping = mapping;

        let currentCancel = null;
        const overlay = document.getElementById('overlay');

        let counter = 0;

        document.addEventListener('dragenter', e => {
            e.preventDefault();
            counter++;
            overlay.style.display = 'flex';
        });

        document.addEventListener('dragleave', e => {
            e.preventDefault();
            counter--;
            if (counter === 0) overlay.style.display = 'none';
        });

        document.addEventListener('dragover', e => {
            console.log("File(s) in drop zone");

            // Prevent default behavior (Prevent file from being opened)
            e.preventDefault(); // Needed to allow drop
        });

        document.addEventListener('drop', e => {
            e.preventDefault();
            counter = 0;
            overlay.style.display = 'none';

            console.log("File(s) dropped");

            // Prevent default behavior (Prevent file from being opened)
            e.preventDefault();
            if (currentCancel) currentCancel.cancelled = true;

            if (e.dataTransfer.items) {
                // Use DataTransferItemList interface to access the file(s)
                [...e.dataTransfer.items].forEach(async (item, i) => {
                    // If dropped items aren't files, reject them
                    if (item.kind === "file") {
                        const file = item.getAsFile();
                        console.log(`... file[${i}].name = ${file.name}`);
                        currentCancel = { cancelled: false };
                            await transcribeAudio(nets, async () => await fetchMonoFloat32ArrayFile(file, OfflineAudioContext), currentCancel, onTranscriptionEvent, loadAndInitializeModels);
                    }
                });
            } else {
                // Use DataTransfer interface to access the file(s)
                [...e.dataTransfer.files].forEach((file, i) => {
                    console.log(`... file[${i}].name = ${file.name}`);
                });
            }
        });

        // currentCancel = { cancelled: false };
        // transcribeAudio(nets, async () => await fetchMonoFloat32Array(`./${AUDIO_PATH}`), currentCancel, onTranscriptionEvent, loadAndInitializeModels);
        loadAndInitializeModels();
    </script>
</body>
</html>