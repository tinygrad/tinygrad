{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"tinygrad documentation","text":"<p>Welcome to the docs for tinygrad. This page is for users of the tinygrad library. tinygrad is not 1.0 yet, but it will be soon. The API has been pretty stable for a while.</p> <p>While you can <code>pip install tinygrad</code>, we encourage you to install from source:</p> <pre><code>git clone https://github.com/tinygrad/tinygrad.git\ncd tinygrad\npython3 -m pip install -e .\n</code></pre> <p>After you have installed tinygrad, try the MNIST tutorial.</p> <p>If you are new to tensor libraries, learn how to use them by solving puzzles from tinygrad-tensor-puzzles.</p> <p>We also have developer docs, and Di Zhu has created a bunch of tutorials to help understand how tinygrad works.</p>"},{"location":"#tinygrad-usage","title":"tinygrad Usage","text":"<p>The main class you will interact with is Tensor. It functions very similarly to PyTorch, but has a bit more of a functional style. tinygrad supports many datatypes.  All operations in tinygrad are lazy, meaning they won't do anything until you realize.</p> <ul> <li>tinygrad has a built in neural network library with some classes, optimizers, and load/save state management.</li> <li>tinygrad has a JIT to make things fast. Decorate your pure function with <code>TinyJit</code></li> <li>tinygrad has amazing support for multiple GPUs, allowing you to shard your Tensors with <code>Tensor.shard</code></li> </ul> <p>To understand what training looks like in tinygrad, you should read <code>beautiful_mnist.py</code></p> <p>We have a quickstart guide and a showcase</p>"},{"location":"#tinygrad-stack","title":"tinygrad Stack","text":""},{"location":"#differences-from-pytorch","title":"Differences from PyTorch","text":"<p>If you are migrating from PyTorch, welcome. Most of the API is the same. We hope you will find tinygrad both familiar and somehow more \"correct feeling\"</p>"},{"location":"#tinygrad-doesnt-have-nnmodule","title":"tinygrad doesn't have nn.Module","text":"<p>There's nothing special about a \"Module\" class in tinygrad, it's just a normal class. <code>nn.state.get_parameters</code> can be used to recursively search normal classes for valid tensors. Instead of the <code>forward</code> method in PyTorch, tinygrad just uses <code>__call__</code></p>"},{"location":"#tinygrad-is-functional","title":"tinygrad is functional","text":"<p>In tinygrad, you can do <code>x.conv2d(w, b)</code> or <code>x.sparse_categorical_crossentropy(y)</code>. We do also have a <code>Conv2D</code> class like PyTorch if you want a place to keep the state, but all stateless operations don't have classes.</p>"},{"location":"#tinygrad-is-lazy","title":"tinygrad is lazy","text":"<p>When you do <code>a+b</code> in tinygrad, nothing happens. It's not until you <code>realize</code> the Tensor that the computation actually runs.</p>"},{"location":"#tinygrad-requires-tinyjit-to-be-fast","title":"tinygrad requires @TinyJit to be fast","text":"<p>PyTorch spends a lot of development effort to make dispatch very fast. tinygrad doesn't. We have a simple decorator that will replay the kernels used in the decorated function.</p>"},{"location":"dtypes/","title":"dtypes","text":""},{"location":"dtypes/#tinygrad.dtype.DType","title":"DType  <code>dataclass</code>","text":"<pre><code>DType(\n    priority: int,\n    itemsize: int,\n    name: str,\n    fmt: FmtStr | None,\n    count: int,\n    _scalar: DType | None,\n)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes","title":"dtypes","text":"<p>Methods:</p> <ul> <li> <code>is_float</code>             \u2013              </li> <li> <code>is_int</code>             \u2013              </li> <li> <code>is_unsigned</code>             \u2013              </li> <li> <code>is_bool</code>             \u2013              </li> <li> <code>from_py</code>             \u2013              </li> <li> <code>as_const</code>             \u2013              </li> <li> <code>min</code>             \u2013              </li> <li> <code>max</code>             \u2013              </li> <li> <code>finfo</code>             \u2013              <p>(exponent, mantissa)</p> </li> <li> <code>fields</code>             \u2013              </li> <li> <code>imageh</code>             \u2013              </li> <li> <code>imagef</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>void</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>index</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>bool</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>int8</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>uint8</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>int16</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>uint16</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>int32</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>uint32</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>int64</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>uint64</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>fp8e4m3</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>fp8e5m2</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>float16</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>bfloat16</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>float32</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>float64</code>               (<code>Final[DType]</code>)           \u2013            </li> <li> <code>half</code>           \u2013            </li> <li> <code>float</code>           \u2013            </li> <li> <code>double</code>           \u2013            </li> <li> <code>uchar</code>           \u2013            </li> <li> <code>ushort</code>           \u2013            </li> <li> <code>uint</code>           \u2013            </li> <li> <code>ulong</code>           \u2013            </li> <li> <code>char</code>           \u2013            </li> <li> <code>short</code>           \u2013            </li> <li> <code>int</code>           \u2013            </li> <li> <code>long</code>           \u2013            </li> <li> <code>default_float</code>               (<code>DType</code>)           \u2013            </li> <li> <code>default_int</code>               (<code>DType</code>)           \u2013            </li> <li> <code>fp8s</code>           \u2013            </li> <li> <code>floats</code>           \u2013            </li> <li> <code>uints</code>           \u2013            </li> <li> <code>sints</code>           \u2013            </li> <li> <code>ints</code>           \u2013            </li> <li> <code>all</code>           \u2013            </li> </ul>"},{"location":"dtypes/#tinygrad.dtype.dtypes.void","title":"void","text":"<pre><code>void: Final[DType] = new(-1, 0, 'void', None)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.index","title":"index","text":"<pre><code>index: Final[DType] = new(-1, 100, 'index', None)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.bool","title":"bool","text":"<pre><code>bool: Final[DType] = new(0, 1, 'bool', '?')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int8","title":"int8","text":"<pre><code>int8: Final[DType] = new(1, 1, 'signed char', 'b')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint8","title":"uint8","text":"<pre><code>uint8: Final[DType] = new(2, 1, 'unsigned char', 'B')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int16","title":"int16","text":"<pre><code>int16: Final[DType] = new(3, 2, 'short', 'h')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint16","title":"uint16","text":"<pre><code>uint16: Final[DType] = new(4, 2, 'unsigned short', 'H')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int32","title":"int32","text":"<pre><code>int32: Final[DType] = new(5, 4, 'int', 'i')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint32","title":"uint32","text":"<pre><code>uint32: Final[DType] = new(6, 4, 'unsigned int', 'I')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int64","title":"int64","text":"<pre><code>int64: Final[DType] = new(7, 8, 'long', 'q')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint64","title":"uint64","text":"<pre><code>uint64: Final[DType] = new(8, 8, 'unsigned long', 'Q')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.fp8e4m3","title":"fp8e4m3","text":"<pre><code>fp8e4m3: Final[DType] = new(9, 1, 'float8_e4m3', None)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.fp8e5m2","title":"fp8e5m2","text":"<pre><code>fp8e5m2: Final[DType] = new(10, 1, 'float8_e5m2', None)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.float16","title":"float16","text":"<pre><code>float16: Final[DType] = new(11, 2, 'half', 'e')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.bfloat16","title":"bfloat16","text":"<pre><code>bfloat16: Final[DType] = new(12, 2, '__bf16', None)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.float32","title":"float32","text":"<pre><code>float32: Final[DType] = new(13, 4, 'float', 'f')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.float64","title":"float64","text":"<pre><code>float64: Final[DType] = new(14, 8, 'double', 'd')\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.half","title":"half","text":"<pre><code>half = float16\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.float","title":"float","text":"<pre><code>float = float32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.double","title":"double","text":"<pre><code>double = float64\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uchar","title":"uchar","text":"<pre><code>uchar = uint8\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.ushort","title":"ushort","text":"<pre><code>ushort = uint16\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uint","title":"uint","text":"<pre><code>uint = uint32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.ulong","title":"ulong","text":"<pre><code>ulong = uint64\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.char","title":"char","text":"<pre><code>char = int8\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.short","title":"short","text":"<pre><code>short = int16\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.int","title":"int","text":"<pre><code>int = int32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.long","title":"long","text":"<pre><code>long = int64\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.default_float","title":"default_float","text":"<pre><code>default_float: DType = float32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.default_int","title":"default_int","text":"<pre><code>default_int: DType = int32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.fp8s","title":"fp8s","text":"<pre><code>fp8s = (fp8e4m3, fp8e5m2)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.floats","title":"floats","text":"<pre><code>floats = fp8s + (float16, bfloat16, float32, float64)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.uints","title":"uints","text":"<pre><code>uints = (uint8, uint16, uint32, uint64)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.sints","title":"sints","text":"<pre><code>sints = (int8, int16, int32, int64)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.ints","title":"ints","text":"<pre><code>ints = uints + sints\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.all","title":"all","text":"<pre><code>all = floats + ints + (bool, index)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.is_float","title":"is_float","text":"<pre><code>is_float(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\n@functools.cache\ndef is_float(x: DType) -&gt; bool: return x.scalar() in dtypes.floats or isinstance(x, ImageDType)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.is_int","title":"is_int","text":"<pre><code>is_int(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod # static methods on top, or bool in the type info will refer to dtypes.bool\n@functools.cache\ndef is_int(x: DType) -&gt; bool: return x.scalar() in dtypes.ints + (dtypes.index,)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.is_unsigned","title":"is_unsigned","text":"<pre><code>is_unsigned(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\n@functools.cache\ndef is_unsigned(x: DType) -&gt; bool: return x.scalar() in dtypes.uints\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.is_bool","title":"is_bool","text":"<pre><code>is_bool(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef is_bool(x: DType) -&gt; bool: return x.scalar() == dtypes.bool\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.from_py","title":"from_py","text":"<pre><code>from_py(x) -&gt; DType\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef from_py(x) -&gt; DType:\n  if x.__class__ is float: return dtypes.default_float\n  if x.__class__ is int: return dtypes.default_int\n  if x.__class__ is bool: return dtypes.bool\n  # put this in the last is faster because there are more items than lists/tuples to check\n  if x.__class__ is list or x.__class__ is tuple: return max(dtypes.from_py(xi) for xi in x) if x else dtypes.default_float\n  raise RuntimeError(f\"Could not infer dtype of {x} with type {type(x)}\")\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.as_const","title":"as_const","text":"<pre><code>as_const(\n    val: (\n        tuple[ConstType | InvalidType, ...]\n        | ConstType\n        | InvalidType\n    ),\n    dtype: DType,\n)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef as_const(val: tuple[ConstType|InvalidType, ...]|ConstType|InvalidType, dtype:DType):\n  if isinstance(val, tuple):\n    assert len(val) == dtype.count, f\"mismatch {val} {dtype}\"\n    return tuple(dtypes.as_const(x, dtype) for x in val)\n  if isinstance(val, InvalidType): return val\n  return int(val) if dtypes.is_int(dtype) else float(val) if dtypes.is_float(dtype) else bool(val)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.min","title":"min","text":"<pre><code>min(dtype: DType)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\n@functools.cache\ndef min(dtype:DType):\n  if dtypes.is_int(dtype): return 0 if dtypes.is_unsigned(dtype) else -2**(dtype.scalar().itemsize*8-1)\n  return -float(\"inf\") if dtypes.is_float(dtype) else False\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.max","title":"max","text":"<pre><code>max(dtype: DType)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\n@functools.cache\ndef max(dtype:DType):\n  if dtypes.is_int(dtype): return 2**(dtype.scalar().itemsize*8)-1+dtypes.min(dtype)\n  return float(\"inf\") if dtypes.is_float(dtype) else True\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.finfo","title":"finfo","text":"<pre><code>finfo(dtype: DType) -&gt; tuple[int, int]\n</code></pre> <p>(exponent, mantissa)</p> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef finfo(dtype:DType) -&gt; tuple[int, int]:\n  \"\"\"(exponent, mantissa)\"\"\"\n  if not dtypes.is_float(dtype): raise ValueError(f\"{dtype} is not a floating point type\")\n  return {dtypes.float16: (5, 10), dtypes.bfloat16: (8, 7), dtypes.float32: (8, 23), dtypes.float64: (11, 52),\n          dtypes.fp8e5m2: (5, 2), dtypes.fp8e4m3: (4, 3)}[dtype]\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.fields","title":"fields","text":"<pre><code>fields() -&gt; dict[str, DType]\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef fields() -&gt; dict[str, DType]: return DTYPES_DICT\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.imageh","title":"imageh","text":"<pre><code>imageh(shp)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef imageh(shp): return ImageDType(100, 2, \"imageh\", 'e', 1, None, dtypes.float32, AddrSpace.GLOBAL, 1, prod(shp), shp)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.dtypes.imagef","title":"imagef","text":"<pre><code>imagef(shp)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef imagef(shp): return ImageDType(100, 4, \"imagef\", 'f', 1, None, dtypes.float32, AddrSpace.GLOBAL, 1, prod(shp), shp)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtype.ConstType","title":"ConstType  <code>module-attribute</code>","text":"<pre><code>ConstType = float | int | bool\n</code></pre>"},{"location":"env_vars/","title":"List of environment variables that control tinygrad behavior.","text":"<p>This is a list of environment variable that control the runtime behavior of tinygrad and its examples. Most of these are self-explanatory, and are usually used to set an option at runtime.</p> <p>Example: <code>CL=1 DEBUG=4 python3 -m pytest</code></p> <p>However you can also decorate a function to set a value only inside that function.</p> <pre><code># in tensor.py (probably only useful if you are a tinygrad developer)\n@Context(DEBUG=4)\ndef numpy(self) -&gt; ...\n</code></pre> <p>Or use contextmanager to temporarily set a value inside some scope:</p> <pre><code>with Context(DEBUG=0):\n  a = Tensor.ones(10, 10)\n  a *= 2\n</code></pre>"},{"location":"env_vars/#global-variables","title":"Global Variables","text":"<p>The columns of this list are are: Variable, Possible Value(s) and Description.</p> <ul> <li>A <code>#</code> means that the variable can take any integer value.</li> </ul> <p>These control the behavior of core tinygrad even when used as a library.</p> Variable Possible Value(s) Description DEBUG [1-7] enable debugging output (operations, timings, speed, generated code and more) CL [1] enable OpenCL backend CUDA [1] enable CUDA backend AMD [1] enable AMD backend NV [1] enable NV backend METAL [1] enable Metal backend (for Mac M1 and after) CPU [1] enable CPU backend BEAM [#] number of beams in kernel beam search DEFAULT_FLOAT [HALF, ...] specify the default float dtype (FLOAT32, HALF, BFLOAT16, FLOAT64, ...), default to FLOAT32 IMAGE [1-2] enable 2d specific optimizations FLOAT16 [1] use float16 for images instead of float32 HCQ_VISIBLE_DEVICES [list[int]] restricts the HCQ devices that are available. The format is a comma-separated list of identifiers (indexing starts with 0). JIT [0-2] 0=disabled, 1=jit enabled (default), 2=jit enabled, but graphs are disabled VIZ [1] 0=disabled, 1=viz enabled ALLOW_TF32 [1] enable TensorFloat-32 tensor cores on Ampere or newer GPUs. WEBGPU_BACKEND [WGPUBackendType_Metal, ...] Force select a backend for WebGPU (Metal, DirectX, OpenGL, Vulkan...) CUDA_PATH str Use <code>CUDA_PATH/include</code> for CUDA headers for CUDA and NV backends. If not set, TinyGrad will use <code>/usr/local/cuda/include</code>, <code>/usr/include</code> and <code>/opt/cuda/include</code>."},{"location":"env_vars/#debug-breakdown","title":"Debug breakdown","text":"Variable Value Description DEBUG &gt;= 1 Enables debugging and lists devices being used DEBUG &gt;= 2 Provides performance metrics for operations, including timing, memory usage, bandwidth for each kernel execution DEBUG &gt;= 3 Outputs buffers used for each kernel (shape, dtype and strides) and the applied optimizations at a kernel level DEBUG &gt;= 4 Outputs the generated kernel code DEBUG &gt;= 5 Displays the intermediate representation of the computation UOps (AST) DEBUG &gt;= 6 Displays the intermediate representation of the computation UOps in a linearized manner, detailing the operation sequence DEBUG &gt;= 7 Outputs the assembly code generated for the target hardware"},{"location":"mnist/","title":"MNIST Tutorial","text":"<p>After you have installed tinygrad, this is a great first tutorial.</p> <p>Start up a notebook locally, or use colab. tinygrad is very lightweight, so it's easy to install anywhere and doesn't need a special colab image, but for speed we recommend a T4 GPU image.</p>"},{"location":"mnist/#one-liner-to-install-tinygrad-in-colab","title":"One-liner to install tinygrad in colab","text":"<pre><code>!pip install git+https://github.com/tinygrad/tinygrad.git\n</code></pre>"},{"location":"mnist/#whats-the-default-device","title":"What's the default device?","text":"<pre><code>from tinygrad import Device\nprint(Device.DEFAULT)\n</code></pre> <p>You will see <code>CUDA</code> here on a GPU instance, or <code>CPU</code> here on a CPU instance.</p>"},{"location":"mnist/#a-simple-model","title":"A simple model","text":"<p>We'll use the model from the Keras tutorial.</p> <pre><code>from tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n</code></pre> <p>Two key differences from PyTorch:</p> <ul> <li>Only the stateful layers are declared in <code>__init__</code></li> <li>There's no <code>nn.Module</code> class or <code>forward</code> function, just a normal class and <code>__call__</code></li> </ul>"},{"location":"mnist/#getting-the-dataset","title":"Getting the dataset","text":"<pre><code>from tinygrad.nn.datasets import mnist\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n# (60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar\n</code></pre> <p>tinygrad includes MNIST, it only adds four lines. Feel free to read the function.</p>"},{"location":"mnist/#using-the-model","title":"Using the model","text":"<p>MNIST is small enough that the <code>mnist()</code> function copies the dataset to the default device.</p> <p>So creating the model and evaluating it is a matter of:</p> <pre><code>model = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\n# NOTE: tinygrad is lazy, and hasn't actually run anything by this point\nprint(acc.item())  # ~10% accuracy, as expected from a random model\n</code></pre>"},{"location":"mnist/#training-the-model","title":"Training the model","text":"<p>We'll use the Adam optimizer. The <code>nn.state.get_parameters</code> will walk the model class and pull out the parameters for the optimizer. Also, in tinygrad, it's typical to write a function to do the training step so it can be jitted.</p> <pre><code>optim = nn.optim.Adam(nn.state.get_parameters(model))\nbatch_size = 128\ndef step():\n  Tensor.training = True  # makes dropout work\n  samples = Tensor.randint(batch_size, high=X_train.shape[0])\n  X, Y = X_train[samples], Y_train[samples]\n  optim.zero_grad()\n  loss = model(X).sparse_categorical_crossentropy(Y).backward()\n  optim.step()\n  return loss\n</code></pre> <p>You can time a step with:</p> <pre><code>import timeit\ntimeit.repeat(step, repeat=5, number=1)\n#[0.08268719699981375,\n# 0.07478952900009972,\n# 0.07714716600003158,\n# 0.07785399599970333,\n# 0.07605237000007037]\n</code></pre> <p>So around 75 ms on T4 colab.</p> <p>If you want to see a breakdown of the time by kernel:</p> <pre><code>from tinygrad import GlobalCounters, Context\nGlobalCounters.reset()\nwith Context(DEBUG=2): step()\n</code></pre>"},{"location":"mnist/#why-so-slow","title":"Why so slow?","text":"<p>Unlike PyTorch, tinygrad isn't designed to be fast like that. While 75 ms for one step is plenty fast for debugging, it's not great for training. Here, we introduce the first quintessentially tinygrad concept, the <code>TinyJit</code>.</p> <pre><code>from tinygrad import TinyJit\njit_step = TinyJit(step)\n</code></pre> <p>Note</p> <p>It can also be used as a decorator <code>@TinyJit</code></p> <p>Now when we time it:</p> <pre><code>import timeit\ntimeit.repeat(jit_step, repeat=5, number=1)\n# [0.2596786549997887,\n#  0.08989566299987928,\n#  0.0012115650001760514,\n#  0.001010227999813651,\n#  0.0012164899999334011]\n</code></pre> <p>1.0 ms is 75x faster! Note that we aren't syncing the GPU, so GPU time may be slower.</p> <p>The slowness the first two times is the JIT capturing the kernels. And this JIT will not run any Python in the function, it will just replay the tinygrad kernels that were run, so be aware that non tinygrad Python operations won't work. Randomness functions work as expected.</p> <p>Unlike other JITs, we JIT everything, including the optimizer. Think of it as a dumb replay on different data.</p>"},{"location":"mnist/#putting-it-together","title":"Putting it together","text":"<p>Since we are just randomly sampling from the dataset, there's no real concept of an epoch. We have a batch size of 128, so the Keras example is taking about 7000 steps.</p> <pre><code>for step in range(7000):\n  loss = jit_step()\n  if step%100 == 0:\n    Tensor.training = False\n    acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n    print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")\n</code></pre> <p>It doesn't take long to reach 98%, and it usually reaches 99%.</p> <pre><code>step    0, loss 4.03, acc 71.43%\nstep  100, loss 0.34, acc 93.86%\nstep  200, loss 0.23, acc 95.97%\nstep  300, loss 0.18, acc 96.32%\nstep  400, loss 0.18, acc 96.76%\nstep  500, loss 0.13, acc 97.46%\nstep  600, loss 0.14, acc 97.45%\nstep  700, loss 0.10, acc 97.27%\nstep  800, loss 0.23, acc 97.49%\nstep  900, loss 0.13, acc 97.51%\nstep 1000, loss 0.13, acc 97.88%\nstep 1100, loss 0.11, acc 97.72%\nstep 1200, loss 0.14, acc 97.65%\nstep 1300, loss 0.12, acc 98.04%\nstep 1400, loss 0.25, acc 98.17%\nstep 1500, loss 0.11, acc 97.86%\nstep 1600, loss 0.21, acc 98.21%\nstep 1700, loss 0.14, acc 98.34%\n...\n</code></pre>"},{"location":"mnist/#from-here","title":"From here?","text":"<p>tinygrad is yours to play with now. It's pure Python and short, so unlike PyTorch, fixing library bugs is well within your abilities.</p> <ul> <li>It's two lines to add multiGPU support to this example (can you find them?). You have to <code>.shard</code> the model to all GPUs, and <code>.shard</code> the dataset by batch.</li> <li><code>with Context(DEBUG=2)</code> shows the running kernels, <code>DEBUG=4</code> shows the code. All <code>Context</code> variables can also be environment variables.</li> <li><code>with Context(BEAM=2)</code> will do a BEAM search on the kernels, searching many possible implementations for what runs the fastest on your hardware. After this search, tinygrad is usually speed competitive with PyTorch, and the results are cached so you won't have to search next time.</li> </ul> <p>Join our Discord for help, and if you want to be a tinygrad developer. Please read the Discord rules when you get there.</p> <p>Follow us on Twitter to keep up with the project.</p>"},{"location":"nn/","title":"nn (Neural Networks)","text":""},{"location":"nn/#neural-network-classes","title":"Neural Network classes","text":""},{"location":"nn/#tinygrad.nn.BatchNorm","title":"BatchNorm","text":"<pre><code>BatchNorm(\n    sz: int,\n    eps=1e-05,\n    affine=True,\n    track_running_stats=True,\n    momentum=0.1,\n)\n</code></pre> <p>Applies Batch Normalization over a 2D or 3D input.</p> <ul> <li>Paper: https://arxiv.org/abs/1502.03167v3</li> </ul> <p>See: <code>Tensor.batchnorm</code></p> <p></p> <p><pre><code>norm = nn.BatchNorm(3)\nt = Tensor.rand(2, 3, 4, 4)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>0.5185703039169312 0.2886631488800049\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>0.518567681312561 0.2886617183685303\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, sz:int, eps=1e-5, affine=True, track_running_stats=True, momentum=0.1):\n  self.eps, self.track_running_stats, self.momentum = eps, track_running_stats, momentum\n\n  self.weight: Tensor|None = Tensor.ones(sz) if affine else None\n  self.bias: Tensor|None = Tensor.zeros(sz) if affine else None\n\n  self.num_batches_tracked = Tensor.zeros(dtype='long' if is_dtype_supported(dtypes.long) else 'int', requires_grad=False)\n  if track_running_stats: self.running_mean, self.running_var = Tensor.zeros(sz, requires_grad=False), Tensor.ones(sz, requires_grad=False)\n</code></pre>"},{"location":"nn/#tinygrad.nn.Conv1d","title":"Conv1d","text":"<pre><code>Conv1d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride=1,\n    padding: int | str = 0,\n    dilation=1,\n    groups=1,\n    bias=True,\n) -&gt; Conv2d\n</code></pre> <p>Applies a 1D convolution over an input signal composed of several input planes.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d</p> <p><pre><code>conv = nn.Conv1d(1, 1, 3)\nt = Tensor.rand(1, 1, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[[0.1133 0.601  0.0855 0.1201]]]\n</code></pre> <pre><code>t = conv(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[[-0.6351 -0.6351]]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def Conv1d(in_channels:int, out_channels:int, kernel_size:int, stride=1, padding:int|str=0, dilation=1, groups=1, bias=True) -&gt; Conv2d:\n  \"\"\"\n  Applies a 1D convolution over an input signal composed of several input planes.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  conv = nn.Conv1d(1, 1, 3)\n  t = Tensor.rand(1, 1, 4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = conv(t)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return Conv2d(in_channels, out_channels, (kernel_size,), stride, padding, dilation, groups, bias)\n</code></pre>"},{"location":"nn/#tinygrad.nn.Conv2d","title":"Conv2d","text":"<pre><code>Conv2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int | tuple[int, ...],\n    stride=1,\n    padding: int | tuple[int, ...] | str = 0,\n    dilation=1,\n    groups=1,\n    bias=True,\n)\n</code></pre> <p>Applies a 2D convolution over an input signal composed of several input planes.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d</p> <p><pre><code>conv = nn.Conv2d(1, 1, 3)\nt = Tensor.rand(1, 1, 4, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[0.7682 0.1358 0.4199 0.3376]\n   [0.0415 0.9464 0.0563 0.4148]\n   [0.6869 0.976  0.4791 0.7606]\n   [0.2725 0.8664 0.3136 0.5489]]]]\n</code></pre> <pre><code>t = conv(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[ 0.2612  0.0785]\n   [-0.1276 -0.1461]]]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, in_channels:int, out_channels:int, kernel_size:int|tuple[int, ...], stride=1, padding:int|tuple[int, ...]|str=0,\n             dilation=1, groups=1, bias=True):\n  self.kernel_size = make_tuple(kernel_size, 2)\n  if isinstance(padding, str):\n    if padding.lower() != 'same': raise ValueError(f\"Invalid padding string {padding!r}, only 'same' is supported\")\n    if stride != 1: raise ValueError(\"padding='same' is not supported for strided convolutions\")\n    pad = [(d*(k-1)//2, d*(k-1) - d*(k-1)//2) for d,k in zip(make_tuple(dilation, len(self.kernel_size)), self.kernel_size[::-1])]\n    padding = tuple(flatten(pad))\n  self.stride, self.dilation, self.groups, self.padding = stride, dilation, groups, padding\n  scale = 1 / math.sqrt(in_channels * prod(self.kernel_size))\n  self.weight = Tensor.uniform(out_channels, in_channels//groups, *self.kernel_size, low=-scale, high=scale)\n  self.bias: Tensor|None = Tensor.uniform(out_channels, low=-scale, high=scale) if bias else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.ConvTranspose1d","title":"ConvTranspose1d","text":"<pre><code>ConvTranspose1d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride=1,\n    padding=0,\n    output_padding=0,\n    dilation=1,\n    groups=1,\n    bias=True,\n) -&gt; ConvTranspose2d\n</code></pre> <p>Applies a 1D transposed convolution operator over an input signal composed of several input planes.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d</p> <p><pre><code>conv = nn.ConvTranspose1d(1, 1, 3)\nt = Tensor.rand(1, 1, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[[0.021  0.0631 0.3674 0.9583]]]\n</code></pre> <pre><code>t = conv(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[[-0.5389 -0.5201 -0.3999 -0.1429 -0.5153 -0.733 ]]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def ConvTranspose1d(in_channels:int, out_channels:int, kernel_size:int, stride=1, padding=0, output_padding=0, dilation=1,\n                      groups=1, bias=True) -&gt; ConvTranspose2d:\n  \"\"\"\n  Applies a 1D transposed convolution operator over an input signal composed of several input planes.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  conv = nn.ConvTranspose1d(1, 1, 3)\n  t = Tensor.rand(1, 1, 4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = conv(t)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return ConvTranspose2d(in_channels, out_channels, (kernel_size,), stride, padding, output_padding, dilation, groups, bias)\n</code></pre>"},{"location":"nn/#tinygrad.nn.ConvTranspose2d","title":"ConvTranspose2d","text":"<pre><code>ConvTranspose2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int | tuple[int, ...],\n    stride=1,\n    padding=0,\n    output_padding=0,\n    dilation=1,\n    groups=1,\n    bias=True,\n)\n</code></pre> <p>               Bases: <code>Conv2d</code></p> <p>Applies a 2D transposed convolution operator over an input image.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d</p> <p><pre><code>conv = nn.ConvTranspose2d(1, 1, 3)\nt = Tensor.rand(1, 1, 4, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[0.8743 0.7737 0.3324 0.4167]\n   [0.3393 0.5252 0.1672 0.3989]\n   [0.2084 0.7387 0.059  0.5065]\n   [0.0191 0.2033 0.4916 0.3371]]]]\n</code></pre> <pre><code>t = conv(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[-0.4597 -0.4928 -0.3642 -0.3576 -0.2781 -0.2493]\n   [-0.3228 -0.4251 -0.6017 -0.5787 -0.3939 -0.3697]\n   [-0.1772 -0.1235  0.072  -0.1359 -0.1515 -0.2346]\n   [-0.212  -0.146  -0.2182 -0.3119 -0.1584 -0.275 ]\n   [-0.2293 -0.1007 -0.0123 -0.0203 -0.2729 -0.2015]\n   [-0.2566 -0.2257 -0.1338 -0.0289 -0.0265 -0.1562]]]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, in_channels:int, out_channels:int, kernel_size:int|tuple[int, ...], stride=1, padding=0, output_padding=0,\n              dilation=1, groups=1, bias=True):\n  super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n  scale = 1 / math.sqrt(in_channels * prod(self.kernel_size))\n  self.weight = Tensor.uniform(in_channels, out_channels//groups, *self.kernel_size, low=-scale, high=scale)\n  self.output_padding = output_padding\n</code></pre>"},{"location":"nn/#tinygrad.nn.Linear","title":"Linear","text":"<pre><code>Linear(in_features: int, out_features: int, bias=True)\n</code></pre> <p>Applies a linear transformation to the incoming data.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Linear</p> <p><pre><code>lin = nn.Linear(3, 4)\nt = Tensor.rand(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0.8842 0.4019 0.8932]\n [0.3422 0.7943 0.9099]]\n</code></pre> <pre><code>t = lin(t)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.0764 -0.0843  0.3063  0.9537]\n [ 0.2811  0.0256  0.3048  1.0553]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, in_features:int, out_features:int, bias=True):\n  bound = 1 / math.sqrt(in_features)\n  self.weight = Tensor.uniform(out_features, in_features, low=-bound, high=bound)\n  self.bias = Tensor.uniform(out_features, low=-bound, high=bound) if bias else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.GroupNorm","title":"GroupNorm","text":"<pre><code>GroupNorm(\n    num_groups: int,\n    num_channels: int,\n    eps=1e-05,\n    affine=True,\n)\n</code></pre> <p>Applies Group Normalization over a mini-batch of inputs.</p> <ul> <li>Paper: https://arxiv.org/abs/1803.08494v3</li> </ul> <p><pre><code>norm = nn.GroupNorm(2, 12)\nt = Tensor.rand(2, 12, 4, 4) * 2 + 1\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>2.0061357021331787 0.5649832487106323\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>-1.656739243571792e-07 1.0012884140014648\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, num_groups:int, num_channels:int, eps=1e-5, affine=True):\n  self.num_groups, self.num_channels, self.eps = num_groups, num_channels, eps\n  self.weight: Tensor|None = Tensor.ones(num_channels) if affine else None\n  self.bias: Tensor|None = Tensor.zeros(num_channels) if affine else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.InstanceNorm","title":"InstanceNorm","text":"<pre><code>InstanceNorm(\n    num_features: int,\n    eps: float = 1e-05,\n    affine: bool = True,\n)\n</code></pre> <p>Applies Instance Normalization over a mini-batch of inputs.</p> <ul> <li>Paper: https://arxiv.org/abs/1607.08022v3</li> </ul> <p><pre><code>norm = nn.InstanceNorm(3)\nt = Tensor.rand(2, 3, 4, 4) * 2 + 1\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>1.9133431911468506 0.5628378391265869\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>2.6529335173108848e-08 1.0052313804626465\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, num_features:int, eps:float=1e-5, affine:bool=True):\n  self.num_features, self.eps = num_features, eps\n  self.weight: Tensor|None = Tensor.ones(num_features) if affine else None\n  self.bias: Tensor|None = Tensor.zeros(num_features) if affine else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.LayerNorm","title":"LayerNorm","text":"<pre><code>LayerNorm(\n    normalized_shape: int | tuple[int, ...],\n    eps: float = 1e-05,\n    elementwise_affine: bool = True,\n)\n</code></pre> <p>Applies Layer Normalization over a mini-batch of inputs.</p> <ul> <li>Paper: https://arxiv.org/abs/1607.06450v1</li> </ul> <p><pre><code>norm = nn.LayerNorm(3)\nt = Tensor.rand(2, 5, 3) * 2 + 1\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>2.021045684814453 0.5872205495834351\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>-1.3620058325614082e-07 1.0170257091522217\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, normalized_shape:int|tuple[int, ...], eps:float=1e-5, elementwise_affine:bool=True):\n  self.normalized_shape: tuple[int, ...] = make_tuple(normalized_shape, 1)\n  self.axis, self.eps = tuple(-1-i for i in range(len(self.normalized_shape))), eps\n  self.weight: Tensor|None = Tensor.ones(*self.normalized_shape) if elementwise_affine else None\n  self.bias: Tensor|None = Tensor.zeros(*self.normalized_shape) if elementwise_affine else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.LayerNorm2d","title":"LayerNorm2d","text":"<pre><code>LayerNorm2d(\n    normalized_shape: int | tuple[int, ...],\n    eps: float = 1e-05,\n    elementwise_affine: bool = True,\n)\n</code></pre> <p>               Bases: <code>LayerNorm</code></p> <p>Applies Layer Normalization over a mini-batch of 2D inputs.</p> <p>See: <code>LayerNorm</code></p> <p><pre><code>norm = nn.LayerNorm2d(3)\nt = Tensor.rand(2, 3, 4, 4) * 2 + 1\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>1.9417126178741455 0.5618748068809509\n</code></pre> <pre><code>t = norm(t)\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>-1.840613776948885e-07 1.0051913261413574\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, normalized_shape:int|tuple[int, ...], eps:float=1e-5, elementwise_affine:bool=True):\n  self.normalized_shape: tuple[int, ...] = make_tuple(normalized_shape, 1)\n  self.axis, self.eps = tuple(-1-i for i in range(len(self.normalized_shape))), eps\n  self.weight: Tensor|None = Tensor.ones(*self.normalized_shape) if elementwise_affine else None\n  self.bias: Tensor|None = Tensor.zeros(*self.normalized_shape) if elementwise_affine else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.RMSNorm","title":"RMSNorm","text":"<pre><code>RMSNorm(dim: int, eps=1e-06, elementwise_affine=True)\n</code></pre> <p>Applies Root Mean Square Normalization to input.</p> <ul> <li>Paper: https://arxiv.org/abs/1910.07467</li> </ul> <p><pre><code>norm = nn.RMSNorm(4)\nt = Tensor.arange(12, dtype=dtypes.float).reshape(3, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.  1.  2.  3.]\n [ 4.  5.  6.  7.]\n [ 8.  9. 10. 11.]]\n</code></pre> <pre><code>print(norm(t).numpy())\n</code></pre> <pre><code>[[0.     0.5345 1.069  1.6036]\n [0.7127 0.8909 1.069  1.2472]\n [0.8363 0.9409 1.0454 1.15  ]]\n</code></pre></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, dim:int, eps=1e-6, elementwise_affine=True):\n  self.eps = eps\n  self.weight = Tensor.ones(dim) if elementwise_affine else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.Embedding","title":"Embedding","text":"<pre><code>Embedding(vocab_size: int, embed_size: int)\n</code></pre> <p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Embedding</p> <pre><code>emb = nn.Embedding(10, 3)\nprint(emb(Tensor([1, 2, 3, 1])).numpy())\n</code></pre> <pre><code>[[-0.3581  0.5092 -0.5223]\n [-0.4426  0.3769  0.1709]\n [-0.305  -0.1404 -0.0182]\n [-0.3581  0.5092 -0.5223]]\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, vocab_size:int, embed_size:int):\n  self.vocab_sz, self.embed_sz, self.weight = vocab_size, embed_size, Tensor.glorot_uniform(vocab_size, embed_size)\n</code></pre>"},{"location":"nn/#tinygrad.nn.LSTMCell","title":"LSTMCell","text":"<pre><code>LSTMCell(\n    input_size: int, hidden_size: int, bias: bool = True\n)\n</code></pre> <p>A long short-term memory (LSTM) cell.</p> <p>Parameters:</p> <ul> <li> <code>input_size</code>               (<code>int</code>)           \u2013            <p>The number of expected features in the input <code>x</code></p> </li> <li> <code>hidden_size</code>               (<code>int</code>)           \u2013            <p>The number of features in the hidden state <code>h</code></p> </li> <li> <code>bias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code></p> </li> </ul> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, input_size:int, hidden_size:int, bias:bool=True):\n  stdv = 1.0 / math.sqrt(hidden_size)\n  self.weight_ih = Tensor.uniform(hidden_size*4, input_size, low=-stdv, high=stdv)\n  self.weight_hh = Tensor.uniform(hidden_size*4, hidden_size, low=-stdv, high=stdv)\n  self.bias_ih: Tensor|None = Tensor.zeros(hidden_size*4) if bias else None\n  self.bias_hh: Tensor|None = Tensor.zeros(hidden_size*4) if bias else None\n</code></pre>"},{"location":"nn/#optimizers","title":"Optimizers","text":""},{"location":"nn/#tinygrad.nn.optim.SGD","title":"SGD","text":"<pre><code>SGD(\n    params: list[Tensor],\n    lr=0.001,\n    momentum=0.0,\n    weight_decay=0.0,\n    nesterov=False,\n    classic=False,\n    fused=FUSE_OPTIM,\n)\n</code></pre> <p>Stochastic Gradient Descent (SGD) optimizer with optional momentum and weight decay.</p> <p><code>classic</code> is a boolean flag that determines whether to use the popular momentum update rule or the classic momentum update rule.</p> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def SGD(params: list[Tensor], lr=0.001, momentum=0.0, weight_decay=0.0, nesterov=False, classic=False, fused=FUSE_OPTIM):\n  \"\"\"\n  Stochastic Gradient Descent (SGD) optimizer with optional momentum and weight decay.\n\n  `classic` is a boolean flag that determines whether to use the popular momentum update rule or the classic momentum update rule.\n  \"\"\"\n  return LARS(params, lr, momentum, weight_decay, 0, None, nesterov, classic=classic, pre_wd=True, tcoef=0.0, fused=fused)\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.LARS","title":"LARS","text":"<pre><code>LARS(\n    params: list[Tensor],\n    lr=0.001,\n    momentum=0.9,\n    weight_decay=0.0001,\n    ns_steps=0,\n    ns_coefficients=None,\n    nesterov=False,\n    classic=True,\n    pre_wd=True,\n    tcoef=0.001,\n    fused=FUSE_OPTIM,\n)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Layer-wise Adaptive Rate Scaling (LARS) optimizer with optional momentum and weight decay.</p> <ul> <li>Paper: https://arxiv.org/abs/1708.03888v3</li> </ul> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def __init__(self, params:list[Tensor], lr=0.001, momentum=0.9, weight_decay=1e-4, ns_steps=0, ns_coefficients=None,\n             nesterov=False, classic=True, pre_wd=True, tcoef=0.001, fused=FUSE_OPTIM):\n  super().__init__(params, lr, fused)\n  self.momentum, self.wd, self.ns_steps, self.ns_coefficients  = momentum, weight_decay, ns_steps, ns_coefficients\n  self.nesterov, self.classic, self.pre_wd, self.tcoef = nesterov, classic, pre_wd, tcoef\n  self.b = self._new_optim_param() if self.momentum else []\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.AdamW","title":"AdamW","text":"<pre><code>AdamW(\n    params: list[Tensor],\n    lr=0.001,\n    b1=0.9,\n    b2=0.999,\n    eps=1e-08,\n    weight_decay=0.01,\n    fused=FUSE_OPTIM,\n)\n</code></pre> <p>AdamW optimizer with optional weight decay.</p> <ul> <li>Paper: https://arxiv.org/abs/1711.05101v3</li> </ul> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def AdamW(params: list[Tensor], lr=0.001, b1=0.9, b2=0.999, eps=1e-8, weight_decay=0.01, fused=FUSE_OPTIM):\n  \"\"\"\n  AdamW optimizer with optional weight decay.\n\n  - Paper: https://arxiv.org/abs/1711.05101v3\n  \"\"\"\n  return LAMB(params, lr, b1, b2, eps, weight_decay, adam=True, fused=fused)\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.Adam","title":"Adam","text":"<pre><code>Adam(\n    params: list[Tensor],\n    lr=0.001,\n    b1=0.9,\n    b2=0.999,\n    eps=1e-08,\n    fused=FUSE_OPTIM,\n)\n</code></pre> <p>Adam optimizer.</p> <ul> <li>Paper: https://arxiv.org/abs/1412.6980</li> </ul> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def Adam(params: list[Tensor], lr=0.001, b1=0.9, b2=0.999, eps=1e-8, fused=FUSE_OPTIM):\n  \"\"\"\n  Adam optimizer.\n\n  - Paper: https://arxiv.org/abs/1412.6980\n  \"\"\"\n  return LAMB(params, lr, b1, b2, eps, 0.0, adam=True, fused=fused)\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.LAMB","title":"LAMB","text":"<pre><code>LAMB(\n    params: list[Tensor],\n    lr=0.001,\n    b1=0.9,\n    b2=0.999,\n    eps=1e-06,\n    weight_decay=0.0,\n    adam=False,\n    fused=FUSE_OPTIM,\n)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>LAMB optimizer with optional weight decay.</p> <ul> <li>Paper: https://arxiv.org/abs/1904.00962</li> </ul> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def __init__(self, params: list[Tensor], lr=0.001, b1=0.9, b2=0.999, eps=1e-6, weight_decay=0.0, adam=False, fused=FUSE_OPTIM):\n  super().__init__(params, lr, fused)\n  self.b1, self.b2, self.eps, self.wd, self.adam = b1, b2, eps, weight_decay, adam\n  self.b1_t, self.b2_t = (Tensor.ones((1,), dtype=dtypes.float32, device=self.device, requires_grad=False).contiguous() for _ in [b1, b2])\n  self.m = self._new_optim_param()\n  self.v = self._new_optim_param()\n</code></pre>"},{"location":"nn/#loadsave","title":"Load/Save","text":""},{"location":"nn/#tinygrad.nn.state.safe_load","title":"safe_load","text":"<pre><code>safe_load(fn: Tensor | str | Path) -&gt; dict[str, Tensor]\n</code></pre> <p>Loads a .safetensor file, returning the <code>state_dict</code>.</p> <pre><code>state_dict = nn.state.safe_load(\"test.safetensor\")\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def safe_load(fn:Tensor|str|pathlib.Path) -&gt; dict[str, Tensor]:\n  \"\"\"\n  Loads a .safetensor file, returning the `state_dict`.\n\n  ```python\n  state_dict = nn.state.safe_load(\"test.safetensor\")\n  ```\n  \"\"\"\n  t, data_start, metadata = safe_load_metadata(fn)\n  data = t[data_start:]\n  return { k: data[v['data_offsets'][0]:v['data_offsets'][1]].bitcast(safe_dtypes[v['dtype']]).reshape(v['shape'])\n          for k, v in metadata.items() if k != \"__metadata__\" }\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.safe_save","title":"safe_save","text":"<pre><code>safe_save(\n    tensors: dict[str, Tensor],\n    fn: str,\n    metadata: dict[str, Any] | None = None,\n)\n</code></pre> <p>Saves a <code>state_dict</code> to disk in a .safetensor file with optional metadata.</p> <pre><code>t = Tensor([1, 2, 3])\nnn.state.safe_save({'t':t}, \"test.safetensor\")\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def safe_save(tensors:dict[str, Tensor], fn:str, metadata:dict[str, Any]|None=None):\n  \"\"\"\n  Saves a `state_dict` to disk in a .safetensor file with optional metadata.\n\n  ```python\n  t = Tensor([1, 2, 3])\n  nn.state.safe_save({'t':t}, \"test.safetensor\")\n  ```\n  \"\"\"\n  headers, offset = {}, 0\n  if metadata: headers['__metadata__'] = metadata\n  for k,v in tensors.items():\n    headers[k] = {'dtype': inverse_safe_dtypes[v.dtype], 'shape': list(v.shape), 'data_offsets':[offset, offset+v.nbytes()]}\n    offset += v.nbytes()\n  j = json.dumps(headers, separators=(',', ':'))\n  j += \"\\x20\"*(round_up(len(j),8)-len(j))\n  pathlib.Path(fn).unlink(missing_ok=True)\n  t = Tensor.empty(8+len(j)+offset, dtype=dtypes.uint8, device=f\"disk:{fn}\")\n  t[0:8].bitcast(dtypes.int64).assign([len(j)])\n  t[8:8+len(j)].assign(list(j.encode('utf-8')))\n  for k,v in safe_load(t).items(): v.assign(tensors[k])\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.get_state_dict","title":"get_state_dict","text":"<pre><code>get_state_dict(\n    obj, prefix: str = \"\", tensor_type=Tensor\n) -&gt; dict[str, Tensor]\n</code></pre> <p>Returns a <code>state_dict</code> of the object, with optional prefix.</p> <pre><code>class Net:\n  def __init__(self):\n    self.l1 = nn.Linear(4, 5)\n    self.l2 = nn.Linear(5, 6)\n\nnet = Net()\nprint(nn.state.get_state_dict(net).keys())\n</code></pre> <pre><code>dict_keys(['l1.weight', 'l1.bias', 'l2.weight', 'l2.bias'])\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def get_state_dict(obj, prefix:str='', tensor_type=Tensor) -&gt; dict[str, Tensor]:\n  \"\"\"\n  Returns a `state_dict` of the object, with optional prefix.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  class Net:\n    def __init__(self):\n      self.l1 = nn.Linear(4, 5)\n      self.l2 = nn.Linear(5, 6)\n\n  net = Net()\n  print(nn.state.get_state_dict(net).keys())\n  ```\n  \"\"\"\n  if isinstance(obj, tensor_type): return {prefix.strip('.'):obj}\n  if hasattr(obj, '_asdict'): return get_state_dict(obj._asdict(), prefix, tensor_type)  # namedtuple\n  if isinstance(obj, OrderedDict): return get_state_dict(dict(obj), prefix, tensor_type)\n  if hasattr(obj, '__dict__'): return get_state_dict(obj.__dict__, prefix, tensor_type)\n  state_dict = {}\n  if isinstance(obj, (list, tuple)):\n    for i,x in enumerate(obj): state_dict.update(get_state_dict(x, f\"{prefix}{str(i)}.\", tensor_type))\n  elif isinstance(obj, dict):\n    for k,v in obj.items(): state_dict.update(get_state_dict(v, f\"{prefix}{str(k)}.\", tensor_type))\n  return state_dict\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters(obj) -&gt; list[Tensor]\n</code></pre> <pre><code>class Net:\n  def __init__(self):\n    self.l1 = nn.Linear(4, 5)\n    self.l2 = nn.Linear(5, 6)\n\nnet = Net()\nprint(len(nn.state.get_parameters(net)))\n</code></pre> <pre><code>4\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def get_parameters(obj) -&gt; list[Tensor]:\n  \"\"\"\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  class Net:\n    def __init__(self):\n      self.l1 = nn.Linear(4, 5)\n      self.l2 = nn.Linear(5, 6)\n\n  net = Net()\n  print(len(nn.state.get_parameters(net)))\n  ```\n  \"\"\"\n  return list(get_state_dict(obj).values())\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(\n    model,\n    state_dict: dict[str, Tensor],\n    strict=True,\n    verbose=True,\n    consume=False,\n    realize=True,\n) -&gt; list[Tensor]\n</code></pre> <p>Loads a <code>state_dict</code> into a model. Return the loaded Tensors.</p> <pre><code>class Net:\n  def __init__(self):\n    self.l1 = nn.Linear(4, 5)\n    self.l2 = nn.Linear(5, 6)\n\nnet = Net()\nstate_dict = nn.state.get_state_dict(net)\nnn.state.load_state_dict(net, state_dict)\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def load_state_dict(model, state_dict:dict[str, Tensor], strict=True, verbose=True, consume=False, realize=True) -&gt; list[Tensor]:\n  \"\"\"\n  Loads a `state_dict` into a model. Return the loaded Tensors.\n\n  ```python\n  class Net:\n    def __init__(self):\n      self.l1 = nn.Linear(4, 5)\n      self.l2 = nn.Linear(5, 6)\n\n  net = Net()\n  state_dict = nn.state.get_state_dict(net)\n  nn.state.load_state_dict(net, state_dict)\n  ```\n  \"\"\"\n  start_mem_used = GlobalCounters.mem_used\n  ret = []\n  with Timing(\"loaded weights in \",\n              lambda et_ns: f\", {(B:=(GlobalCounters.mem_used-start_mem_used))/1e9:.2f} GB loaded at {B/et_ns:.2f} GB/s\", enabled=verbose):\n    model_state_dict = get_state_dict(model)\n    if DEBUG &gt;= 1 and len(state_dict) &gt; len(model_state_dict):\n      print(\"WARNING: unused weights in state_dict\", sorted(list(state_dict.keys() - model_state_dict.keys())))\n    for k,v in (t := tqdm(model_state_dict.items(), disable=CI or not verbose)):\n      t.desc = f\"ram used: {GlobalCounters.mem_used/1e9:5.2f} GB, {k:50s}: \"\n      if k not in state_dict and not strict:\n        if DEBUG &gt;= 1: print(f\"WARNING: not loading {k}\")\n        continue\n      if v.shape != state_dict[k].shape:\n        raise ValueError(f'Shape mismatch in layer `{k}`: Expected shape {v.shape}, but found {state_dict[k].shape} in state dict.')\n      if isinstance(v.device, tuple):\n        if isinstance(state_dict[k].device, tuple): v.replace(state_dict[k])\n        else: v.replace(state_dict[k].shard(v.device, v.uop.axis))\n      else: v.replace(state_dict[k].to(v.device))\n      if realize: v.realize()\n      if consume: del state_dict[k]\n      ret.append(v)\n  return ret\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.tar_extract","title":"<code>tar_extract</code>","text":"<pre><code>tar_extract(fn: Tensor | str | Path) -&gt; dict[str, Tensor]\n</code></pre> <p>Extracts files from a tar archive and returns them as a dictionary of names (keys) and tensors (values).</p> <pre><code>tensors = nn.state.tar_extract(Tensor(pathlib.Path(\"archive.tar\")))\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>@accept_filename\ndef tar_extract(t: Tensor) -&gt; dict[str, Tensor]:\n  \"\"\"\n  ```python\n  tar_extract(fn: Tensor | str | Path) -&gt; dict[str, Tensor]\n  ```\n\n  Extracts files from a tar archive and returns them as a dictionary of names (keys) and tensors (values).\n\n  ```python\n  tensors = nn.state.tar_extract(Tensor(pathlib.Path(\"archive.tar\")))\n  ```\n  \"\"\"\n  with tarfile.open(fileobj=TensorIO(t), mode=\"r\") as tar:\n    return {member.name:t[member.offset_data:member.offset_data+member.size] for member in tar if member.type == tarfile.REGTYPE}\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.torch_load","title":"<code>torch_load</code>","text":"<pre><code>torch_load(fn: Tensor | str | Path) -&gt; dict[str, Tensor]\n</code></pre> <p>Loads a torch .pth file, returning the <code>state_dict</code>.</p> <pre><code>state_dict = nn.state.torch_load(\"test.pth\")\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>@accept_filename\ndef torch_load(t:Tensor) -&gt; dict[str, Tensor]:\n  \"\"\"\n  ```python\n  torch_load(fn: Tensor | str | Path) -&gt; dict[str, Tensor]\n  ```\n\n  Loads a torch .pth file, returning the `state_dict`.\n\n  ```python\n  state_dict = nn.state.torch_load(\"test.pth\")\n  ```\n  \"\"\"\n  offsets: dict[str|int, int] = {}\n  lens: dict[str|int, int] = {}\n  def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad=None, backward_hooks=None, metadata=None):\n    #print(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata)\n    lens[storage[2]] = storage[4] * storage[1].itemsize\n    if storage[2] not in offsets: return None\n    byte_offset = offsets[storage[2]]+storage_offset*storage[1].itemsize\n    ret = t[byte_offset:byte_offset+prod(size)*storage[1].itemsize].bitcast(storage[1])\n\n    # 7 lines to deal with permuted tensors. NOTE: this currently requires reading off the disk\n    shape_strides = [(s, st) for s,st in zip(size, stride) if s != 1]\n    permute_indexes = [len(shape_strides)-1-y for y in argsort([x[1] for x in shape_strides])]\n    if tuple(permute_indexes) != tuple(range(len(permute_indexes))):\n      intermediate_shape = tuple([shape_strides[x][0] for x in argsort(permute_indexes)])\n      assert tuple([shape_strides[i][1] for i in argsort(permute_indexes)]) == strides_for_shape(intermediate_shape), \"nonpermutable strides\"\n      if DEBUG &gt;= 3: print(f\"WARNING: this torch load is slow. to permute {intermediate_shape} with {permute_indexes}\")\n      assert storage[1] != dtypes.bfloat16, \"can't permute BF16\"\n      # TODO: find a nice way to support all movement ops on disktensors\n      ret = ret.to(None).reshape(intermediate_shape).permute(permute_indexes)\n\n    return ret.reshape(size)\n\n  class Parameter:\n    def __setstate__(self, state): self.tensor = state[0]\n\n  deserialized_objects: dict[str, Any] = {}\n  intercept = {\"HalfStorage\": dtypes.float16, \"FloatStorage\": dtypes.float32, \"BFloat16Storage\": dtypes.bfloat16,\n               \"IntStorage\": dtypes.int32, \"BoolStorage\": dtypes.bool,\n               \"LongStorage\": dtypes.int64, \"_rebuild_tensor_v2\": _rebuild_tensor_v2, \"FloatTensor\": None, \"Parameter\": Parameter}\n  whitelist = {\"torch\", \"collections\", \"numpy\", \"_codecs\"}  # NOTE: this is not for security, only speed\n  class Dummy: pass\n  class TorchPickle(pickle.Unpickler):\n    def find_class(self, module, name):\n      module_root = module.split(\".\")[0]\n      if module_root not in whitelist:\n        if DEBUG &gt;= 2: print(f\"WARNING: returning Dummy for {module} {name}\")\n        return Dummy\n      return intercept[name] if module_root == \"torch\" else super().find_class(module, name)\n    def persistent_load(self, pid): return deserialized_objects.get(pid, pid)\n\n  fobj = io.BufferedReader(TensorIO(t))\n  def passthrough_reset(v: bool): return fobj.seek(0, 0) or v\n\n  if passthrough_reset(zipfile.is_zipfile(fobj)): # NOTE: passthrough_reset required to support python &lt; 3.14\n    myzip = zipfile.ZipFile(fobj, 'r')\n    base_name = None\n    header_offsets = {}\n    for zi in myzip.filelist:\n      if base_name is None: base_name = zi.filename.split('/', 1)[0]\n      if zi.filename.startswith(f'{base_name}/data/'): header_offsets[zi.filename.split(\"/\")[-1]] = zi.header_offset\n    # sadly there's no way to get the start of the file in the zip without reading the header\n    # at least here we read them in parallel\n    header_contents = [t[v+26:v+30].bitcast(dtypes.uint16).to('CPU') for v in header_offsets.values()]\n    Tensor.realize(*header_contents)\n    for (n,o),c in zip(header_offsets.items(), header_contents):\n      # header_offset + sizeFileHeader + File name length + Extra field length : https://en.wikipedia.org/wiki/ZIP_(file_format)\n      offsets[n] = o+30+sum(cast(list[int], c.tolist()))\n    with myzip.open(f'{base_name}/data.pkl') as myfile:\n      return TorchPickle(myfile).load()\n  elif passthrough_reset(tarfile.is_tarfile(fobj)): # NOTE: passthrough_reset required to support python &lt; 3.11\n    with tarfile.open(fileobj=fobj, mode=\"r\") as tar:\n      storages_offset = tar.getmember('storages').offset_data\n      f = unwrap(tar.extractfile('storages'))\n      for i in range(TorchPickle(f).load()):  # num_storages\n        (key, _, storage_type), sz = TorchPickle(f).load(), struct.unpack('&lt;q', f.read(8))[0]\n        offsets[key] = storages_offset + f.tell()\n        f.seek(sz*storage_type.itemsize, 1)\n      f = unwrap(tar.extractfile('tensors'))\n      for _ in range(TorchPickle(f).load()):  # num_tensors\n        (key, storage_id, _), ndim, _ = TorchPickle(f).load(), struct.unpack('&lt;i', f.read(4))[0], f.read(4)\n        size, stride = struct.unpack(f'&lt;{ndim}q', f.read(8 * ndim)), struct.unpack(f'&lt;{ndim}q', f.read(8 * ndim))\n        storage_offset = struct.unpack('&lt;q', f.read(8))[0]\n        deserialized_objects[str(key)] = _rebuild_tensor_v2((None, storage_type, storage_id, None, -1), storage_offset, size, stride)\n      return {k:v.tensor if isinstance(v, Parameter) else v for k,v in TorchPickle(unwrap(tar.extractfile('pickle'))).load().items()}\n  else:\n    pkl = TorchPickle(fobj)\n    _, _, _, rwd, _, ids, base_offset = pkl.load(), pkl.load(), pkl.load(), fobj.tell(), pkl.load(), pkl.load(), fobj.tell()\n    for i in ids:\n      offsets[i] = base_offset + 8\n      base_offset += 8 + lens[i]\n    fobj.seek(rwd)\n    return TorchPickle(fobj).load()\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.gguf_load","title":"gguf_load","text":"<pre><code>gguf_load(tensor: Tensor) -&gt; tuple[dict, dict[str, Tensor]]\n</code></pre> <p>Loads a .gguf file, returning the <code>kv_data</code> and <code>state_dict</code>.</p> <pre><code>gguf_tensor = Tensor(pathlib.Path(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")).to(Device.DEFAULT)\nkv_data, state_dict = nn.state.gguf_load(gguf_tensor)\n</code></pre> <p>Note</p> <p>The provided tensor must be on a device that supports execution.</p> Source code in <code>tinygrad/nn/state.py</code> <pre><code>@accept_filename\ndef gguf_load(tensor: Tensor) -&gt; tuple[dict, dict[str, Tensor]]:\n  \"\"\"\n  Loads a .gguf file, returning the `kv_data` and `state_dict`.\n\n  ```python\n  gguf_tensor = Tensor(pathlib.Path(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")).to(Device.DEFAULT)\n  kv_data, state_dict = nn.state.gguf_load(gguf_tensor)\n  ```\n\n  NOTE: The provided tensor must be on a device that supports execution.\n  \"\"\"\n  reader, kv_data, state_dict = io.BufferedReader(TensorIO(tensor), 1_000_000), {}, {}\n  def read_unpack(fmt: str, n: int): return struct.unpack(fmt, reader.read(n))[0]\n  def read_str(): return str(reader.read(read_uint64()), \"utf-8\")\n  def read_arr():\n    reader, n = readers[read_int32()], read_uint64()\n    return [ reader() for _ in range(n) ]\n\n  readers: dict[int, Callable[[], Any]] = { 8: read_str, 9: read_arr, **{ t: functools.partial(read_unpack, \"&lt;\"+f, nb) for t,f,nb in \\\n    [ (0,\"c\",1), (1,\"b\",1), (2,\"H\",2), (3,\"h\",2), (4,\"I\",4), (5,\"i\",4), (6,\"f\",4), (7,\"?\",1), (10,\"Q\",8), (11,\"q\",8), (12,\"d\",8) ] } }\n  read_uint32, read_int32, read_uint64, read_int64 = readers[4], readers[5], readers[10], readers[11]\n\n  magic, version, n_tensors, n_kv = reader.read(4), read_int32(), read_int64(), read_int64()\n  if magic != b\"GGUF\" or version not in [2, 3]: raise ValueError(\"Invalid GGUF format!\")\n  for _ in range(n_kv):\n    k, typ = read_str(), read_int32()\n    kv_data[k] = readers[typ]()\n\n  t_infos = [ (read_str(), tuple(read_uint64() for _ in range(read_uint32())), read_int32(), read_uint64()) for _ in range(n_tensors) ]\n  alignment, pos = kv_data.get(\"general.alignment\", 32), reader.tell()\n  data_start = round_up(pos, alignment)\n\n  for name, dims, typ, off in t_infos: state_dict[name] = ggml_data_to_tensor(tensor[data_start + off:], prod(dims), typ).reshape(*reversed(dims))\n\n  return kv_data, state_dict\n</code></pre>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>This guide assumes no prior knowledge of pytorch or any other deep learning framework, but does assume some basic knowledge of neural networks. It is intended to be a very quick overview of the high level API that tinygrad provides.</p> <p>This guide is also structured as a tutorial which at the end of it you will have a working model that can classify handwritten digits.</p> <p>We need some imports to get started:</p> <pre><code>import numpy as np\nfrom tinygrad.helpers import Timing\n</code></pre>"},{"location":"quickstart/#tensors","title":"Tensors","text":"<p>Tensors are the base data structure in tinygrad. They can be thought of as a multidimensional array of a specific data type. All high level operations in tinygrad operate on these tensors.</p> <p>The tensor class can be imported like so:</p> <pre><code>from tinygrad import Tensor\n</code></pre> <p>Tensors can be created from an existing data structure like a python list or numpy ndarray:</p> <pre><code>t1 = Tensor([1, 2, 3, 4, 5])\nna = np.array([1, 2, 3, 4, 5])\nt2 = Tensor(na)\n</code></pre> <p>Tensors can also be created using one of the many factory methods:</p> <pre><code>full = Tensor.full(shape=(2, 3), fill_value=5) # create a tensor of shape (2, 3) filled with 5\nzeros = Tensor.zeros(2, 3) # create a tensor of shape (2, 3) filled with 0\nones = Tensor.ones(2, 3) # create a tensor of shape (2, 3) filled with 1\n\nfull_like = Tensor.full_like(full, fill_value=2) # create a tensor of the same shape as `full` filled with 2\nzeros_like = Tensor.zeros_like(full) # create a tensor of the same shape as `full` filled with 0\nones_like = Tensor.ones_like(full) # create a tensor of the same shape as `full` filled with 1\n\neye = Tensor.eye(3) # create a 3x3 identity matrix\narange = Tensor.arange(start=0, stop=10, step=1) # create a tensor of shape (10,) filled with values from 0 to 9\n\nrand = Tensor.rand(2, 3) # create a tensor of shape (2, 3) filled with random values from a uniform distribution\nrandn = Tensor.randn(2, 3) # create a tensor of shape (2, 3) filled with random values from a standard normal distribution\nuniform = Tensor.uniform(2, 3, low=0, high=10) # create a tensor of shape (2, 3) filled with random values from a uniform distribution between 0 and 10\n</code></pre> <p>There are even more of these factory methods, you can find them in the Tensor Creation file.</p> <p>All the tensors creation methods can take a <code>dtype</code> argument to specify the data type of the tensor, find the supported <code>dtype</code> in dtypes.</p> <pre><code>from tinygrad import dtypes\n\nt3 = Tensor([1, 2, 3, 4, 5], dtype=dtypes.int32)\n</code></pre> <p>Tensors allow you to perform operations on them like so:</p> <pre><code>t4 = Tensor([1, 2, 3, 4, 5])\nt5 = (t4 + 1) * 2\nt6 = (t5 * t4).relu().log_softmax()\n</code></pre> <p>All of these operations are lazy and are only executed when you realize the tensor using <code>.realize()</code> or <code>.numpy()</code>.</p> <pre><code>print(t6.numpy())\n# [-56. -48. -36. -20.   0.]\n</code></pre> <p>There are a lot more operations that can be performed on tensors, you can find them in the Tensor Ops file. Additionally reading through abstractions2.py will help you understand how operations on these tensors make their way down to your hardware.</p>"},{"location":"quickstart/#models","title":"Models","text":"<p>Neural networks in tinygrad are really just represented by the operations performed on tensors. These operations are commonly grouped into the <code>__call__</code> method of a class which allows modularization and reuse of these groups of operations. These classes do not need to inherit from any base class, in fact if they don't need any trainable parameters they don't even need to be a class!</p> <p>An example of this would be the <code>nn.Linear</code> class which represents a linear layer in a neural network.</p> <pre><code>class Linear:\n  def __init__(self, in_features, out_features, bias=True, initialization: str='kaiming_uniform'):\n    self.weight = getattr(Tensor, initialization)(out_features, in_features)\n    self.bias = Tensor.zeros(out_features) if bias else None\n\n  def __call__(self, x):\n    return x.linear(self.weight.transpose(), self.bias)\n</code></pre> <p>There are more neural network modules already implemented in nn, and you can also implement your own.</p> <p>We will be implementing a simple neural network that can classify handwritten digits from the MNIST dataset. Our classifier will be a simple 2 layer neural network with a Leaky ReLU activation function. It will use a hidden layer size of 128 and an output layer size of 10 (one for each digit) with no bias on either Linear layer.</p> <pre><code>class TinyNet:\n  def __init__(self):\n    self.l1 = Linear(784, 128, bias=False)\n    self.l2 = Linear(128, 10, bias=False)\n\n  def __call__(self, x):\n    x = self.l1(x)\n    x = x.leaky_relu()\n    x = self.l2(x)\n    return x\n\nnet = TinyNet()\n</code></pre> <p>We can see that the forward pass of our neural network is just the sequence of operations performed on the input tensor <code>x</code>. We can also see that functional operations like <code>leaky_relu</code> are not defined as classes and instead are just methods we can just call. Finally, we just initialize an instance of our neural network, and we are ready to start training it.</p>"},{"location":"quickstart/#training","title":"Training","text":"<p>Now that we have our neural network defined we can start training it. Training neural networks in tinygrad is super simple. All we need to do is define our neural network, define our loss function, and then call <code>.backward()</code> on the loss function to compute the gradients. They can then be used to update the parameters of our neural network using one of the many Optimizers.</p> <p>For our loss function we will be using sparse categorical cross entropy loss. The implementation below is taken from tensor.py, it's copied below to highlight an important detail of tinygrad.</p> <pre><code>def sparse_categorical_crossentropy(self, Y, ignore_index=-1) -&gt; Tensor:\n    loss_mask = Y != ignore_index\n    y_counter = Tensor.arange(self.shape[-1], dtype=dtypes.int32, requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n    y = ((y_counter == Y.flatten().reshape(-1, 1)).where(-1.0, 0) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n    return self.log_softmax().mul(y).sum() / loss_mask.sum()\n</code></pre> <p>As we can see in this implementation of cross entropy loss, there are certain operations that tinygrad does not support natively. Load/store ops are not supported in tinygrad natively because they add complexity when trying to port to different backends, 90% of the models out there don't use/need them, and they can be implemented like it's done above with an <code>arange</code> mask.</p> <p>For our optimizer we will be using the traditional stochastic gradient descent optimizer with a learning rate of 3e-4.</p> <pre><code>from tinygrad.nn.optim import SGD\n\nopt = SGD([net.l1.weight, net.l2.weight], lr=3e-4)\n</code></pre> <p>We can see that we are passing in the parameters of our neural network to the optimizer. This is due to the fact that the optimizer needs to know which parameters to update. There is a simpler way to do this just by using <code>get_parameters(net)</code> from <code>tinygrad.nn.state</code> which will return a list of all the parameters in the neural network. The parameters are just listed out explicitly here for clarity.</p> <p>Now that we have our network, loss function, and optimizer defined all we are missing is the data to train on! There are a couple of dataset loaders in tinygrad located in /extra/datasets. We will be using the MNIST dataset loader.</p> <pre><code>from extra.datasets import fetch_mnist\n</code></pre> <p>Now we have everything we need to start training our neural network. We will be training for 1000 steps with a batch size of 64.</p> <p>We use <code>with Tensor.train()</code> to set the internal flag <code>Tensor.training</code> to <code>True</code> during training. Upon exit, the flag is restored to its previous value by the context manager.</p> <pre><code>X_train, Y_train, X_test, Y_test = fetch_mnist()\n\nwith Tensor.train():\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_train.shape[0], size=(64))\n    batch = Tensor(X_train[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Tensor(Y_train[samp])\n\n    # forward pass\n    out = net(batch)\n\n    # compute loss\n    loss = sparse_categorical_crossentropy(out, labels)\n\n    # zero gradients\n    opt.zero_grad()\n\n    # backward pass\n    loss.backward()\n\n    # update parameters\n    opt.step()\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1)\n    acc = (pred == labels).mean()\n\n    if step % 100 == 0:\n      print(f\"Step {step+1} | Loss: {loss.numpy()} | Accuracy: {acc.numpy()}\")\n</code></pre>"},{"location":"quickstart/#evaluation","title":"Evaluation","text":"<p>Now that we have trained our neural network we can evaluate it on the test set. We will be using the same batch size of 64 and will be evaluating for 1000 of those batches.</p> <pre><code>with Timing(\"Time: \"):\n  avg_acc = 0\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_test.shape[0], size=(64))\n    batch = Tensor(X_test[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Y_test[samp]\n\n    # forward pass\n    out = net(batch)\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1).numpy()\n    avg_acc += (pred == labels).mean()\n  print(f\"Test Accuracy: {avg_acc / 1000}\")\n</code></pre>"},{"location":"quickstart/#and-thats-it","title":"And that's it","text":"<p>Highly recommend you check out the examples/ folder for more examples of using tinygrad. Reading the source code of tinygrad is also a great way to learn how it works. Specifically the tests in test/ are a great place to see how to use and the semantics of the different operations. There are also a bunch of models implemented in models/ that you can use as a reference.</p> <p>Additionally, feel free to ask questions in the <code>#learn-tinygrad</code> channel on the discord. Don't ask to ask, just ask!</p>"},{"location":"quickstart/#extras","title":"Extras","text":""},{"location":"quickstart/#jit","title":"JIT","text":"<p>Additionally, it is possible to speed up the computation of certain neural networks by using the JIT. Currently, this does not support models with varying input sizes and non tinygrad operations.</p> <p>To use the JIT we just need to add a function decorator to the forward pass of our neural network and ensure that the input and output are realized tensors. Or in this case we will create a wrapper function and decorate the wrapper function to speed up the evaluation of our neural network.</p> <pre><code>from tinygrad import TinyJit\n\n@TinyJit\ndef jit(x):\n  return net(x).realize()\n\nwith Timing(\"Time: \"):\n  avg_acc = 0\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_test.shape[0], size=(64))\n    batch = Tensor(X_test[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Y_test[samp]\n\n    # forward pass with jit\n    out = jit(batch)\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1).numpy()\n    avg_acc += (pred == labels).mean()\n  print(f\"Test Accuracy: {avg_acc / 1000}\")\n</code></pre> <p>You will find that the evaluation time is much faster than before and that your accelerator utilization is much higher.</p>"},{"location":"quickstart/#saving-and-loading-models","title":"Saving and Loading Models","text":"<p>The standard weight format for tinygrad is safetensors. This means that you can load the weights of any model also using safetensors into tinygrad. There are functions in state.py to save and load models to and from this format.</p> <pre><code>from tinygrad.nn.state import safe_save, safe_load, get_state_dict, load_state_dict\n\n# first we need the state dict of our model\nstate_dict = get_state_dict(net)\n\n# then we can just save it to a file\nsafe_save(state_dict, \"model.safetensors\")\n\n# and load it back in\nstate_dict = safe_load(\"model.safetensors\")\nload_state_dict(net, state_dict)\n</code></pre> <p>Many of the models in the models/ folder have a <code>load_from_pretrained</code> method that will download and load the weights for you. These usually are pytorch weights meaning that you would need pytorch installed to load them.</p>"},{"location":"quickstart/#environment-variables","title":"Environment Variables","text":"<p>There exist a bunch of environment variables that control the runtime behavior of tinygrad. Some of the commons ones are <code>DEBUG</code> and the different backend enablement variables.</p> <p>You can find a full list and their descriptions in env_vars.md.</p>"},{"location":"quickstart/#visualizing-the-computation-graph","title":"Visualizing the Computation Graph","text":"<p>It is possible to visualize the computation graph of a neural network using VIZ=1.</p>"},{"location":"runtime/","title":"Runtimes","text":"<p>tinygrad supports various runtimes, enabling your code to scale across a wide range of devices. The default runtime can be automatically selected based on the available hardware, or you can force a specific runtime to be default using environment variables (e.g., <code>CPU=1</code>).</p> Runtime Description Compiler Options Requirements NV Provides acceleration for NVIDIA GPUs nvrtc (default)PTX (<code>NV_PTX=1</code>) Ampere/Ada/Blackwell series GPUs.You can select an interface via <code>NV_IFACE=(NVK\\|PCI)</code>. See NV interfaces for details. AMD Provides acceleration for AMD GPUs LLVM (<code>AMD_LLVM=1</code>)HIP/COMGR (<code>AMD_HIP=1</code>) RDNA2 or newer GPUs.You can select an interface via <code>AMD_IFACE=(KFD\\|PCI\\|USB)</code>. See AMD interfaces for details. QCOM Provides acceleration for QCOM GPUs - 6xx series GPUs METAL Utilizes Metal for acceleration on Apple devices - M1+ Macs; Metal 3.0+ for <code>bfloat</code> support CUDA Utilizes CUDA for acceleration on NVIDIA GPUs nvrtc (default) PTX (<code>CUDA_PTX=1</code>) NVIDIA GPU with CUDA support CL Accelerates computations using OpenCL on GPUs - OpenCL 2.0 compatible device CPU Runs on CPU using the clang or llvm compiler Clang JIT (default)LLVM IR (<code>CPU_LLVM=1</code>) <code>clang</code> compiler in system <code>PATH</code> WEBGPU Runs on GPU using the Dawn WebGPU engine (used in Google Chrome) - Dawn library installed and discoverable. Binaries: pydawn v0.3.0"},{"location":"runtime/#interoperability","title":"Interoperability","text":"<p>tinygrad provides interoperability with OpenCL and PyTorch, allowing efficient tensor data sharing between frameworks through the <code>Tensor.from_blob</code> API. This enables zero-copy operations by working directly with external memory pointers.</p> <p>Important: When using external memory pointers with tinygrad tensors, you must ensure these pointers remain valid throughout the entire lifetime of the tinygrad tensor to prevent memory corruption.</p>"},{"location":"runtime/#cudametal-pytorch-interoperability","title":"<code>CUDA</code>/<code>METAL</code> PyTorch Interoperability","text":"<p>You can seamlessly work with CUDA/MPS tensors between PyTorch and tinygrad without data copying: <pre><code>from tinygrad.dtype import _from_torch_dtype\ntensor1 = torch.tensor([1.0, 2.0, 3.0], device=torch.device(\"cuda\"))\ntiny_tensor1 = Tensor.from_blob(tensor1.data_ptr(), tensor1.shape, dtype=_from_torch_dtype(tensor1.dtype), device='CUDA')\n\n# Before tinygrad calculations, mps needs to be synchronized to make sure data is valid.\nif data.device.type == \"mps\": torch.mps.synchronize()\nelse: torch.cuda.synchronize()\n\nx = (tiny_tensor1 + 1).realize()\n</code></pre></p>"},{"location":"runtime/#qcom-opencl-interoperability","title":"<code>QCOM</code> OpenCL Interoperability","text":"<p>tinygrad supports OpenCL interoperability on <code>QCOM</code> backend.</p> <p>Buffer interop allows direct access to OpenCL memory buffers: <pre><code># create raw opencl buffer.\ncl_buf = cl.clCreateBuffer(cl_context, cl.CL_MEM_READ_WRITE, 0x100, None, status := ctypes.c_int32())\n\n# extract pointers\ncl_buf_desc_ptr = to_mv(ctypes.addressof(cl_buf), 8).cast('Q')[0]\nrawbuf_ptr = to_mv(cl_buf_desc_ptr, 0x100).cast('Q')[20] # offset 0xA0 is a raw gpu pointer.\n\n# create tiny tensor\ntiny = Tensor.from_blob(rawbuf_ptr, (8, 8), dtype=dtypes.int, device='QCOM')\n</code></pre></p> <p>And the same for the images: <pre><code># create cl image.\ncl_img = cl.clCreateImage2D(cl_context, cl.CL_MEM_READ_WRITE, cl.cl_image_format(cl.CL_RGBA, cl.CL_FLOAT), w, h, 0, None, status := ctypes.c_int32())\n\n# extract pointers\ncl_buf_desc_ptr = to_mv(ctypes.addressof(cl_img), 8).cast('Q')[0]\nrawbuf_ptr = to_mv(cl_buf_desc_ptr, 0x100).cast('Q')[20] # offset 0xA0 is a raw gpu pointer.\n\n# create tiny tensor\ntiny = Tensor.from_blob(rawbuf_ptr, (h*w*4,), dtype=dtypes.imagef((h,w)), device='QCOM')\n</code></pre></p>"},{"location":"runtime/#amd-interfaces","title":"AMD Interfaces","text":"<p>AMD backend supports several interfaces for communicating with devices:</p> <ul> <li><code>KFD</code>: uses the amdgpu driver</li> <li><code>PCI</code>: uses the AM driver</li> <li><code>USB</code>: USB3 interafce for asm24xx chips.</li> </ul> <p>You can force an interface by setting <code>AMD_IFACE</code> to one of these values. In the case of <code>AMD_IFACE=PCI</code>, this may unbind your GPU from the amdgpu driver.</p>"},{"location":"runtime/#nv-interfaces","title":"NV Interfaces","text":"<p>NV backend supports several interfaces for communicating with devices:</p> <ul> <li><code>NVK</code>: uses the nvidia driver</li> <li><code>PCI</code>: uses the NV driver</li> </ul>"},{"location":"showcase/","title":"Showcase","text":"<p>Despite being a tiny library, tinygrad is capable of doing a lot of things. From state-of-the-art vision to state-of-the-art language models.</p>"},{"location":"showcase/#vision","title":"Vision","text":""},{"location":"showcase/#efficientnet","title":"EfficientNet","text":"<p>You can either pass in the URL of a picture to discover what it is: <pre><code>python3 examples/efficientnet.py ./test/models/efficientnet/Chicken.jpg\n</code></pre> Or, if you have a camera and OpenCV installed, you can detect what is in front of you: <pre><code>python3 examples/efficientnet.py webcam\n</code></pre></p>"},{"location":"showcase/#yolov8","title":"YOLOv8","text":"<p>Take a look at yolov8.py.</p> <p></p>"},{"location":"showcase/#audio","title":"Audio","text":""},{"location":"showcase/#whisper","title":"Whisper","text":"<p>Take a look at whisper.py. You need pyaudio and torchaudio installed.</p> <pre><code>SMALL=1 python3 examples/whisper.py\n</code></pre>"},{"location":"showcase/#generative","title":"Generative","text":""},{"location":"showcase/#stable-diffusion","title":"Stable Diffusion","text":"<pre><code>python3 examples/stable_diffusion.py\n</code></pre> <p>\"a horse sized cat eating a bagel\"</p>"},{"location":"showcase/#llama","title":"LLaMA","text":"<p>You will need to download and put the weights into the <code>weights/LLaMA</code> directory, which may need to be created.</p> <p>Then you can have a chat with Stacy: <pre><code>python3 examples/llama.py\n</code></pre></p>"},{"location":"showcase/#conversation","title":"Conversation","text":"<p>Make sure you have espeak installed and <code>PHONEMIZER_ESPEAK_LIBRARY</code> set.</p> <p>Then you can talk to Stacy: <pre><code>python3 examples/conversation.py\n</code></pre></p>"},{"location":"tinybox/","title":"tinybox","text":"<p>Although these docs live in tinygrad, they pertain to deep learning hardware sold by the tiny corp. tinyboxes are used heavily in tinygrad's CI, and are the best tested platform to use tinygrad with. They appeared running tinygrad on MLPerf Training 4.0</p> <p>If you don't have a tinybox and you want one, see tinygrad.org. If you don't want one, that's okay too.</p>"},{"location":"tinybox/#welcome","title":"Welcome","text":"<p>Welcome to your tinybox! The tinybox is the universal system purpose-built for all AI infrastructure and workloads, from training to inference. The red box includes six 7900XTX GPUs, the green box includes six 4090 GPUs, and the green v2 box includes four 5090 GPUs. Whether you bought a red one or a green one, we want you to love it.</p> <p>We don't have a stupid cloud service, you don't have to create a tiny account to set it up, and we aren't tracking how you use the box. We're just happy you bought one. This petaflop is your petaflop.</p>"},{"location":"tinybox/#plugging-it-in","title":"Plugging it in","text":"<p>tinybox has two 1600W PSUs, which together exceed the capacity of most 120V household circuits. Fortunately, it comes with two plugs. You'll want to plug each plug into a different circuit. You can verify that they are different circuits by flipping the breaker and seeing what turns off. If you have at least a 120V 30A or 220V 20A circuit, you are welcome to use only that one.</p> <p>You'll also want to connect the Ethernet port without a rubber stopper to your home network.</p> <p>While it's designed primarily for the home or office, the tinybox is 12U rack mountable using these rails.</p>"},{"location":"tinybox/#power-limiting-the-box","title":"Power limiting the box","text":"<p>While a tinybox should ideally be run without power limits, there are cases where you might want to run the box off of a single outlet.</p> <p>In such cases, it is possible to power limit the box using the provided <code>power-limit</code> script, which will power limit all of the GPUs to a specified wattage.</p> <p><code>sudo power-limit 150</code> should be good to run off of a single 120V 15A outlet.</p>"},{"location":"tinybox/#connecting-to-the-box","title":"Connecting to the box","text":"<p>tinybox ships with a relatively basic install of Ubuntu 22.04. To do initial setup, you can either plug in a VGA monitor and keyboard, or you can connect remotely to the machine using the BMC. The BMC IP and password are displayed on the screen.</p> <p><code>ipmitool -H &lt;BMC IP&gt; -U admin -P &lt;BMC PW&gt; -I lanplus sol activate</code></p> <p>The default username is <code>tiny</code> and the default password is <code>tiny</code>. Once you are logged in, you can add an SSH key to authorized keys to connect over SSH (on the normal IP). Exit <code>ipmitool</code> with <code>~.</code> after a newline.</p> <p>The BMC also has a web interface you can use if you find that easier.</p>"},{"location":"tinybox/#changing-the-bmc-password","title":"Changing the BMC password","text":"<p>It is recommended that you change the BMC password after setting up the box, as the password on the screen is only the initial password.</p> <p>If you do decide to change the BMC password and no longer want the initial password to be displayed, remove the <code>/root/.bmc_password</code> file. Reboot after making these changes or restart the <code>displayservice.service</code> service.</p>"},{"location":"tinybox/#what-do-i-use-it-for","title":"What do I use it for?","text":"<p>The default tinybox image ships with tinygrad and PyTorch. While we develop tinygrad, the box is universal hardware. Use whatever framework you desire, run notebooks, download demos, install more things, train, inference, live, laugh, love, you aren't paying per hour for this box so the only limit is your imagination.</p>"},{"location":"tinybox/#building-the-os-image","title":"Building the OS image","text":"<p>The OS image is built using <code>ubuntu-image</code> from https://github.com/tinygrad/tinyos.</p> <p>After cloning, run <code>make green</code> or <code>make red</code> to build a tinybox green or tinybox red image respectively.</p>"},{"location":"developer/am/","title":"AM Driver","text":"<p>AM driver is a userspace driver targeting AMD's RDNA3/RDNA4. You only need tinygrad to send compute tasks to your GPU!</p>"},{"location":"developer/am/#how-to-run","title":"How to run?","text":"<p>Make sure that amdgpu module is unloaded and just run tinygrad with <code>AMD=1</code>!</p> <p>Optional requirements:</p> <ul> <li>System without IOMMU for P2P / SDMA support</li> <li>vfio-pci module for IRQ handling</li> </ul>"},{"location":"developer/am/#environment-variables","title":"Environment Variables","text":"Variable Possible Value(s) Description AM_RESET [1] Performs a full GPU reset (reloading all firmware and IP blocks) AM_DEBUG [0-4] Sets the level of additional debugging information"},{"location":"developer/am/#am-driver-details","title":"AM Driver Details","text":""},{"location":"developer/am/#compute-sdma-queues","title":"Compute &amp; SDMA Queues","text":"<p>AM binds compute queues directly to MEC (bypassing MES). Tinygrad uses only one compute queue, which is bound at <code>pipe=0 queue=0</code>. Similarly, the single SDMA queue is bound at <code>engine=0 queue=0</code>.</p>"},{"location":"developer/am/#boot","title":"Boot","text":"<p>The GPU being passed can be in one of several states: 1. Not initialized 2. Initialized by amdgpu 3. Initialized by AM</p> <p>The first and second states require a full GPU setup since their states are unknown. The second state also requires a mode1 reset to reinitialize all components.</p> <p>The third state can be set up partially to optimize boot time. In this case, only the GFX and SDMA IPs need to be initialized. To enable this, AM uses a separate boot memory that is guaranteed not to be overwritten. This physical memory is utilized for all blocks that are initialized only during the initial AM boot. To determine if the GPU is in the third state, AM uses <code>regSCRATCH_REG7</code> as a flag.</p>"},{"location":"developer/am/#vm-management","title":"VM Management","text":"<p>Each AM device sets up only a single <code>VMID=0</code> and one page directory. The page directory used is 3-level and thus supports up to 512GB of virtual addresses. All AM devices are located in one virtual address space.</p>"},{"location":"developer/developer/","title":"Intro","text":"<p>The tinygrad framework has four pieces</p> <ul> <li>a PyTorch like frontend.</li> <li>a scheduler which breaks the compute into kernels.</li> <li>a lowering engine which converts ASTs into code that can run on the accelerator.</li> <li>an execution engine which can run that code.</li> </ul> <p>There is a good bunch of tutorials by Di Zhu that go over tinygrad internals.</p> <p>There's also a doc describing speed</p>"},{"location":"developer/developer/#frontend","title":"Frontend","text":"<p>Everything in Tensor is syntactic sugar around constructing a graph of UOps.</p> <p>The <code>UOp</code> graph specifies the compute in terms of low level tinygrad ops. Not all UOps will actually become realized. There's two types of UOps, base and view. base contains compute into a contiguous buffer, and view is a view (specified by a ShapeTracker). Inputs to a base can be either base or view, inputs to a view can only be a single base.</p>"},{"location":"developer/developer/#scheduling","title":"Scheduling","text":"<p>The scheduler converts the graph of UOps into a list of <code>ScheduleItem</code>. One <code>ScheduleItem</code> is one kernel on the GPU, and the scheduler is responsible for breaking the large compute graph into subgraphs that can fit in a kernel. <code>ast</code> specifies what compute to run, and <code>bufs</code> specifies what buffers to run it on.</p>"},{"location":"developer/developer/#tinygrad.engine.schedule.ScheduleItem","title":"ScheduleItem  <code>dataclass</code>","text":"<pre><code>ScheduleItem(\n    ast: UOp,\n    bufs: tuple[Buffer, ...],\n    metadata: tuple[Metadata, ...] = (),\n    fixedvars: dict[str, int] = dict(),\n)\n</code></pre>"},{"location":"developer/developer/#lowering","title":"Lowering","text":"<p>The code in realize lowers <code>ScheduleItem</code> to <code>ExecItem</code> with</p> <p>There's a ton of complexity hidden behind this, see the <code>codegen/</code> directory.</p> <p>First we lower the AST to UOps, which is a linear list of the compute to be run. This is where the BEAM search happens.</p> <p>Then we render the UOps into code with a <code>Renderer</code>, then we compile the code to binary with a <code>Compiler</code>.</p>"},{"location":"developer/developer/#tinygrad.engine.realize.lower_schedule","title":"lower_schedule","text":"<pre><code>lower_schedule(\n    schedule: list[ScheduleItem],\n) -&gt; Generator[tuple[ScheduleItem, ExecItem], None, None]\n</code></pre> Source code in <code>tinygrad/engine/realize.py</code> <pre><code>def lower_schedule(schedule:list[ScheduleItem]) -&gt; Generator[tuple[ScheduleItem, ExecItem], None, None]:\n  while len(schedule):\n    si = schedule.pop(0)\n    try: yield (si, lower_schedule_item(si))\n    except Exception as e:\n      if DEBUG &gt;= 2:\n        print(f\"error lowering {si.ast.op}\")\n        print(\"tensor operations:\")\n        pprint.pprint(si.metadata, indent=2)\n      raise e\n</code></pre>"},{"location":"developer/developer/#execution","title":"Execution","text":"<p>Creating <code>ExecItem</code>, which has a run method</p> <p>Lists of <code>ExecItem</code> can be condensed into a single ExecItem with the Graph API (rename to Queue?)</p>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem","title":"ExecItem  <code>dataclass</code>","text":"<pre><code>ExecItem(\n    prg: Runner,\n    bufs: list[Buffer | None],\n    metadata: tuple[Metadata, ...] | None = None,\n    fixedvars: dict[str, int] = dict(),\n)\n</code></pre> <p>Methods:</p> <ul> <li> <code>run</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>bufs</code>               (<code>list[Buffer | None]</code>)           \u2013            </li> <li> <code>fixedvars</code>               (<code>dict[str, int]</code>)           \u2013            </li> <li> <code>metadata</code>               (<code>tuple[Metadata, ...] | None</code>)           \u2013            </li> <li> <code>prg</code>               (<code>Runner</code>)           \u2013            </li> </ul>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem.bufs","title":"bufs  <code>instance-attribute</code>","text":"<pre><code>bufs: list[Buffer | None]\n</code></pre>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem.fixedvars","title":"fixedvars  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fixedvars: dict[str, int] = field(default_factory=dict)\n</code></pre>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: tuple[Metadata, ...] | None = None\n</code></pre>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem.prg","title":"prg  <code>instance-attribute</code>","text":"<pre><code>prg: Runner\n</code></pre>"},{"location":"developer/developer/#tinygrad.engine.realize.ExecItem.run","title":"run","text":"<pre><code>run(\n    _var_vals: dict[str, int] | None = None,\n    wait=False,\n    jit=False,\n    do_update_stats=True,\n) -&gt; float | None\n</code></pre> Source code in <code>tinygrad/engine/realize.py</code> <pre><code>def run(self, _var_vals:dict[str, int]|None=None, wait=False, jit=False, do_update_stats=True) -&gt; float|None:\n  var_vals = self.fixedvars if _var_vals is None else (_var_vals|self.fixedvars)\n  bufs = [unwrap(x) for x in self.bufs] if jit else [unwrap(x).ensure_allocated() for x in self.bufs]\n  if PROFILE:\n    payload = {\"metadata\":self.metadata, \"var_vals\":var_vals, \"bufs\":[b.trace_num for b in bufs], \"name\":self.prg.display_name}\n    payload[\"outputs\"], payload[\"inputs\"] = (self.prg.p.outs, self.prg.p.ins) if isinstance(self.prg, CompiledRunner) else ([0], [1])\n    cpu_events.append(ProfilePointEvent(self.prg.device, \"exec\", len(cpu_events), payload))\n  et = self.prg(bufs, var_vals, wait=wait or DEBUG &gt;= 2)\n  if do_update_stats:\n    GlobalCounters.kernel_count += 1\n    GlobalCounters.global_ops += (op_est:=sym_infer(self.prg.estimates.ops, var_vals))\n    GlobalCounters.global_mem += (mem_est:=sym_infer(self.prg.estimates.mem, var_vals))\n    if et is not None: GlobalCounters.time_sum_s += et\n    if DEBUG &gt;= 2:\n      lds_est = sym_infer(self.prg.estimates.lds, var_vals)\n      mem_est = min(mem_est, lds_est)   # there can't be more memory accessed than loads/stores. remove this when symbolic is fixed\n      header_color = 'magenta' if jit else ('green' if self.prg.first_run else None)\n      ptm = colored(time_to_str(et, w=9), \"yellow\" if et &gt; 0.01 else None) if et is not None else \"\"\n      flops, membw, ldsbw = op_est/(et or 1e-20), mem_est/(et or 1e-20), lds_est/(et or 1e-20)\n      flops_str = f\"{flops*1e-9:7.0f} GFLOPS\" if flops &lt; 1e14 else colored(f\"{flops*1e-12:7.0f} TFLOPS\", 'green')\n      mem_str = f\"{membw*1e-9:4.0f}|{ldsbw*1e-9:&lt;6.0f} GB/s\" if membw &lt; 1e13 and ldsbw &lt; 1e15 else \\\n        colored(f\"{membw*1e-12:4.0f}|{ldsbw*1e-12:&lt;6.0f} TB/s\", 'green')\n      print(f\"{colored(f'*** {self.prg.device[:7]:7s} {GlobalCounters.kernel_count:4d}', header_color)}\"+\n        f\" {self.prg.display_name+' '*(46-ansilen(self.prg.display_name))} arg {len(bufs):2d} mem {GlobalCounters.mem_used/1e9:6.2f} GB\"+\n        (\"\" if et is None else f\" tm {ptm}/{GlobalCounters.time_sum_s*1e3:9.2f}ms ({flops_str} {mem_str})\")+\n        f\" {[repr(m) if TRACEMETA &gt;= 2 else str(m) for m in self.metadata] if self.metadata else ''}\")\n    self.prg.first_run = False\n  return et\n</code></pre>"},{"location":"developer/developer/#runtime","title":"Runtime","text":"<p>Runtimes are responsible for device-specific interactions. They handle tasks such as initializing devices, allocating memory, loading/launching programs, and more. You can find more information about the runtimes API on the runtime overview page.</p> <p>All runtime implementations can be found in the runtime directory.</p>"},{"location":"developer/developer/#hcq-compatible-runtimes","title":"HCQ Compatible Runtimes","text":"<p>HCQ API is a lower-level API for defining runtimes. Interaction with HCQ-compatible devices occurs at a lower level, with commands issued directly to hardware queues. Some examples of such backends are NV and AMD, which are userspace drivers for NVIDIA and AMD devices respectively. You can find more information about the API on HCQ overview page</p>"},{"location":"developer/hcq/","title":"HCQ Compatible Runtime","text":""},{"location":"developer/hcq/#overview","title":"Overview","text":"<p>The main aspect of HCQ-compatible runtimes is how they interact with devices. In HCQ, all interactions with devices occur in a hardware-friendly manner using command queues. This approach allows commands to be issued directly to devices, bypassing runtime overhead such as HIP or CUDA. Additionally, by using the HCQ API, these runtimes can benefit from various optimizations and features, including HCQGraph and built-in profiling capabilities.</p>"},{"location":"developer/hcq/#command-queues","title":"Command Queues","text":"<p>To interact with devices you create a <code>HWQueue</code>. Some methods are required, like timestamp and synchronization methods like signal and wait, while others are dependent on it being a compute or copy queue.</p> <p>For example, the following Python code enqueues a wait, execute, and signal command on the HCQ-compatible device: <pre><code>HWQueue().wait(signal_to_wait, value_to_wait) \\\n         .exec(program, args_state, global_dims, local_dims) \\\n         .signal(signal_to_fire, value_to_fire) \\\n         .submit(your_device)\n</code></pre></p> <p>Each runtime should implement the required functions that are defined in the <code>HWQueue</code> classes.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWQueue","title":"HWQueue","text":"<pre><code>HWQueue()\n</code></pre> <p>               Bases: <code>Generic[SignalType, HCQDeviceType, ProgramType, ArgsStateType]</code></p> <p>A base class for hardware command queues in the HCQ (Hardware Command Queue) API.</p> <p>Methods:</p> <ul> <li> <code>signal</code>             \u2013              <p>Enqueues a signal command which sets the signal to the given value, ensuring all previous operations are completed.</p> </li> <li> <code>wait</code>             \u2013              <p>Enqueues a wait command which halts execution until the signal is greater than or equal to a specific value.</p> </li> <li> <code>timestamp</code>             \u2013              <p>Enqueues a timestamp command which records the current time in a signal after all previously enqueued commands are completed.</p> </li> <li> <code>bind</code>             \u2013              <p>Associates the queue with a specific device for optimized execution.</p> </li> <li> <code>submit</code>             \u2013              <p>Submits the command queue to a specific device for execution.</p> </li> <li> <code>memory_barrier</code>             \u2013              <p>Enqueues a memory barrier command to ensure memory coherence between agents. Only on compute queues.</p> </li> <li> <code>exec</code>             \u2013              <p>Enqueues an execution command for a kernel program. Only on compute queues.</p> </li> <li> <code>copy</code>             \u2013              <p>Enqueues a copy command to transfer data. Only on copy queues.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWQueue.signal","title":"signal","text":"<pre><code>signal(signal: SignalType, value: sint)\n</code></pre> <p>Enqueues a signal command which sets the signal to the given value, ensuring all previous operations are completed.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>SignalType</code>)           \u2013            <p>The signal to set</p> </li> <li> <code>value</code>               (<code>sint</code>)           \u2013            <p>The value to set the signal to</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWQueue.wait","title":"wait","text":"<pre><code>wait(signal: SignalType, value: sint)\n</code></pre> <p>Enqueues a wait command which halts execution until the signal is greater than or equal to a specific value.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>SignalType</code>)           \u2013            <p>The signal to wait on</p> </li> <li> <code>value</code>               (<code>sint</code>)           \u2013            <p>The value to wait for</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWQueue.timestamp","title":"timestamp","text":"<pre><code>timestamp(signal: SignalType)\n</code></pre> <p>Enqueues a timestamp command which records the current time in a signal after all previously enqueued commands are completed.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>SignalType</code>)           \u2013            <p>The signal to store the timestamp</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWQueue.bind","title":"bind","text":"<pre><code>bind(dev: HCQDeviceType)\n</code></pre> <p>Associates the queue with a specific device for optimized execution.</p> <p>This optional method allows backend implementations to tailor the queue for efficient use on the given device. When implemented, it can eliminate the need to copy queues into the device, thereby enhancing performance.</p> <p>Parameters:</p> <ul> <li> <code>dev</code>               (<code>HCQDeviceType</code>)           \u2013            <p>The target device for queue optimization.</p> </li> </ul> Note <p>Implementing this method is optional but recommended for performance gains.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWQueue.submit","title":"submit","text":"<pre><code>submit(\n    dev: HCQDeviceType,\n    var_vals: dict[str, int] | None = None,\n)\n</code></pre> <p>Submits the command queue to a specific device for execution.</p> <p>Parameters:</p> <ul> <li> <code>dev</code>               (<code>HCQDeviceType</code>)           \u2013            <p>The device to submit the queue to</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWQueue.memory_barrier","title":"memory_barrier","text":"<pre><code>memory_barrier()\n</code></pre> <p>Enqueues a memory barrier command to ensure memory coherence between agents. Only on compute queues.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWQueue.exec","title":"exec","text":"<pre><code>exec(\n    prg: ProgramType,\n    args_state: ArgsStateType,\n    global_size: tuple[sint, ...],\n    local_size: tuple[sint, ...],\n)\n</code></pre> <p>Enqueues an execution command for a kernel program. Only on compute queues.</p> <p>Parameters:</p> <ul> <li> <code>prg</code>               (<code>ProgramType</code>)           \u2013            <p>The program to execute</p> </li> <li> <code>args_state</code>               (<code>ArgsStateType</code>)           \u2013            <p>The args state to execute program with</p> </li> <li> <code>global_size</code>               (<code>tuple[sint, ...]</code>)           \u2013            <p>The global work size</p> </li> <li> <code>local_size</code>               (<code>tuple[sint, ...]</code>)           \u2013            <p>The local work size</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HWQueue.copy","title":"copy","text":"<pre><code>copy(dest: sint, src: sint, copy_size: int)\n</code></pre> <p>Enqueues a copy command to transfer data. Only on copy queues.</p> <p>Parameters:</p> <ul> <li> <code>dest</code>               (<code>sint</code>)           \u2013            <p>The destination of the copy</p> </li> <li> <code>src</code>               (<code>sint</code>)           \u2013            <p>The source of the copy</p> </li> <li> <code>copy_size</code>               (<code>int</code>)           \u2013            <p>The size of data to copy</p> </li> </ul>"},{"location":"developer/hcq/#hcq-compatible-device","title":"HCQ Compatible Device","text":"<p>The <code>HCQCompiled</code> class defines the API for HCQ-compatible devices. This class serves as an abstract base class that device-specific implementations should inherit from and implement.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQCompiled","title":"HCQCompiled","text":"<pre><code>HCQCompiled(\n    device: str,\n    allocator: HCQAllocatorBase,\n    compilers: Sequence[CompilerPairT],\n    runtime,\n    signal_t: Type[SignalType],\n    comp_queue_t: Callable[[], HWQueue],\n    copy_queue_t: Callable[[], HWQueue] | None = None,\n    kernargs_size=16 &lt;&lt; 20,\n    sigalloc_size=4096,\n)\n</code></pre> <p>               Bases: <code>Compiled</code>, <code>Generic[SignalType]</code></p> <p>A base class for devices compatible with the HCQ (Hardware Command Queue) API.</p>"},{"location":"developer/hcq/#signals","title":"Signals","text":"<p>Signals are device-dependent structures used for synchronization and timing in HCQ-compatible devices. They should be designed to record both a <code>value</code> and a <code>timestamp</code> within the same signal. HCQ-compatible backend implementations should use <code>HCQSignal</code> as a base class.</p> <p>The following Python code demonstrates the usage of signals:</p> <pre><code>signal = your_device.new_signal(value=0)\n\nHWQueue().timestamp(signal) \\\n         .signal(signal, value_to_fire) \\\n         .submit(your_device)\n\nsignal.wait(value_to_fire)\nsignaled_value = signal.value # should be the same as `value_to_fire`\ntimestamp = signal.timestamp\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQSignal","title":"HCQSignal","text":"<pre><code>HCQSignal(\n    base_buf: HCQBuffer,\n    value: int = 0,\n    owner: HCQDeviceType | None = None,\n    is_timeline: bool = False,\n    timestamp_divider=1000,\n)\n</code></pre> <p>               Bases: <code>Generic[HCQDeviceType]</code></p> <p>Methods:</p> <ul> <li> <code>wait</code>             \u2013              <p>Waits the signal is greater than or equal to a specific value.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>value</code>               (<code>int</code>)           \u2013            </li> <li> <code>timestamp</code>               (<code>Decimal</code>)           \u2013            <p>Get the timestamp field of the signal.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQSignal.value","title":"value  <code>property</code> <code>writable</code>","text":"<pre><code>value: int\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQSignal.timestamp","title":"timestamp  <code>property</code>","text":"<pre><code>timestamp: Decimal\n</code></pre> <p>Get the timestamp field of the signal.</p> <p>This property provides read-only access to the signal's timestamp.</p> <p>Returns:</p> <ul> <li> <code>Decimal</code>           \u2013            <p>The timestamp in microseconds.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQSignal.wait","title":"wait","text":"<pre><code>wait(\n    value: int,\n    timeout: int = getenv(\"HCQDEV_WAIT_TIMEOUT_MS\", 30000),\n)\n</code></pre> <p>Waits the signal is greater than or equal to a specific value.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>int</code>)           \u2013            <p>The value to wait for.</p> </li> <li> <code>timeout</code>               (<code>int</code>, default:                   <code>getenv('HCQDEV_WAIT_TIMEOUT_MS', 30000)</code> )           \u2013            <p>Maximum time to wait in milliseconds. Defaults to 30s.</p> </li> </ul>"},{"location":"developer/hcq/#synchronization-signals","title":"Synchronization signals","text":"<p>Each HCQ-compatible device must allocate two signals for global synchronization purposes. These signals are passed to the <code>HCQCompiled</code> base class during initialization: an active timeline signal <code>self.timeline_signal</code> and a shadow timeline signal <code>self._shadow_timeline_signal</code> which helps to handle signal value overflow issues. You can find more about synchronization in the synchronization section</p>"},{"location":"developer/hcq/#hcq-compatible-allocator","title":"HCQ Compatible Allocator","text":"<p>The <code>HCQAllocator</code> base class simplifies allocator logic by leveraging command queues abstractions. This class efficiently handles copy and transfer operations, leaving only the alloc and free functions to be implemented by individual backends.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQAllocator","title":"HCQAllocator","text":"<pre><code>HCQAllocator(\n    dev: HCQDeviceType,\n    batch_size: int = 2 &lt;&lt; 20,\n    batch_cnt: int = 32,\n    copy_bufs=None,\n    max_copyout_size: int | None = None,\n)\n</code></pre> <p>               Bases: <code>HCQAllocatorBase</code>, <code>Generic[HCQDeviceType]</code></p>"},{"location":"developer/hcq/#hcq-allocator-result-protocol","title":"HCQ Allocator Result Protocol","text":"<p>Backends must adhere to the <code>HCQBuffer</code> protocol when returning allocation results.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQBuffer","title":"HCQBuffer","text":"<pre><code>HCQBuffer(\n    va_addr: sint,\n    size: int,\n    texture_info: Any = None,\n    meta: Any = None,\n    _base: HCQBuffer | None = None,\n    view: MMIOInterface | None = None,\n    owner: HCQCompiled | None = None,\n)\n</code></pre> <p>Methods:</p> <ul> <li> <code>cpu_view</code>             \u2013              </li> <li> <code>offset</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>_mappings</code>               (<code>dict[HCQCompiled, HCQBuffer]</code>)           \u2013            </li> <li> <code>mapped_devs</code>           \u2013            </li> <li> <code>mappings</code>           \u2013            </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQBuffer._mappings","title":"_mappings  <code>instance-attribute</code>","text":"<pre><code>_mappings: dict[HCQCompiled, HCQBuffer] = {}\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQBuffer.mapped_devs","title":"mapped_devs  <code>property</code>","text":"<pre><code>mapped_devs\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQBuffer.mappings","title":"mappings  <code>property</code>","text":"<pre><code>mappings\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQBuffer.cpu_view","title":"cpu_view","text":"<pre><code>cpu_view() -&gt; MMIOInterface\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQBuffer.offset","title":"offset","text":"<pre><code>offset(\n    offset: int = 0, size: int | None = None\n) -&gt; HCQBuffer\n</code></pre>"},{"location":"developer/hcq/#hcq-compatible-program","title":"HCQ Compatible Program","text":"<p><code>HCQProgram</code> is a base class for defining programs compatible with HCQ-enabled devices. It provides a flexible framework for handling different argument layouts (see <code>HCQArgsState</code>).</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQProgram","title":"HCQProgram","text":"<pre><code>HCQProgram(\n    args_state_t: Type[HCQArgsState],\n    dev: HCQDeviceType,\n    name: str,\n    kernargs_alloc_size: int,\n    lib: bytes | None = None,\n    base: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Generic[HCQDeviceType]</code></p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Enqueues the program for execution with the given arguments and dimensions.</p> </li> <li> <code>_fini</code>             \u2013              </li> <li> <code>fill_kernargs</code>             \u2013              <p>Fills arguments for the kernel, optionally allocating space from the device if <code>kernargs_ptr</code> is not provided.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQProgram.__call__","title":"__call__","text":"<pre><code>__call__(\n    *bufs: HCQBuffer,\n    global_size: tuple[int, int, int] = (1, 1, 1),\n    local_size: tuple[int, int, int] = (1, 1, 1),\n    vals: tuple[int, ...] = (),\n    wait: bool = False\n) -&gt; float | None\n</code></pre> <p>Enqueues the program for execution with the given arguments and dimensions.</p> <p>Parameters:</p> <ul> <li> <code>bufs</code>               (<code>HCQBuffer</code>, default:                   <code>()</code> )           \u2013            <p>Buffer arguments to execute the kernel with.</p> </li> <li> <code>global_size</code>               (<code>tuple[int, int, int]</code>, default:                   <code>(1, 1, 1)</code> )           \u2013            <p>Specifies the global work size for kernel execution (equivalent to CUDA's grid size).</p> </li> <li> <code>local_size</code>               (<code>tuple[int, int, int]</code>, default:                   <code>(1, 1, 1)</code> )           \u2013            <p>Specifies the local work size for kernel execution (equivalent to CUDA's block size).</p> </li> <li> <code>vals</code>               (<code>tuple[int, ...]</code>, default:                   <code>()</code> )           \u2013            <p>Value arguments to execute the kernel with.</p> </li> <li> <code>wait</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, waits for the kernel to complete execution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float | None</code>           \u2013            <p>Execution time of the kernel if 'wait' is True, otherwise None.</p> </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQProgram._fini","title":"_fini  <code>staticmethod</code>","text":"<pre><code>_fini(dev, buf, spec)\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQProgram.fill_kernargs","title":"fill_kernargs","text":"<pre><code>fill_kernargs(\n    bufs: tuple[HCQBuffer, ...],\n    vals: tuple[int, ...] = (),\n    kernargs: HCQBuffer | None = None,\n) -&gt; HCQArgsState\n</code></pre> <p>Fills arguments for the kernel, optionally allocating space from the device if <code>kernargs_ptr</code> is not provided. Args:   bufs: Buffers to be written to kernel arguments.   vals: Values to be written to kernel arguments.   kernargs_ptr: Optional pointer to pre-allocated kernel arguments memory. Returns:   Arguments state with the given buffers and values set for the program.</p>"},{"location":"developer/hcq/#arguments-state","title":"Arguments State","text":"<p><code>HCQArgsState</code> is a base class for managing the argument state for HCQ programs. Backend implementations should create a subclass of <code>HCQArgsState</code> to manage arguments for the given program.</p> <p>Lifetime: The <code>HCQArgsState</code> is passed to <code>HWQueue.exec</code> and is guaranteed not to be freed until <code>HWQueue.submit</code> for the same queue is called.</p>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQArgsState","title":"HCQArgsState","text":"<pre><code>HCQArgsState(\n    buf: HCQBuffer,\n    prg: ProgramType,\n    bufs: tuple[HCQBuffer, ...],\n    vals: tuple[sint, ...] = (),\n)\n</code></pre> <p>               Bases: <code>Generic[ProgramType]</code></p> <p>Methods:</p> <ul> <li> <code>bind_sints_to_buf</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>bind_data</code>               (<code>list[tuple[tuple[sint, ...], MMIOInterface, str]]</code>)           \u2013            </li> </ul>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQArgsState.bind_data","title":"bind_data  <code>instance-attribute</code>","text":"<pre><code>bind_data: list[\n    tuple[tuple[sint, ...], MMIOInterface, str]\n] = []\n</code></pre>"},{"location":"developer/hcq/#tinygrad.runtime.support.hcq.HCQArgsState.bind_sints_to_buf","title":"bind_sints_to_buf","text":"<pre><code>bind_sints_to_buf(\n    *vals: sint, buf: HCQBuffer, fmt, offset=0\n)\n</code></pre>"},{"location":"developer/hcq/#synchronization","title":"Synchronization","text":"<p>HCQ-compatible devices use a global timeline signal for synchronizing all operations. This mechanism ensures proper ordering and completion of tasks across the device. By convention, <code>self.timeline_value</code> points to the next value to signal. So, to wait for all previous operations on the device to complete, wait for <code>self.timeline_value - 1</code> value. The following Python code demonstrates the typical usage of signals to synchronize execution to other operations on the device:</p> <pre><code>HWQueue().wait(your_device.timeline_signal, your_device.timeline_value - 1) \\\n         .exec(...)\n         .signal(your_device.timeline_signal, your_device.next_timeline()) \\\n         .submit(your_device)\n\n# Optionally wait for execution\nyour_device.timeline_signal.wait(your_device.timeline_value - 1)\n</code></pre>"},{"location":"developer/hcq/#hcqgraph","title":"HCQGraph","text":"<p>HCQGraph is a core feature that implements <code>GraphRunner</code> for HCQ-compatible devices. <code>HCQGraph</code> builds static <code>HWQueue</code> for all operations per device. To optimize enqueue time, only the necessary parts of the queues are updated for each run using the symbolic variables, avoiding a complete rebuild. Optionally, queues can implement a <code>bind</code> API, which allows further optimization by eliminating the need to copy the queues into the device ring.</p>"},{"location":"developer/layout/","title":"tinygrad directory layout","text":"<p>This explains the flow of a big graph down to programs.</p> <p>Directories are listed in order of how they are processed.</p>"},{"location":"developer/layout/#tinygradschedule","title":"tinygrad/schedule","text":"<p>Group UOps into kernels.</p>"},{"location":"developer/layout/#tinygrad.schedule.rangeify.get_rangeify_map","title":"get_rangeify_map","text":"<pre><code>get_rangeify_map(sink: UOp) -&gt; dict[UOp, UOp]\n</code></pre>"},{"location":"developer/layout/#tinygradcodegenopt","title":"tinygrad/codegen/opt","text":"<p>Transforms the ast into an optimized ast. This is where BEAM search and heuristics live.</p>"},{"location":"developer/layout/#tinygradcodegen","title":"tinygrad/codegen","text":"<p>Transform the optimized ast into a linearized list of UOps.</p>"},{"location":"developer/layout/#tinygrad.codegen.full_rewrite","title":"full_rewrite","text":"<pre><code>full_rewrite(\n    sink: UOp, ren: Renderer | None = None\n) -&gt; list[UOp]\n</code></pre> <p>Function to transform the Kernel UOp graph into a linearized program.</p> <p>Parameters:</p> <ul> <li> <code>sink</code>               (<code>UOp</code>)           \u2013            <p>The Ops.SINK rooting the Kernel graph.</p> </li> <li> <code>ren</code>               (<code>Renderer | None</code>, default:                   <code>None</code> )           \u2013            <p>The Renderer (can change how things are processed, fix this).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[UOp]</code>           \u2013            <p>Linear program in UOps.</p> </li> </ul>"},{"location":"developer/layout/#tinygradrenderer","title":"tinygrad/renderer","text":"<p>Transform the linearized list of UOps into a program, represented as a string.</p>"},{"location":"developer/layout/#tinygrad.renderer.Renderer","title":"Renderer","text":"<p>Methods:</p> <ul> <li> <code>render</code>             \u2013              </li> </ul>"},{"location":"developer/layout/#tinygrad.renderer.Renderer.render","title":"render","text":"<pre><code>render(uops: list[UOp]) -&gt; str\n</code></pre>"},{"location":"developer/layout/#tinygradengine","title":"tinygrad/engine","text":"<p>Abstracted high level interface to the runtimes.</p>"},{"location":"developer/layout/#tinygrad.engine.realize.get_program","title":"get_program","text":"<pre><code>get_program(\n    ast: UOp,\n    renderer: Renderer | None = None,\n    opts: list[Opt] | None = None,\n) -&gt; ProgramSpec\n</code></pre> <p>Transform an AST into a ProgramSpec. May trigger BEAM search.</p> <p>Parameters:</p> <ul> <li> <code>ast</code>               (<code>UOp</code>)           \u2013            <p>The Ops.SINK rooted AST</p> </li> <li> <code>renderer</code>               (<code>Renderer | None</code>, default:                   <code>None</code> )           \u2013            <p>The renderer used to generate the code</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ProgramSpec</code>           \u2013            <p>The ProgramSpec of the program.</p> </li> </ul>"},{"location":"developer/runtime/","title":"Runtime Overview","text":""},{"location":"developer/runtime/#overview","title":"Overview","text":"<p>A typical runtime consists of the following parts:</p> <ul> <li>Compiled</li> <li>Allocator</li> <li>Program</li> <li>Compiler</li> </ul>"},{"location":"developer/runtime/#compiled","title":"Compiled","text":"<p>The <code>Compiled</code> class is responsible for initializing and managing a device.</p>"},{"location":"developer/runtime/#tinygrad.device.Compiled","title":"Compiled","text":"<pre><code>Compiled(\n    device: str,\n    allocator: Allocator,\n    compilers: Sequence[CompilerPairT] | None,\n    runtime,\n    graph=None,\n    group_id=None,\n)\n</code></pre> <p>Methods:</p> <ul> <li> <code>synchronize</code>             \u2013              <p>Synchronize all pending operations on the device.</p> </li> </ul>"},{"location":"developer/runtime/#tinygrad.device.Compiled.synchronize","title":"synchronize","text":"<pre><code>synchronize()\n</code></pre> <p>Synchronize all pending operations on the device.</p> <p>This method ensures that all previously queued operations on the device have been completed before proceeding.</p>"},{"location":"developer/runtime/#allocator","title":"Allocator","text":"<p>The <code>Allocator</code> class is responsible for managing memory on the device. There is also a version called the <code>LRUAllocator</code>, which caches allocated buffers to optimize performance.</p>"},{"location":"developer/runtime/#tinygrad.device.Allocator","title":"Allocator","text":"<pre><code>Allocator(dev: DeviceType)\n</code></pre> <p>               Bases: <code>Generic[DeviceType]</code></p> <p>Methods:</p> <ul> <li> <code>_alloc</code>             \u2013              </li> <li> <code>_copyin</code>             \u2013              </li> <li> <code>_copyout</code>             \u2013              </li> <li> <code>_free</code>             \u2013              </li> <li> <code>alloc</code>             \u2013              </li> <li> <code>free</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>default_buffer_spec</code>               (<code>BufferSpec</code>)           \u2013            </li> <li> <code>dev</code>               (<code>DeviceType</code>)           \u2013            </li> <li> <code>supports_copy_from_disk</code>               (<code>bool</code>)           \u2013            </li> </ul>"},{"location":"developer/runtime/#tinygrad.device.Allocator.default_buffer_spec","title":"default_buffer_spec  <code>instance-attribute</code>","text":"<pre><code>default_buffer_spec: BufferSpec = BufferSpec()\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator.dev","title":"dev  <code>instance-attribute</code>","text":"<pre><code>dev: DeviceType = dev\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator.supports_copy_from_disk","title":"supports_copy_from_disk  <code>instance-attribute</code>","text":"<pre><code>supports_copy_from_disk: bool = True\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator._alloc","title":"_alloc","text":"<pre><code>_alloc(size: int, options: BufferSpec)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator._copyin","title":"_copyin","text":"<pre><code>_copyin(dest, src: memoryview)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator._copyout","title":"_copyout","text":"<pre><code>_copyout(dest: memoryview, src)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator._free","title":"_free","text":"<pre><code>_free(opaque, options: BufferSpec)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator.alloc","title":"alloc","text":"<pre><code>alloc(size: int, options: BufferSpec | None = None)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Allocator.free","title":"free","text":"<pre><code>free(opaque, size: int, options: BufferSpec | None = None)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator","title":"LRUAllocator","text":"<pre><code>LRUAllocator(dev: DeviceType)\n</code></pre> <p>               Bases: <code>Allocator</code>, <code>Generic[DeviceType]</code></p> <p>The LRU Allocator is responsible for caching buffers. It ensures that buffers are not freed until it is absolutely necessary, optimizing performance.</p> <p>Methods:</p> <ul> <li> <code>alloc</code>             \u2013              </li> <li> <code>free</code>             \u2013              </li> <li> <code>free_cache</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>cache</code>               (<code>dict[tuple[int, BufferSpec | None], Any]</code>)           \u2013            </li> </ul>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator.cache","title":"cache  <code>instance-attribute</code>","text":"<pre><code>cache: dict[tuple[int, BufferSpec | None], Any] = (\n    defaultdict(list)\n)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator.alloc","title":"alloc","text":"<pre><code>alloc(size: int, options: BufferSpec | None = None)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator.free","title":"free","text":"<pre><code>free(\n    opaque: Any,\n    size: int,\n    options: BufferSpec | None = None,\n)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.LRUAllocator.free_cache","title":"free_cache","text":"<pre><code>free_cache()\n</code></pre>"},{"location":"developer/runtime/#program","title":"Program","text":"<p>The <code>Program</code> class is created for each loaded program. It is responsible for executing the program on the device. As an example, here is a <code>CPUProgram</code> implementation which loads program and runs it.</p>"},{"location":"developer/runtime/#tinygrad.runtime.ops_cpu.CPUProgram","title":"CPUProgram","text":"<pre><code>CPUProgram(dev, name: str, lib: bytes)\n</code></pre> <p>               Bases: <code>HCQProgram</code></p> <p>Methods:</p> <ul> <li> <code>__del__</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>fxn</code>           \u2013            </li> <li> <code>mem</code>           \u2013            </li> <li> <code>rt_lib</code>           \u2013            </li> </ul> Source code in <code>tinygrad/runtime/ops_cpu.py</code> <pre><code>def __init__(self, dev, name:str, lib:bytes):\n  LVP = isinstance(dev.compiler, LVPCompiler)\n  if sys.platform == \"win32\": # mypy doesn't understand when WIN is used here\n    PAGE_EXECUTE_READWRITE, MEM_COMMIT, MEM_RESERVE = 0x40, 0x1000, 0x2000\n    ctypes.windll.kernel32.VirtualAlloc.restype = ctypes.c_void_p\n    self.mem = ctypes.windll.kernel32.VirtualAlloc(ctypes.c_void_p(0), ctypes.c_size_t(len(lib)), MEM_COMMIT | MEM_RESERVE, PAGE_EXECUTE_READWRITE)\n    ctypes.memmove(self.mem, lib, len(lib))\n    ctypes.windll.kernel32.GetCurrentProcess.restype = ctypes.c_void_p\n    proc = ctypes.windll.kernel32.GetCurrentProcess()\n    ctypes.windll.kernel32.FlushInstructionCache(ctypes.c_void_p(proc), ctypes.c_void_p(self.mem), ctypes.c_size_t(len(lib)))\n    self.fxn = ctypes.CFUNCTYPE(None)(self.mem)\n  else:\n    # On apple silicon with SPRR enabled (it always is in macos) RWX pages are unrepresentable: https://blog.svenpeter.dev/posts/m1_sprr_gxf/\n    # MAP_JIT allows us to easily flip pages from RW- to R-X and vice versa. It is a noop on intel cpus. (man pthread_jit_write_protect_np)\n    self.mem = mmap.mmap(-1, len(lib), mmap.MAP_ANON|mmap.MAP_PRIVATE|(MAP_JIT if OSX else 0), mmap.PROT_READ|mmap.PROT_WRITE|mmap.PROT_EXEC)\n\n    if OSX: unwrap(CPUProgram.rt_lib).pthread_jit_write_protect_np(False)\n    if LVP: lib = jit_loader(lib, base=ctypes.addressof(ctypes.c_void_p.from_buffer(self.mem)), link_libs=['m'])\n    self.mem.write(lib)\n    if OSX: unwrap(CPUProgram.rt_lib).pthread_jit_write_protect_np(True)\n\n    # __clear_cache isn't a normal libc function, but a compiler support routine found in libgcc_s for gcc and compiler-rt for clang.\n    # libgcc_s comes as shared library but compiler-rt is only a bunch of static library archives which we can't directly load, but fortunately\n    # it somehow found its way into libSystem on macos (likely because it used __builtin_clear_cache) and libgcc_s is ~always present on linux\n    # Using [\"name\"] instead of .name because otherwise name is getting mangled: https://docs.python.org/3.12/reference/expressions.html#index-5\n    if CPUProgram.rt_lib is not None:\n      CPUProgram.rt_lib[\"__clear_cache\"](ctypes.c_void_p(mv_address(self.mem)), ctypes.c_void_p(mv_address(self.mem) + len(lib)))\n    else:\n      # msync should be a universal POSIX way to do this\n      from tinygrad.runtime.autogen import libc\n      libc.msync(ctypes.c_void_p(mv_address(self.mem)), len(lib), libc.MS_SYNC | libc.MS_INVALIDATE)\n\n    self.fxn = ctypes.CFUNCTYPE(None)(mv_address(self.mem))\n\n  super().__init__(LVPArgsState if LVP else HCQArgsState, dev, name, kernargs_alloc_size=12+256 if LVP else 0)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.runtime.ops_cpu.CPUProgram.fxn","title":"fxn  <code>instance-attribute</code>","text":"<pre><code>fxn = CFUNCTYPE(None)(mem)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.runtime.ops_cpu.CPUProgram.mem","title":"mem  <code>instance-attribute</code>","text":"<pre><code>mem = VirtualAlloc(\n    c_void_p(0),\n    c_size_t(len(lib)),\n    MEM_COMMIT | MEM_RESERVE,\n    PAGE_EXECUTE_READWRITE,\n)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.runtime.ops_cpu.CPUProgram.rt_lib","title":"rt_lib  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rt_lib = CDLL(\n    find_library(\"System\" if OSX else \"kernel32\")\n    if (OSX or WIN)\n    else \"libgcc_s.so.1\"\n)\n</code></pre>"},{"location":"developer/runtime/#tinygrad.runtime.ops_cpu.CPUProgram.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> Source code in <code>tinygrad/runtime/ops_cpu.py</code> <pre><code>@suppress_finalizing\ndef __del__(self):\n  if sys.platform == 'win32': ctypes.windll.kernel32.VirtualFree(ctypes.c_void_p(self.mem), ctypes.c_size_t(0), 0x8000) #0x8000 - MEM_RELEASE\n</code></pre>"},{"location":"developer/runtime/#compiler","title":"Compiler","text":"<p>The <code>Compiler</code> class compiles the output from the <code>Renderer</code> and produces it in a device-specific format.</p>"},{"location":"developer/runtime/#tinygrad.device.Compiler","title":"Compiler","text":"<pre><code>Compiler(cachekey: str | None = None)\n</code></pre> <p>Methods:</p> <ul> <li> <code>compile</code>             \u2013              </li> <li> <code>compile_cached</code>             \u2013              </li> <li> <code>disassemble</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>cachekey</code>           \u2013            </li> </ul> Source code in <code>tinygrad/device.py</code> <pre><code>def __init__(self, cachekey:str|None=None): self.cachekey = None if DISABLE_COMPILER_CACHE else cachekey\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Compiler.cachekey","title":"cachekey  <code>instance-attribute</code>","text":"<pre><code>cachekey = None if DISABLE_COMPILER_CACHE else cachekey\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Compiler.compile","title":"compile","text":"<pre><code>compile(src: str) -&gt; bytes\n</code></pre> Source code in <code>tinygrad/device.py</code> <pre><code>def compile(self, src:str) -&gt; bytes: return src.encode()   # NOTE: empty compiler is the default\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Compiler.compile_cached","title":"compile_cached","text":"<pre><code>compile_cached(src: str) -&gt; bytes\n</code></pre> Source code in <code>tinygrad/device.py</code> <pre><code>def compile_cached(self, src:str) -&gt; bytes:\n  if self.cachekey is None or (lib := diskcache_get(self.cachekey, src)) is None:\n    assert not getenv(\"ASSERT_COMPILE\"), f\"tried to compile with ASSERT_COMPILE set\\n{src}\"\n    lib = self.compile(src)\n    if self.cachekey is not None: diskcache_put(self.cachekey, src, lib)\n  return lib\n</code></pre>"},{"location":"developer/runtime/#tinygrad.device.Compiler.disassemble","title":"disassemble","text":"<pre><code>disassemble(lib: bytes)\n</code></pre> Source code in <code>tinygrad/device.py</code> <pre><code>def disassemble(self, lib:bytes): pass\n</code></pre>"},{"location":"developer/speed/","title":"speed in tinygrad","text":""},{"location":"developer/speed/#overview","title":"Overview","text":"<p>Speed refers to many different things. To break it down to four, there's:</p> <ul> <li>Compile Speed (Python)</li> <li>Execution Speed (driver)</li> <li>Model Speed (scheduler)</li> <li>Kernel Speed (codegen)</li> </ul>"},{"location":"developer/speed/#compile-speed-python","title":"Compile Speed (Python)","text":"<p>This is how long the first run of your model takes. It's limited largely by the runtime of the Python doing UOp rewrites. Currently it's a bit slow, but on par with torch.compile. It gets even slower if you are using BEAM, since that's compiling many variants of each kernel.</p> <p>This will be improved by writing faster graph_rewrite, doing less graph_rewrite, and better parallelization.</p>"},{"location":"developer/speed/#execution-speed-driver","title":"Execution Speed (driver)","text":"<p>After your model is compiled, you are often using the <code>TinyJIT</code>. tinygrad has the best execution speed of any framework because it usually bypasses the GPU driver and prebuilds the command queue. It's tons faster than normal CUDA, and often even faster than CUDA Graph.</p> <p>There's very little to improve here, as this is almost never the bottleneck.</p>"},{"location":"developer/speed/#model-speed-scheduler","title":"Model Speed (scheduler)","text":"<p>The scheduler determines how operations are grouped into kernels and which Tensors are written to memory. This is currently a big bottleneck of training speed.</p> <p>The decisions are often not obvious. For example, when is it worth recomputing an arithmetic operation instead of storing and loading from memory? Example:</p> <pre><code>from tinygrad import Tensor\na = Tensor.rand(100)\nb = Tensor.rand(100)\nc = Tensor.rand(100)\nd = Tensor.rand(100)\nout1 = a+b+c\nout2 = a+b+d\nTensor.realize(out1, out2)\n</code></pre> <p>The real answer is obvious, compute both <code>out1</code> and <code>out2</code> in the same kernel. But you can't always do that. If you can't, should <code>a+b</code> first be saved to a subbuffer? Or should both the <code>out1</code> and <code>out2</code> kernels recompute <code>a+b</code>?</p> <p>In this case: with recompute (6 reads + 2 writes), no recompute (6 reads + 3 writes), so we should probably recompute. However, once you add movement ops and casts this is even harder to figure out. tinygrad doesn't yet have a systematic way to do it.</p>"},{"location":"developer/speed/#kernel-speed-codegen","title":"Kernel Speed (codegen)","text":"<p>Given that you have decided how the model ops will be grouped and what will be written to memory, kernel speed determines how fast that operation is done. This is what BEAM changes, it searches over a set of equivalent kernels which all perform the same operation and finds the one which performs the task the fastest.</p> <p>In <code>kernel.py</code> we have a set of <code>OptOps</code>, these control the parameters of the speed optimizations applied to the kernel.</p>"},{"location":"developer/speed/#memory","title":"Memory","text":"<p>The main bottleneck in most kernels is accessing memory. In a freshman algorithms class, you'll learn about cache aware matrix multiplication, and this is all forms of that. While the same math is run, the order in which you run it can have large impacts on the speed depending on if the data you are loading. OptOps will change this order.</p> <p>Memory, even cache, is often much slower than accessing the register file. The amount of times data is used in math is called the \"arithmetic intensity\". For operations like BS=1 GEMV, the arithmetic intensity is 1, but for GEMMs and convs it can be much higher. OptOps like UPCAST and UNROLL can increase this, but be careful of making them too large, as if there's too much register pressure on the GPU the warp scheduler may not be able to fit many warps, or even worse, it could be spilling to local memory.</p> <p>4090s have 1 TB/s of ram bandwidth and ~160 TFLOPS of compute, so you need to use each loaded value ~100 times. The L1 cache has around 40 TB/s of bandwidth, so in order to get full compute utilization you need to use each value ~4 times.</p> <p>A lot of work can still be done here. For example, we never copy the inputs to on chip SRAM, but this is often quite helpful for kernel speed. Also, we aren't doing a good job with L2 cache awareness (the locals handle L1 quite well)</p>"},{"location":"developer/speed/#tensor-cores","title":"Tensor Cores","text":"<p>Many accelerators have Tensor Cores / MAC arrays / systolic arrays. The main value of these is that, since they are 2-D, they create an n^2 ratio between the compute and the input data.</p> <p>GPUs use Tensor Cores instead of MAC arrays to fit better in the GPU warp paradigm. This is because the output of Tensor Cores is O(n) wrt the input, while the output of MAC arrays like the AMX is O(n^2)</p> <p>We have a simple framework in tinygrad for adding these ALU blocks and achieving good performance from them.</p>"},{"location":"developer/speed/#indexing","title":"Indexing","text":"<p>Indexing determines the address of the memory we need to load. GPUs often have less integer math resources than floating point math, so this can sometimes be the bottleneck. We have a symbolic math engine in our rewrite rules to simplify indexing before it's emitted to the kernel. Newer NVIDIA GPUs have a \"Tensor Memory Accelerator\" to assist with fast indexing, however, this is not supported in tinygrad yet.</p>"},{"location":"developer/uop/","title":"UOp","text":""},{"location":"developer/uop/#tinygrad.uop.ops.UOp","title":"UOp","text":"<pre><code>UOp(\n    op: Ops,\n    dtype: DType = void,\n    src: tuple[UOp, ...] = tuple(),\n    arg: Any = None,\n    tag: Any = None,\n)\n</code></pre> <p>               Bases: <code>MathTrait</code></p>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops","title":"Ops","text":"<p>               Bases: <code>FastEnum</code></p> <p>Attributes:</p> <ul> <li> <code>NOOP</code>           \u2013            </li> <li> <code>SINK</code>           \u2013            </li> <li> <code>UNIQUE</code>           \u2013            </li> <li> <code>DEVICE</code>           \u2013            </li> <li> <code>KERNEL</code>           \u2013            </li> <li> <code>PRECAST</code>           \u2013            </li> <li> <code>REWRITE_ERROR</code>           \u2013            </li> <li> <code>SENTINEL</code>           \u2013            </li> <li> <code>AFTER</code>           \u2013            </li> <li> <code>GROUP</code>           \u2013            </li> <li> <code>COPY</code>           \u2013            </li> <li> <code>BUFFER</code>           \u2013            </li> <li> <code>BUFFER_VIEW</code>           \u2013            </li> <li> <code>MSELECT</code>           \u2013            </li> <li> <code>MSTACK</code>           \u2013            </li> <li> <code>BUFFERIZE</code>           \u2013            </li> <li> <code>CONTIGUOUS</code>           \u2013            </li> <li> <code>CONTIGUOUS_BACKWARD</code>           \u2013            </li> <li> <code>DETACH</code>           \u2013            </li> <li> <code>FUSE</code>           \u2013            </li> <li> <code>RESHAPE</code>           \u2013            </li> <li> <code>PERMUTE</code>           \u2013            </li> <li> <code>EXPAND</code>           \u2013            </li> <li> <code>PAD</code>           \u2013            </li> <li> <code>SHRINK</code>           \u2013            </li> <li> <code>FLIP</code>           \u2013            </li> <li> <code>MULTI</code>           \u2013            </li> <li> <code>DEFINE_GLOBAL</code>           \u2013            </li> <li> <code>DEFINE_LOCAL</code>           \u2013            </li> <li> <code>DEFINE_REG</code>           \u2013            </li> <li> <code>DEFINE_VAR</code>           \u2013            </li> <li> <code>BIND</code>           \u2013            </li> <li> <code>SPECIAL</code>           \u2013            </li> <li> <code>REDUCE_AXIS</code>           \u2013            </li> <li> <code>REDUCE</code>           \u2013            </li> <li> <code>ALLREDUCE</code>           \u2013            </li> <li> <code>UNROLL</code>           \u2013            </li> <li> <code>CONTRACT</code>           \u2013            </li> <li> <code>GEP</code>           \u2013            </li> <li> <code>VECTORIZE</code>           \u2013            </li> <li> <code>CAT</code>           \u2013            </li> <li> <code>PTRCAT</code>           \u2013            </li> <li> <code>CAST</code>           \u2013            </li> <li> <code>BITCAST</code>           \u2013            </li> <li> <code>EXP2</code>           \u2013            </li> <li> <code>LOG2</code>           \u2013            </li> <li> <code>SIN</code>           \u2013            </li> <li> <code>SQRT</code>           \u2013            </li> <li> <code>RECIPROCAL</code>           \u2013            </li> <li> <code>NEG</code>           \u2013            </li> <li> <code>TRUNC</code>           \u2013            </li> <li> <code>LOAD</code>           \u2013            </li> <li> <code>STORE</code>           \u2013            </li> <li> <code>ASSIGN</code>           \u2013            </li> <li> <code>WMMA</code>           \u2013            </li> <li> <code>INDEX</code>           \u2013            </li> <li> <code>ADD</code>           \u2013            </li> <li> <code>MUL</code>           \u2013            </li> <li> <code>SHL</code>           \u2013            </li> <li> <code>SHR</code>           \u2013            </li> <li> <code>IDIV</code>           \u2013            </li> <li> <code>MAX</code>           \u2013            </li> <li> <code>MOD</code>           \u2013            </li> <li> <code>CMPLT</code>           \u2013            </li> <li> <code>CMPNE</code>           \u2013            </li> <li> <code>CMPEQ</code>           \u2013            </li> <li> <code>XOR</code>           \u2013            </li> <li> <code>OR</code>           \u2013            </li> <li> <code>AND</code>           \u2013            </li> <li> <code>THREEFRY</code>           \u2013            </li> <li> <code>SUB</code>           \u2013            </li> <li> <code>FDIV</code>           \u2013            </li> <li> <code>POW</code>           \u2013            </li> <li> <code>WHERE</code>           \u2013            </li> <li> <code>MULACC</code>           \u2013            </li> <li> <code>BARRIER</code>           \u2013            </li> <li> <code>RANGE</code>           \u2013            </li> <li> <code>IF</code>           \u2013            </li> <li> <code>END</code>           \u2013            </li> <li> <code>ENDIF</code>           \u2013            </li> <li> <code>VCONST</code>           \u2013            </li> <li> <code>CONST</code>           \u2013            </li> <li> <code>CUSTOM</code>           \u2013            </li> <li> <code>CUSTOMI</code>           \u2013            </li> </ul>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.NOOP","title":"NOOP","text":"<pre><code>NOOP = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.SINK","title":"SINK","text":"<pre><code>SINK = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.UNIQUE","title":"UNIQUE","text":"<pre><code>UNIQUE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.DEVICE","title":"DEVICE","text":"<pre><code>DEVICE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.KERNEL","title":"KERNEL","text":"<pre><code>KERNEL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.PRECAST","title":"PRECAST","text":"<pre><code>PRECAST = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.REWRITE_ERROR","title":"REWRITE_ERROR","text":"<pre><code>REWRITE_ERROR = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.SENTINEL","title":"SENTINEL","text":"<pre><code>SENTINEL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.AFTER","title":"AFTER","text":"<pre><code>AFTER = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.GROUP","title":"GROUP","text":"<pre><code>GROUP = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.COPY","title":"COPY","text":"<pre><code>COPY = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.BUFFER","title":"BUFFER","text":"<pre><code>BUFFER = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.BUFFER_VIEW","title":"BUFFER_VIEW","text":"<pre><code>BUFFER_VIEW = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.MSELECT","title":"MSELECT","text":"<pre><code>MSELECT = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.MSTACK","title":"MSTACK","text":"<pre><code>MSTACK = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.BUFFERIZE","title":"BUFFERIZE","text":"<pre><code>BUFFERIZE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CONTIGUOUS","title":"CONTIGUOUS","text":"<pre><code>CONTIGUOUS = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CONTIGUOUS_BACKWARD","title":"CONTIGUOUS_BACKWARD","text":"<pre><code>CONTIGUOUS_BACKWARD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.DETACH","title":"DETACH","text":"<pre><code>DETACH = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.FUSE","title":"FUSE","text":"<pre><code>FUSE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.RESHAPE","title":"RESHAPE","text":"<pre><code>RESHAPE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.PERMUTE","title":"PERMUTE","text":"<pre><code>PERMUTE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.EXPAND","title":"EXPAND","text":"<pre><code>EXPAND = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.PAD","title":"PAD","text":"<pre><code>PAD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.SHRINK","title":"SHRINK","text":"<pre><code>SHRINK = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.FLIP","title":"FLIP","text":"<pre><code>FLIP = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.MULTI","title":"MULTI","text":"<pre><code>MULTI = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.DEFINE_GLOBAL","title":"DEFINE_GLOBAL","text":"<pre><code>DEFINE_GLOBAL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.DEFINE_LOCAL","title":"DEFINE_LOCAL","text":"<pre><code>DEFINE_LOCAL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.DEFINE_REG","title":"DEFINE_REG","text":"<pre><code>DEFINE_REG = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.DEFINE_VAR","title":"DEFINE_VAR","text":"<pre><code>DEFINE_VAR = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.BIND","title":"BIND","text":"<pre><code>BIND = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.SPECIAL","title":"SPECIAL","text":"<pre><code>SPECIAL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.REDUCE_AXIS","title":"REDUCE_AXIS","text":"<pre><code>REDUCE_AXIS = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.REDUCE","title":"REDUCE","text":"<pre><code>REDUCE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.ALLREDUCE","title":"ALLREDUCE","text":"<pre><code>ALLREDUCE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.UNROLL","title":"UNROLL","text":"<pre><code>UNROLL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CONTRACT","title":"CONTRACT","text":"<pre><code>CONTRACT = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.GEP","title":"GEP","text":"<pre><code>GEP = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.VECTORIZE","title":"VECTORIZE","text":"<pre><code>VECTORIZE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CAT","title":"CAT","text":"<pre><code>CAT = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.PTRCAT","title":"PTRCAT","text":"<pre><code>PTRCAT = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CAST","title":"CAST","text":"<pre><code>CAST = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.BITCAST","title":"BITCAST","text":"<pre><code>BITCAST = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.EXP2","title":"EXP2","text":"<pre><code>EXP2 = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.LOG2","title":"LOG2","text":"<pre><code>LOG2 = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.SIN","title":"SIN","text":"<pre><code>SIN = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.SQRT","title":"SQRT","text":"<pre><code>SQRT = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.RECIPROCAL","title":"RECIPROCAL","text":"<pre><code>RECIPROCAL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.NEG","title":"NEG","text":"<pre><code>NEG = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.TRUNC","title":"TRUNC","text":"<pre><code>TRUNC = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.LOAD","title":"LOAD","text":"<pre><code>LOAD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.STORE","title":"STORE","text":"<pre><code>STORE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.ASSIGN","title":"ASSIGN","text":"<pre><code>ASSIGN = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.WMMA","title":"WMMA","text":"<pre><code>WMMA = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.INDEX","title":"INDEX","text":"<pre><code>INDEX = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.ADD","title":"ADD","text":"<pre><code>ADD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.MUL","title":"MUL","text":"<pre><code>MUL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.SHL","title":"SHL","text":"<pre><code>SHL = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.SHR","title":"SHR","text":"<pre><code>SHR = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.IDIV","title":"IDIV","text":"<pre><code>IDIV = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.MAX","title":"MAX","text":"<pre><code>MAX = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.MOD","title":"MOD","text":"<pre><code>MOD = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CMPLT","title":"CMPLT","text":"<pre><code>CMPLT = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CMPNE","title":"CMPNE","text":"<pre><code>CMPNE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CMPEQ","title":"CMPEQ","text":"<pre><code>CMPEQ = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.XOR","title":"XOR","text":"<pre><code>XOR = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.OR","title":"OR","text":"<pre><code>OR = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.AND","title":"AND","text":"<pre><code>AND = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.THREEFRY","title":"THREEFRY","text":"<pre><code>THREEFRY = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.SUB","title":"SUB","text":"<pre><code>SUB = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.FDIV","title":"FDIV","text":"<pre><code>FDIV = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.POW","title":"POW","text":"<pre><code>POW = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.WHERE","title":"WHERE","text":"<pre><code>WHERE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.MULACC","title":"MULACC","text":"<pre><code>MULACC = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.BARRIER","title":"BARRIER","text":"<pre><code>BARRIER = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.RANGE","title":"RANGE","text":"<pre><code>RANGE = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.IF","title":"IF","text":"<pre><code>IF = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.END","title":"END","text":"<pre><code>END = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.ENDIF","title":"ENDIF","text":"<pre><code>ENDIF = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.VCONST","title":"VCONST","text":"<pre><code>VCONST = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CONST","title":"CONST","text":"<pre><code>CONST = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CUSTOM","title":"CUSTOM","text":"<pre><code>CUSTOM = auto()\n</code></pre>"},{"location":"developer/uop/#tinygrad.uop.ops.Ops.CUSTOMI","title":"CUSTOMI","text":"<pre><code>CUSTOMI = auto()\n</code></pre>"},{"location":"tensor/","title":"Tensor","text":""},{"location":"tensor/#tinygrad.Tensor","title":"Tensor","text":"<pre><code>Tensor(\n    data: (\n        ConstType\n        | bytes\n        | list\n        | tuple\n        | UOp\n        | \"np.ndarray\"\n        | Path\n        | None\n    ),\n    device: str | tuple | list | None = None,\n    dtype: DTypeLike | None = None,\n    requires_grad: bool | None = None,\n    _force_unique: bool = False,\n)\n</code></pre> <p>               Bases: <code>MathTrait</code></p> <p>A <code>Tensor</code> is a multi-dimensional matrix containing elements of a single data type.</p> <p></p>"},{"location":"tensor/creation/","title":"Creation","text":""},{"location":"tensor/creation/#creation-basic","title":"Creation (basic)","text":""},{"location":"tensor/creation/#tinygrad.Tensor.empty","title":"empty  <code>staticmethod</code>","text":"<pre><code>empty(\n    *shape,\n    device: str | tuple[str, ...] | None = None,\n    dtype: DTypeLike | None = None,\n    **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates an empty tensor with the given shape.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.empty(2, 3)\nprint(t.shape)\n</code></pre> <pre><code>(2, 3)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef empty(*shape, device:str|tuple[str, ...]|None=None, dtype:DTypeLike|None=None, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates an empty tensor with the given shape.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.empty(2, 3)\n  print(t.shape)\n  ```\n  \"\"\"\n  dtype, shape = to_dtype(dtype) if dtype is not None else dtypes.default_float, argfix(*shape)\n  if not isinstance(size:=prod([x.vmax if isinstance(x, UOp) else x for x in shape]), int): raise ValueError(f\"size must be int {size}\")\n  # TODO: add test for multidevice tensor\n  device = tuple(canonicalize_device(d) for d in device) if isinstance(device, tuple) else canonicalize_device(device)\n  return Tensor(UOp.new_buffer(device, size, dtype), device, dtype, **kwargs).shrink(((0,prod(shape)),)).reshape(shape)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.zeros","title":"zeros  <code>staticmethod</code>","text":"<pre><code>zeros(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with zeros.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.zeros(2, 3).numpy())\n</code></pre> <pre><code>[[0. 0. 0.]\n [0. 0. 0.]]\n</code></pre> <pre><code>print(Tensor.zeros(2, 3, dtype=dtypes.int32).numpy())\n</code></pre> <pre><code>[[0 0 0]\n [0 0 0]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef zeros(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with zeros.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.zeros(2, 3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.zeros(2, 3, dtype=dtypes.int32).numpy())\n  ```\n  \"\"\"\n  return Tensor.full(argfix(*shape), 0.0, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.ones","title":"ones  <code>staticmethod</code>","text":"<pre><code>ones(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with ones.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.ones(2, 3).numpy())\n</code></pre> <pre><code>[[1. 1. 1.]\n [1. 1. 1.]]\n</code></pre> <pre><code>print(Tensor.ones(2, 3, dtype=dtypes.int32).numpy())\n</code></pre> <pre><code>[[1 1 1]\n [1 1 1]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef ones(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with ones.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(2, 3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(2, 3, dtype=dtypes.int32).numpy())\n  ```\n  \"\"\"\n  return Tensor.full(argfix(*shape), 1.0, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.full","title":"full  <code>staticmethod</code>","text":"<pre><code>full(\n    shape: tuple[sint, ...], fill_value: ConstType, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with the given value.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.full((2, 3), 42).numpy())\n</code></pre> <pre><code>[[42 42 42]\n [42 42 42]]\n</code></pre> <pre><code>print(Tensor.full((2, 3), False).numpy())\n</code></pre> <pre><code>[[False False False]\n [False False False]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef full(shape:tuple[sint, ...], fill_value:ConstType, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with the given value.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.full((2, 3), 42).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.full((2, 3), False).numpy())\n  ```\n  \"\"\"\n  return Tensor(fill_value, _force_unique=True, **kwargs).reshape((1, )*len(new_shape := argfix(shape))).expand(new_shape)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.arange","title":"arange  <code>staticmethod</code>","text":"<pre><code>arange(start, stop=None, step=1, **kwargs) -&gt; Tensor\n</code></pre> <p>Returns a 1-D tensor of size <code>ceil((stop - start) / step)</code> with values from <code>[start, stop)</code>, with spacing between values given by <code>step</code>.</p> <p>If <code>stop</code> is not specified, values are generated from <code>[0, start)</code> with the given <code>step</code>.</p> <p>If <code>stop</code> is specified, values are generated from <code>[start, stop)</code> with the given <code>step</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.arange(5).numpy())\n</code></pre> <pre><code>[0 1 2 3 4]\n</code></pre> <pre><code>print(Tensor.arange(5, 10).numpy())\n</code></pre> <pre><code>[5 6 7 8 9]\n</code></pre> <pre><code>print(Tensor.arange(5, 10, 2).numpy())\n</code></pre> <pre><code>[5 7 9]\n</code></pre> <pre><code>print(Tensor.arange(5.5, 10, 2).numpy())\n</code></pre> <pre><code>[5.5 7.5 9.5]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef arange(start, stop=None, step=1, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Returns a 1-D tensor of size `ceil((stop - start) / step)` with values from `[start, stop)`, with spacing between values given by `step`.\n\n  If `stop` is not specified, values are generated from `[0, start)` with the given `step`.\n\n  If `stop` is specified, values are generated from `[start, stop)` with the given `step`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.arange(5).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.arange(5, 10).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.arange(5, 10, 2).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.arange(5.5, 10, 2).numpy())\n  ```\n  \"\"\"\n  if stop is None: stop, start = start, 0\n  dtype = kwargs.pop(\"dtype\", dtypes.default_float if any(isinstance(x, float) for x in (start, stop, step)) else dtypes.default_int)\n  if start &lt; (dt:=to_dtype(dtype)).min or dt.max &lt; (stop-step): raise ValueError(f\"arange [{start}, {stop}) is not representable in dtype {dtype}\")\n  # NOTE: this matches numpy, torch raises RuntimeError if stop-start and step have different signs\n  if (output_len:=ceildiv(stop-start, step)) &lt;= 0: return Tensor([], dtype=dtype, **kwargs)\n  return (Tensor.full((output_len,), step, dtype=dtype, **kwargs)._cumalu(0, Ops.ADD) + (start - step)).cast(dtype)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.linspace","title":"linspace  <code>staticmethod</code>","text":"<pre><code>linspace(\n    start: int | float,\n    stop: int | float,\n    steps: int,\n    **kwargs\n) -&gt; Tensor\n</code></pre> <p>Returns a 1-D tensor of <code>steps</code> evenly spaced values from <code>start</code> to <code>stop</code>, inclusive.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <p><pre><code>print(Tensor.linspace(0, 10, 5).numpy())\n</code></pre> <pre><code>[ 0.   2.5  5.   7.5 10. ]\n</code></pre> <pre><code>print(Tensor.linspace(-1, 1, 5).numpy())\n</code></pre> <pre><code>[-1.  -0.5  0.   0.5  1. ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef linspace(start:int|float, stop:int|float, steps:int, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Returns a 1-D tensor of `steps` evenly spaced values from `start` to `stop`, inclusive.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.linspace(0, 10, 5).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.linspace(-1, 1, 5).numpy())\n  ```\n  \"\"\"\n  if steps &lt; 0: raise ValueError(\"number of steps must be non-negative\")\n  if (dtype := to_dtype(kwargs.pop(\"dtype\", dtypes.default_float))) == dtypes.bool: raise ValueError(\"linspace with bool dtype is not supported\")\n  if steps == 1: return Tensor([start], dtype=dtype, **kwargs)\n  return (start + Tensor.arange(steps, **kwargs) * ((stop - start) / (steps - 1))).cast(dtype)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.eye","title":"eye  <code>staticmethod</code>","text":"<pre><code>eye(n: int, m: int | None = None, **kwargs) -&gt; Tensor\n</code></pre> <p>Returns a 2-D tensor with <code>n</code> rows and <code>m</code> columns, with ones on the diagonal and zeros elsewhere.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>print(Tensor.eye(3).numpy())\n</code></pre> <pre><code>[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n</code></pre> <pre><code>print(Tensor.eye(2, 4).numpy())\n</code></pre> <pre><code>[[1. 0. 0. 0.]\n [0. 1. 0. 0.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef eye(n:int, m:int|None=None, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Returns a 2-D tensor with `n` rows and `m` columns, with ones on the diagonal and zeros elsewhere.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.eye(3).numpy())\n  ```\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.eye(2, 4).numpy())\n  ```\n  \"\"\"\n  if n &lt; 0 or (m is not None and m &lt; 0): raise ValueError(f\"cannot have negative {n=}, {m=}\")\n  x = Tensor.ones(n, **kwargs).diag()\n  return x if m is None else x.pad((None, (0, m-n))) if m &gt; n else x.shrink((None, (0, m)))\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.full_like","title":"full_like","text":"<pre><code>full_like(fill_value: ConstType, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the same shape as <code>self</code>, filled with the given value. If <code>dtype</code> is not specified, the dtype of <code>self</code> is used.</p> <p>You can pass in the <code>device</code> keyword argument to control device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.ones(2, 3)\nprint(Tensor.full_like(t, 42).numpy())\n</code></pre> <pre><code>[[42. 42. 42.]\n [42. 42. 42.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def full_like(self, fill_value:ConstType, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the same shape as `self`, filled with the given value.\n  If `dtype` is not specified, the dtype of `self` is used.\n\n  You can pass in the `device` keyword argument to control device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.ones(2, 3)\n  print(Tensor.full_like(t, 42).numpy())\n  ```\n  \"\"\"\n  return Tensor.full(self.shape, fill_value, dtype=kwargs.pop(\"dtype\", self.dtype), device=kwargs.pop(\"device\", self.device), **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.zeros_like","title":"zeros_like","text":"<pre><code>zeros_like(**kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the same shape as <code>self</code>, filled with zeros.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.ones(2, 3)\nprint(Tensor.zeros_like(t).numpy())\n</code></pre> <pre><code>[[0. 0. 0.]\n [0. 0. 0.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def zeros_like(self, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the same shape as `self`, filled with zeros.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.ones(2, 3)\n  print(Tensor.zeros_like(t).numpy())\n  ```\n  \"\"\"\n  return self.full_like(0, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.ones_like","title":"ones_like","text":"<pre><code>ones_like(**kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the same shape as <code>self</code>, filled with ones.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.zeros(2, 3)\nprint(Tensor.ones_like(t).numpy())\n</code></pre> <pre><code>[[1. 1. 1.]\n [1. 1. 1.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def ones_like(self, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the same shape as `self`, filled with ones.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.zeros(2, 3)\n  print(Tensor.ones_like(t).numpy())\n  ```\n  \"\"\"\n  return self.full_like(1, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#creation-external","title":"Creation (external)","text":""},{"location":"tensor/creation/#tinygrad.Tensor.from_blob","title":"from_blob  <code>staticmethod</code>","text":"<pre><code>from_blob(\n    ptr: int, shape: tuple[int, ...], **kwargs\n) -&gt; Tensor\n</code></pre> <p>Exposes the pointer as a Tensor without taking ownership of the original data. The pointer must remain valid for the entire lifetime of the created Tensor.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef from_blob(ptr:int, shape:tuple[int, ...], **kwargs) -&gt; Tensor:\n  \"\"\"\n  Exposes the pointer as a Tensor without taking ownership of the original data.\n  The pointer must remain valid for the entire lifetime of the created Tensor.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n  \"\"\"\n  r = Tensor.empty(*shape, **kwargs)\n  assert isinstance(r.device, str)\n  cast(Buffer, r.uop.buffer).allocate(external_ptr=ptr)\n  return r\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.from_url","title":"from_url  <code>staticmethod</code>","text":"<pre><code>from_url(\n    url: str, gunzip: bool = False, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a Tensor from a URL.</p> <p>This is the preferred way to access Internet resources. It currently returns a DISK Tensor, but in the future it may return an HTTP Tensor. This also will soon become lazy (when possible) and not print progress without DEBUG.</p> <p>The <code>gunzip</code> flag will gzip extract the resource and return an extracted Tensor.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef from_url(url:str, gunzip:bool=False, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a Tensor from a URL.\n\n  This is the preferred way to access Internet resources.\n  It currently returns a DISK Tensor, but in the future it may return an HTTP Tensor.\n  This also will soon become lazy (when possible) and not print progress without DEBUG.\n\n  The `gunzip` flag will gzip extract the resource and return an extracted Tensor.\n  \"\"\"\n  return Tensor(fetch(url, gunzip=gunzip), **kwargs)\n</code></pre>"},{"location":"tensor/creation/#creation-random","title":"Creation (random)","text":""},{"location":"tensor/creation/#tinygrad.Tensor.manual_seed","title":"manual_seed  <code>staticmethod</code>","text":"<pre><code>manual_seed(seed=0) -&gt; None\n</code></pre> <p>Sets the seed for random operations.</p> <p><pre><code>Tensor.manual_seed(42)\nprint(Tensor.rand(5).numpy())\nprint(Tensor.rand(5).numpy())\n</code></pre> <pre><code>[0.997  0.5899 0.2225 0.7551 0.9057]\n[0.6162 0.6213 0.9791 0.7851 0.4178]\n</code></pre> <pre><code>Tensor.manual_seed(42)  # reset to the same seed\nprint(Tensor.rand(5).numpy())\nprint(Tensor.rand(5).numpy())\n</code></pre> <pre><code>[0.997  0.5899 0.2225 0.7551 0.9057]\n[0.6162 0.6213 0.9791 0.7851 0.4178]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef manual_seed(seed=0) -&gt; None:\n  \"\"\"\n  Sets the seed for random operations.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.rand(5).numpy())\n  print(Tensor.rand(5).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)  # reset to the same seed\n  print(Tensor.rand(5).numpy())\n  print(Tensor.rand(5).numpy())\n  ```\n  \"\"\"\n  Tensor._seed, Tensor._device_seeds, Tensor._device_rng_counters = seed, {}, {}\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.rand","title":"rand  <code>staticmethod</code>","text":"<pre><code>rand(\n    *shape,\n    device: str | None = None,\n    dtype: DTypeLike | None = None,\n    contiguous: bool = True,\n    **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval <code>[0, 1)</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.rand(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0.997  0.5899 0.2225]\n [0.7551 0.9057 0.8649]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef rand(*shape, device:str|None=None, dtype:DTypeLike|None=None, contiguous:bool=True, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval `[0, 1)`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.rand(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  if not dtypes.is_float(dtype := to_dtype(dtype or dtypes.default_float)): raise ValueError(f\"rand only supports float dtypes, got {dtype}\")\n  if not all_int(shape:=argfix(*shape)) or not all(s &gt;= 0 for s in shape): raise ValueError(f\"invalid input {shape=}\")\n  if device is not None and not isinstance(device, str): raise ValueError(f\"rand only supports single device, got {device=}\")\n  device = canonicalize_device(device)\n\n  # if shape has 0, return zero tensor\n  if (numel := prod(shape)) == 0: return Tensor.zeros(shape, device=device, dtype=dtype, **kwargs)\n  num = ceildiv(numel * dtype.itemsize, 4)\n\n  # generate per device seeds and rng counter if we haven't seen this device yet\n  if device not in Tensor._device_seeds:\n    Tensor._device_seeds[device] = Tensor(\n      [int.from_bytes(hashlib.sha256(len(Tensor._device_seeds).to_bytes(4, \"big\")).digest(), \"big\"), Tensor._seed],\n      device=device, dtype=dtypes.uint32, requires_grad=False)\n    Tensor._device_rng_counters[device] = Tensor([num], device=device, dtype=dtypes.uint32, requires_grad=False)\n  # increment rng counter for devices\n  else: Tensor._device_rng_counters[device].assign(Tensor._device_rng_counters[device] + num)\n\n  # threefry random bits\n  bits_count = Tensor._device_rng_counters[device] - num\n  counts0 = (Tensor.arange(ceildiv(num, 2), device=device, dtype=dtypes.uint32, requires_grad=False)+bits_count)\n  counts1 = counts0 + ceildiv(num, 2)\n  bits = Tensor._threefry_random_bits(Tensor._device_seeds[device], counts0, counts1)[:num]\n\n  # bitcast to uint with same number of bits\n  _, nmant = dtypes.finfo(dtype)\n  uint_dtype = {1: dtypes.uint8, 2: dtypes.uint16, 4: dtypes.uint32, 8: dtypes.uint64}[dtype.itemsize]\n  bits = bits.bitcast(uint_dtype)\n  # only randomize the mantissa bits and set the exponent to 1\n  one = Tensor.ones_like(bits, device=bits.device, dtype=dtype).bitcast(uint_dtype)\n  bits = bits.rshift((dtype.itemsize * 8) - nmant).bitwise_or(one)\n  # bitcast back to the original dtype and reshape\n  out = bits.bitcast(dtype)[:numel].sub(1).reshape(shape).requires_grad_(kwargs.get(\"requires_grad\"))\n  return out.contiguous() if contiguous else out\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.rand_like","title":"rand_like","text":"<pre><code>rand_like(**kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the same shape and sharding as <code>self</code>, filled with random values from a uniform distribution over the interval <code>[0, 1)</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.ones(2, 3)\nprint(Tensor.rand_like(t).numpy())\n</code></pre> <pre><code>[[0.6213 0.9791 0.8408]\n [0.4178 0.6334 0.9325]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def rand_like(self, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the same shape and sharding as `self`, filled with random values from a uniform distribution over the interval `[0, 1)`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.ones(2, 3)\n  print(Tensor.rand_like(t).numpy())\n  ```\n  \"\"\"\n  dtype = kwargs.pop(\"dtype\", self.dtype)\n  if isinstance(self.device, tuple):\n    if kwargs.get(\"device\") is not None: raise RuntimeError(\"cannot specify `device` on `rand_like` of a multi device tensor\")\n    if self.uop.axis is None: return Tensor.rand(*self.shape, dtype=dtype, **kwargs).shard(self.device)\n    contiguous = kwargs.pop(\"contiguous\", True)\n    sharded_shape = tuple(s//len(self.device) if a==self.uop.axis else s for a,s in enumerate(self.shape))\n    rands = UOp(Ops.MSTACK, dtype=dtype,\n                src=tuple([Tensor.rand(sharded_shape, device=d, dtype=dtype, contiguous=contiguous, **kwargs).uop for d in self.device]))\n    return Tensor(UOp.multi(rands, axis=self.uop.axis), device=self.device, dtype=dtype, **kwargs)\n  return Tensor.rand(*self.shape, device=kwargs.pop(\"device\", self.device), dtype=dtype, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.randn","title":"randn  <code>staticmethod</code>","text":"<pre><code>randn(\n    *shape,\n    dtype: DTypeLike | None = None,\n    requires_grad: bool | None = None,\n    **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a normal distribution with mean <code>0</code> and standard deviation <code>1</code>. If <code>dtype</code> is not specified, the default type is used.</p> <p>You can pass in the <code>device</code> keyword argument to control device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.randn(2, 3).numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef randn(*shape, dtype:DTypeLike|None=None, requires_grad:bool|None=None, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a normal distribution with mean `0` and standard deviation `1`.\n  If `dtype` is not specified, the default type is used.\n\n  You can pass in the `device` keyword argument to control device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.randn(2, 3).numpy())\n  ```\n  \"\"\"\n  return Tensor.empty(*shape, **kwargs).randn_like(dtype=dtype, requires_grad=requires_grad)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.randn_like","title":"randn_like","text":"<pre><code>randn_like(\n    dtype: DTypeLike | None = None,\n    requires_grad: bool | None = None,\n    **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the same shape and sharding as <code>self</code>, filled with random values from a normal distribution with mean 0 and variance 1.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.ones(2, 3)\nprint(Tensor.randn_like(t).numpy())\n</code></pre> <pre><code>[[ 0.0229 -0.8954  0.415 ]\n [-1.5933  0.96   -1.2354]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def randn_like(self, dtype:DTypeLike|None=None, requires_grad:bool|None=None, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the same shape and sharding as `self`, filled with random values from a normal distribution with mean 0 and variance 1.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.ones(2, 3)\n  print(Tensor.randn_like(t).numpy())\n  ```\n  \"\"\"\n  src = self.stack(self).rand_like(**{**kwargs, \"dtype\": dtypes.float32})\n  # https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform\n  return (src[0].mul(2*math.pi).cos().mul((1 - src[1]).log().mul(-2).sqrt()).cast(dtype or self.dtype)).requires_grad_(requires_grad)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.randint","title":"randint  <code>staticmethod</code>","text":"<pre><code>randint(\n    *shape, low=0, high=10, dtype=int32, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random integer values generated uniformly from the interval <code>[low, high)</code>. If <code>dtype</code> is not specified, the default type is used.</p> <p>You can pass in the <code>device</code> keyword argument to control device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.randint(2, 3, low=5, high=10).numpy())\n</code></pre> <pre><code>[[9 7 6]\n [8 9 9]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef randint(*shape, low=0, high=10, dtype=dtypes.int32, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random integer values generated uniformly from the interval `[low, high)`.\n  If `dtype` is not specified, the default type is used.\n\n  You can pass in the `device` keyword argument to control device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.randint(2, 3, low=5, high=10).numpy())\n  ```\n  \"\"\"\n  if not isinstance(low, int) or not isinstance(high, int): raise TypeError(f\"{low=} and {high=} must be integers\")\n  dtype = to_dtype(dtype)\n  if not dtypes.is_int(dtype): raise TypeError(f\"{dtype=} must be int\")\n  return Tensor.uniform(*shape, low=low, high=high, dtype=dtype, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.randperm","title":"randperm  <code>staticmethod</code>","text":"<pre><code>randperm(\n    n: int, device=None, dtype=int32, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Returns a tensor with a random permutation of integers from <code>0</code> to <code>n-1</code>.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.randperm(6).numpy())\n</code></pre> <pre><code>[2 1 3 5 4 0]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef randperm(n:int, device=None, dtype=dtypes.int32, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with a random permutation of integers from `0` to `n-1`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.randperm(6).numpy())\n  ```\n  \"\"\"\n  return Tensor.rand(n, device=device, **kwargs).argsort().cast(dtype)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.normal","title":"normal  <code>staticmethod</code>","text":"<pre><code>normal(\n    *shape,\n    mean=0.0,\n    std=1.0,\n    requires_grad: bool | None = None,\n    **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a normal distribution with the given <code>mean</code> and standard deviation <code>std</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.normal(2, 3, mean=10, std=2).numpy())\n</code></pre> <pre><code>[[11.9557 10.9356 11.1053]\n [ 9.3423  8.289  10.5505]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef normal(*shape, mean=0.0, std=1.0, requires_grad:bool|None=None, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a normal distribution with the given `mean` and standard deviation `std`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.normal(2, 3, mean=10, std=2).numpy())\n  ```\n  \"\"\"\n  return ((std * Tensor.randn(*shape, **kwargs)) + mean).requires_grad_(requires_grad)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.uniform","title":"uniform  <code>staticmethod</code>","text":"<pre><code>uniform(\n    *shape,\n    low=0.0,\n    high=1.0,\n    dtype: DTypeLike | None = None,\n    requires_grad: bool | None = None,\n    **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval <code>[low, high)</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.uniform(2, 3, low=2, high=10).numpy())\n</code></pre> <pre><code>[[9.9763 6.7193 3.7804]\n [8.0404 9.2452 8.9191]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef uniform(*shape, low=0.0, high=1.0, dtype:DTypeLike|None=None, requires_grad:bool|None=None, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval `[low, high)`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.uniform(2, 3, low=2, high=10).numpy())\n  ```\n  \"\"\"\n  return (((high-low) * Tensor.rand(*shape, **kwargs)).cast(dtype or dtypes.default_float) + low).requires_grad_(requires_grad)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.scaled_uniform","title":"scaled_uniform  <code>staticmethod</code>","text":"<pre><code>scaled_uniform(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval <code>[-prod(shape)**-0.5, prod(shape)**-0.5)</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.scaled_uniform(2, 3).numpy())\n</code></pre> <pre><code>[[ 0.4058  0.0734 -0.2265]\n [ 0.2082  0.3312  0.2979]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef scaled_uniform(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a uniform distribution\n  over the interval `[-prod(shape)**-0.5, prod(shape)**-0.5)`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.scaled_uniform(2, 3).numpy())\n  ```\n  \"\"\"\n  return Tensor.uniform(*shape, low=-1.0, high=1.0, **kwargs).mul(prod(argfix(*shape))**-0.5)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.glorot_uniform","title":"glorot_uniform  <code>staticmethod</code>","text":"<pre><code>glorot_uniform(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.glorot_uniform(2, 3).numpy())\n</code></pre> <pre><code>[[ 1.0889  0.197  -0.6079]\n [ 0.5588  0.8887  0.7994]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef glorot_uniform(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  &lt;https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform&gt;\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.glorot_uniform(2, 3).numpy())\n  ```\n  \"\"\"\n  return Tensor.uniform(*shape, low=-1.0, high=1.0, **kwargs).mul((6/(argfix(*shape)[0]+prod(argfix(*shape)[1:])))**0.5)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.kaiming_uniform","title":"kaiming_uniform  <code>staticmethod</code>","text":"<pre><code>kaiming_uniform(\n    *shape, a: float = 0.01, **kwargs\n) -&gt; Tensor\n</code></pre> <p>https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.kaiming_uniform(2, 3).numpy())\n</code></pre> <pre><code>[[ 1.4058  0.2543 -0.7847]\n [ 0.7214  1.1473  1.032 ]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef kaiming_uniform(*shape, a:float = 0.01, **kwargs) -&gt; Tensor:\n  \"\"\"\n  &lt;https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_&gt;\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.kaiming_uniform(2, 3).numpy())\n  ```\n  \"\"\"\n  bound = math.sqrt(3.0) * math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(argfix(*shape)[1:]))\n  return Tensor.uniform(*shape, low=-bound, high=bound, **kwargs)\n</code></pre>"},{"location":"tensor/creation/#tinygrad.Tensor.kaiming_normal","title":"kaiming_normal  <code>staticmethod</code>","text":"<pre><code>kaiming_normal(*shape, a: float = 0.01, **kwargs) -&gt; Tensor\n</code></pre> <p>https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nprint(Tensor.kaiming_normal(2, 3).numpy())\n</code></pre> <pre><code>[[ 0.7984  0.3819  0.4512]\n [-0.2685 -0.6985  0.2247]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef kaiming_normal(*shape, a:float = 0.01, **kwargs) -&gt; Tensor:\n  \"\"\"\n  &lt;https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_&gt;\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  print(Tensor.kaiming_normal(2, 3).numpy())\n  ```\n  \"\"\"\n  std = math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(argfix(*shape)[1:]))\n  return Tensor.normal(*shape, mean=0.0, std=std, **kwargs)\n</code></pre>"},{"location":"tensor/elementwise/","title":"Elementwise","text":"<p>Elementwise ops operate on a per element basis. They don't change the shape of the tensor.</p>"},{"location":"tensor/elementwise/#unary-ops-math","title":"Unary Ops (math)","text":""},{"location":"tensor/elementwise/#tinygrad.Tensor.logical_not","title":"logical_not","text":"<pre><code>logical_not() -&gt; Tensor\n</code></pre> <p>Computes the logical NOT of the tensor element-wise.</p> <pre><code>print(Tensor([False, True]).logical_not().numpy())\n</code></pre> <pre><code>[ True False]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logical_not(self) -&gt; Tensor:\n  \"\"\"\n  Computes the logical NOT of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([False, True]).logical_not().numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.bool)._apply_broadcasted_uop(UOp.ne, True)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.neg","title":"neg","text":"<pre><code>neg() -&gt; Tensor\n</code></pre> <p>Negates the tensor element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).neg().numpy())\n</code></pre> <pre><code>[ 3.  2.  1. -0. -1. -2. -3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def neg(self) -&gt; Tensor:\n  \"\"\"\n  Negates the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).neg().numpy())\n  ```\n  \"\"\"\n  return self*-1 if self.dtype != dtypes.bool else self.logical_not()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.log","title":"log","text":"<pre><code>log() -&gt; Tensor\n</code></pre> <p>Computes the natural logarithm element-wise.</p> <p>See: https://en.wikipedia.org/wiki/Logarithm</p> <pre><code>print(Tensor([1., 2., 4., 8.]).log().numpy())\n</code></pre> <pre><code>[0.     0.6931 1.3863 2.0794]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def log(self) -&gt; Tensor:\n  \"\"\"\n  Computes the natural logarithm element-wise.\n\n  See: https://en.wikipedia.org/wiki/Logarithm\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 4., 8.]).log().numpy())\n  ```\n  \"\"\"\n  return self.log2()*math.log(2)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.log2","title":"log2","text":"<pre><code>log2() -&gt; Tensor\n</code></pre> <p>Computes the base-2 logarithm element-wise.</p> <p>See: https://en.wikipedia.org/wiki/Logarithm</p> <pre><code>print(Tensor([1., 2., 4., 8.]).log2().numpy())\n</code></pre> <pre><code>[0. 1. 2. 3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def log2(self) -&gt; Tensor:\n  \"\"\"\n  Computes the base-2 logarithm element-wise.\n\n  See: https://en.wikipedia.org/wiki/Logarithm\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 4., 8.]).log2().numpy())\n  ```\n  \"\"\"\n  return self.cast(least_upper_float(self.dtype))._apply_uop(UOp.log2)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.exp","title":"exp","text":"<pre><code>exp() -&gt; Tensor\n</code></pre> <p>Computes the exponential function element-wise.</p> <p>See: https://en.wikipedia.org/wiki/Exponential_function</p> <pre><code>print(Tensor([0., 1., 2., 3.]).exp().numpy())\n</code></pre> <pre><code>[ 1.      2.7183  7.3891 20.0855]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def exp(self) -&gt; Tensor:\n  \"\"\"\n  Computes the exponential function element-wise.\n\n  See: https://en.wikipedia.org/wiki/Exponential_function\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., 1., 2., 3.]).exp().numpy())\n  ```\n  \"\"\"\n  # TODO: make it generic, and same thing to log and cos\n  if self.is_floating_point(): return self.cast(least_upper_dtype(self.dtype, dtypes.float32)).mul(1/math.log(2)).exp2().cast(self.dtype)\n  # TODO: behavior when DEFAULT_FLOAT is bfloat16 and input is int32?\n  return self.mul(1/math.log(2)).exp2()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.exp2","title":"exp2","text":"<pre><code>exp2() -&gt; Tensor\n</code></pre> <p>Computes the base-2 exponential function element-wise.</p> <p>See: https://en.wikipedia.org/wiki/Exponential_function</p> <pre><code>print(Tensor([0., 1., 2., 3.]).exp2().numpy())\n</code></pre> <pre><code>[1. 2. 4. 8.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def exp2(self) -&gt; Tensor:\n  \"\"\"\n  Computes the base-2 exponential function element-wise.\n\n  See: https://en.wikipedia.org/wiki/Exponential_function\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., 1., 2., 3.]).exp2().numpy())\n  ```\n  \"\"\"\n  return self.cast(least_upper_float(self.dtype))._apply_uop(UOp.exp2)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sqrt","title":"sqrt","text":"<pre><code>sqrt() -&gt; Tensor\n</code></pre> <p>Computes the square root of the tensor element-wise.</p> <pre><code>print(Tensor([1., 2., 3., 4.]).sqrt().numpy())\n</code></pre> <pre><code>[1.     1.4142 1.7321 2.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sqrt(self) -&gt; Tensor:\n  \"\"\"\n  Computes the square root of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 3., 4.]).sqrt().numpy())\n  ```\n  \"\"\"\n  return self.cast(least_upper_float(self.dtype))._apply_uop(UOp.sqrt)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.rsqrt","title":"rsqrt","text":"<pre><code>rsqrt() -&gt; Tensor\n</code></pre> <p>Computes the reciprocal of the square root of the tensor element-wise.</p> <pre><code>print(Tensor([1., 2., 3., 4.]).rsqrt().numpy())\n</code></pre> <pre><code>[1.     0.7071 0.5774 0.5   ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def rsqrt(self) -&gt; Tensor:\n  \"\"\"\n  Computes the reciprocal of the square root of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 3., 4.]).rsqrt().numpy())\n  ```\n  \"\"\"\n  return self.sqrt().reciprocal()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sin","title":"sin","text":"<pre><code>sin() -&gt; Tensor\n</code></pre> <p>Computes the sine of the tensor element-wise.</p> <pre><code>print(Tensor([0., math.pi/2, math.pi, 3*math.pi/2, 2*math.pi]).sin().numpy())\n</code></pre> <pre><code>[ 0.  1. -0. -1.  0.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sin(self) -&gt; Tensor:\n  \"\"\"\n  Computes the sine of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., math.pi/2, math.pi, 3*math.pi/2, 2*math.pi]).sin().numpy())\n  ```\n  \"\"\"\n  return self.cast(least_upper_float(self.dtype))._apply_uop(UOp.sin)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.cos","title":"cos","text":"<pre><code>cos() -&gt; Tensor\n</code></pre> <p>Computes the cosine of the tensor element-wise.</p> <pre><code>print(Tensor([0., math.pi/2, math.pi, 3*math.pi/2, 2*math.pi]).cos().numpy())\n</code></pre> <pre><code>[ 1.0000e+00  0.0000e+00 -1.0000e+00 -2.3842e-07  1.0000e+00]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cos(self) -&gt; Tensor:\n  \"\"\"\n  Computes the cosine of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., math.pi/2, math.pi, 3*math.pi/2, 2*math.pi]).cos().numpy())\n  ```\n  \"\"\"\n  if self.is_floating_point(): return ((math.pi/2)-self.cast(least_upper_dtype(self.dtype, dtypes.float32))).sin().cast(self.dtype)\n  return ((math.pi/2)-self).sin()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.tan","title":"tan","text":"<pre><code>tan() -&gt; Tensor\n</code></pre> <p>Computes the tangent of the tensor element-wise.</p> <pre><code>print(Tensor([0., math.pi/4, math.pi/2, 3*math.pi/4, math.pi]).tan().numpy())\n</code></pre> <pre><code>[ 0.  1. inf -1.  0.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tan(self) -&gt; Tensor:\n  \"\"\"\n  Computes the tangent of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0., math.pi/4, math.pi/2, 3*math.pi/4, math.pi]).tan().numpy())\n  ```\n  \"\"\"\n  return self.sin() / self.cos()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.asin","title":"asin","text":"<pre><code>asin() -&gt; Tensor\n</code></pre> <p>Computes the inverse sine (arcsine) of the tensor element-wise.</p> <pre><code>print(Tensor([-0.9, -0.6, -0.3, 0., 0.3, 0.6, 0.9]).asin().numpy())\n</code></pre> <pre><code>[-1.1198 -0.6435 -0.3047  0.      0.3047  0.6435  1.1198]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def asin(self) -&gt; Tensor:\n  \"\"\"\n  Computes the inverse sine (arcsine) of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-0.9, -0.6, -0.3, 0., 0.3, 0.6, 0.9]).asin().numpy())\n  ```\n  \"\"\"\n  # https://personal.math.ubc.ca/~cbm/aands/page_81.htm 4.4.46\n  coefficients = [-0.0012624911, 0.0066700901, -0.0170881256, 0.0308918810, -0.0501743046, 0.0889789874, -0.2145988016, 1.5707963050]\n  x = math.pi / 2 - (1.0 - self.abs()).sqrt() * polyN(self.abs(), coefficients)\n  return self.sign() * x\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.acos","title":"acos","text":"<pre><code>acos() -&gt; Tensor\n</code></pre> <p>Computes the inverse cosine (arccosine) of the tensor element-wise.</p> <pre><code>print(Tensor([-0.9, -0.6, -0.3, 0., 0.3, 0.6, 0.9]).acos().numpy())\n</code></pre> <pre><code>[2.6906 2.2143 1.8755 1.5708 1.2661 0.9273 0.451 ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def acos(self) -&gt; Tensor:\n  \"\"\"\n  Computes the inverse cosine (arccosine) of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-0.9, -0.6, -0.3, 0., 0.3, 0.6, 0.9]).acos().numpy())\n  ```\n  \"\"\"\n  return math.pi / 2 - self.asin()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.atan","title":"atan","text":"<pre><code>atan() -&gt; Tensor\n</code></pre> <p>Computes the inverse tangent (arctan) of the tensor element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).atan().numpy())\n</code></pre> <pre><code>[-1.249  -1.1071 -0.7854  0.      0.7854  1.1071  1.249 ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def atan(self) -&gt; Tensor:\n  \"\"\"\n  Computes the inverse tangent (arctan) of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).atan().numpy())\n  ```\n  \"\"\"\n  return (self / (1 + self * self).sqrt()).asin()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.trunc","title":"trunc","text":"<pre><code>trunc() -&gt; Tensor\n</code></pre> <p>Truncates the tensor element-wise.</p> <pre><code>print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).trunc().numpy())\n</code></pre> <pre><code>[-3. -2. -1. -0.  0.  1.  2.  3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def trunc(self: Tensor) -&gt; Tensor:\n  \"\"\"\n  Truncates the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).trunc().numpy())\n  ```\n  \"\"\"\n  return self._apply_uop(UOp.trunc)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.ceil","title":"ceil","text":"<pre><code>ceil() -&gt; Tensor\n</code></pre> <p>Rounds the tensor element-wise towards positive infinity.</p> <pre><code>print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).ceil().numpy())\n</code></pre> <pre><code>[-3. -2. -1. -0.  1.  2.  3.  4.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def ceil(self: Tensor) -&gt; Tensor:\n  \"\"\"\n  Rounds the tensor element-wise towards positive infinity.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).ceil().numpy())\n  ```\n  \"\"\"\n  return (self &gt; (b := self.trunc())).where(b+1, b)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.floor","title":"floor","text":"<pre><code>floor() -&gt; Tensor\n</code></pre> <p>Rounds the tensor element-wise towards negative infinity.</p> <pre><code>print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).floor().numpy())\n</code></pre> <pre><code>[-4. -3. -2. -1.  0.  1.  2.  3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def floor(self: Tensor) -&gt; Tensor:\n  \"\"\"\n  Rounds the tensor element-wise towards negative infinity.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).floor().numpy())\n  ```\n  \"\"\"\n  return (self &lt; (b := self.trunc())).where(b-1, b)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.round","title":"round","text":"<pre><code>round() -&gt; Tensor\n</code></pre> <p>Rounds the tensor element-wise with rounding half to even.</p> <pre><code>print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).round().numpy())\n</code></pre> <pre><code>[-4. -2. -2.  0.  0.  2.  2.  4.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def round(self: Tensor) -&gt; Tensor:\n  \"\"\"\n  Rounds the tensor element-wise with rounding half to even.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5]).round().numpy())\n  ```\n  \"\"\"\n  return ((self &gt; 0) == ((b := self.trunc() / 2.0).trunc() == b)).where((self - 0.5).ceil(), (self + 0.5).floor())\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.isinf","title":"isinf","text":"<pre><code>isinf(\n    detect_positive: bool = True,\n    detect_negative: bool = True,\n) -&gt; Tensor\n</code></pre> <p>Checks the tensor element-wise to return True where the element is infinity, otherwise returns False</p> <pre><code>print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isinf().numpy())\n</code></pre> <pre><code>[False  True False  True False]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def isinf(self:Tensor, detect_positive:bool=True, detect_negative:bool=True) -&gt; Tensor:\n  \"\"\"\n  Checks the tensor element-wise to return True where the element is infinity, otherwise returns False\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isinf().numpy())\n  ```\n  \"\"\"\n  return (self == float(\"inf\")) * detect_positive + (self == float(\"-inf\")) * detect_negative\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.isnan","title":"isnan","text":"<pre><code>isnan() -&gt; Tensor\n</code></pre> <p>Checks the tensor element-wise to return True where the element is NaN, otherwise returns False</p> <pre><code>print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isnan().numpy())\n</code></pre> <pre><code>[False False False False  True]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def isnan(self:Tensor) -&gt; Tensor:\n  \"\"\"\n  Checks the tensor element-wise to return True where the element is NaN, otherwise returns False\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isnan().numpy())\n  ```\n  \"\"\"\n  return self != self\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.isfinite","title":"isfinite","text":"<pre><code>isfinite() -&gt; Tensor\n</code></pre> <p>Checks the tensor element-wise to return True where the element is finite, otherwise returns False</p> <pre><code>print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isfinite().numpy())\n</code></pre> <pre><code>[ True False  True False False]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def isfinite(self:Tensor) -&gt; Tensor:\n  \"\"\"\n  Checks the tensor element-wise to return True where the element is finite, otherwise returns False\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, float('inf'), 2, float('-inf'), float('nan')]).isfinite().numpy())\n  ```\n  \"\"\"\n  return (self.isinf()|self.isnan()).logical_not()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.lerp","title":"lerp","text":"<pre><code>lerp(end: Tensor, weight: Tensor | float) -&gt; Tensor\n</code></pre> <p>Linearly interpolates between <code>self</code> and <code>end</code> by <code>weight</code>.</p> <pre><code>print(Tensor([1., 2., 3.]).lerp(Tensor([4., 5., 6.]), 0.5).numpy())\n</code></pre> <pre><code>[2.5 3.5 4.5]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def lerp(self, end:Tensor, weight:Tensor|float) -&gt; Tensor:\n  \"\"\"\n  Linearly interpolates between `self` and `end` by `weight`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 3.]).lerp(Tensor([4., 5., 6.]), 0.5).numpy())\n  ```\n  \"\"\"\n  if self.dtype == dtypes.uint8 and isinstance(weight, Tensor):\n    w_i = (weight * (1&lt;&lt;(W_PREC:=7)) + 0.5).cast(dtypes.int16)\n    return (self+(((end - self).cast(dtypes.int8) * w_i + (1&lt;&lt;W_PREC-1)).cast(dtypes.uint16) &gt;&gt; W_PREC)).cast(dtypes.uint8)\n  return self + (end - self) * weight\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.square","title":"square","text":"<pre><code>square() -&gt; Tensor\n</code></pre> <p>Squares the tensor element-wise. Equivalent to <code>self*self</code>.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).square().numpy())\n</code></pre> <pre><code>[9. 4. 1. 0. 1. 4. 9.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def square(self) -&gt; Tensor:\n  \"\"\"\n  Squares the tensor element-wise.\n  Equivalent to `self*self`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).square().numpy())\n  ```\n  \"\"\"\n  return self*self\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.clamp","title":"clamp","text":"<pre><code>clamp(min_=None, max_=None) -&gt; Tensor\n</code></pre> <p>Clips (clamps) the values in the tensor between <code>min_</code> and <code>max_</code> element-wise. If <code>min_</code> is <code>None</code>, there is no lower bound. If <code>max_</code> is None, there is no upper bound.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).clip(-1, 1).numpy())\n</code></pre> <pre><code>[-1. -1. -1.  0.  1.  1.  1.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def clamp(self, min_=None, max_=None) -&gt; Tensor:\n  \"\"\"\n  Clips (clamps) the values in the tensor between `min_` and `max_` element-wise.\n  If `min_` is `None`, there is no lower bound. If `max_` is None, there is no upper bound.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).clip(-1, 1).numpy())\n  ```\n  \"\"\"\n  if min_ is None and max_ is None: raise RuntimeError(\"at least one of 'min_' or 'max_' must not be None\")\n  ret = self.maximum(min_) if min_ is not None else self\n  return ret.minimum(max_) if max_ is not None else ret\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.clip","title":"clip","text":"<pre><code>clip(min_=None, max_=None) -&gt; Tensor\n</code></pre> <p>Alias for <code>Tensor.clamp</code>.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def clip(self, min_=None, max_=None) -&gt; Tensor:\n  \"\"\"\n  Alias for `Tensor.clamp`.\n  \"\"\"\n  return self.clamp(min_, max_)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sign","title":"sign","text":"<pre><code>sign() -&gt; Tensor\n</code></pre> <p>Returns the sign of the tensor element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sign().numpy())\n</code></pre> <pre><code>[-1. -1. -1.  0.  1.  1.  1.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sign(self) -&gt; Tensor:\n  \"\"\"\n  Returns the sign of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sign().numpy())\n  ```\n  \"\"\"\n  return self.ne(0).where((self&lt;0).where(self.full_like(-1), self.full_like(1)), self.full_like(0)) + self*0\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.abs","title":"abs","text":"<pre><code>abs() -&gt; Tensor\n</code></pre> <p>Computes the absolute value of the tensor element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).abs().numpy())\n</code></pre> <pre><code>[3. 2. 1. 0. 1. 2. 3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def abs(self) -&gt; Tensor:\n  \"\"\"\n  Computes the absolute value of the tensor element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).abs().numpy())\n  ```\n  \"\"\"\n  return self * self.sign()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.reciprocal","title":"reciprocal","text":"<pre><code>reciprocal() -&gt; Tensor\n</code></pre> <p>Computes <code>1/x</code> element-wise.</p> <pre><code>print(Tensor([1., 2., 3., 4.]).reciprocal().numpy())\n</code></pre> <pre><code>[1.     0.5    0.3333 0.25  ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def reciprocal(self) -&gt; Tensor:\n  \"\"\"\n  Computes `1/x` element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1., 2., 3., 4.]).reciprocal().numpy())\n  ```\n  \"\"\"\n  return self.cast(least_upper_float(self.dtype))._apply_uop(UOp.reciprocal)\n</code></pre>"},{"location":"tensor/elementwise/#unary-ops-activation","title":"Unary Ops (activation)","text":""},{"location":"tensor/elementwise/#tinygrad.Tensor.relu","title":"relu","text":"<pre><code>relu() -&gt; Tensor\n</code></pre> <p>Applies the Rectified Linear Unit (ReLU) function element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).relu().numpy())\n</code></pre> <pre><code>[0. 0. 0. 0. 1. 2. 3.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def relu(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Rectified Linear Unit (ReLU) function element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).relu().numpy())\n  ```\n  \"\"\"\n  # NOTE: if you write this as self.maximum(0) the gradient is wrong, passing through half when self is 0\n  return (self&gt;0).where(self, 0)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sigmoid","title":"sigmoid","text":"<pre><code>sigmoid() -&gt; Tensor\n</code></pre> <p>Applies the Sigmoid function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Sigmoid_function</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sigmoid().numpy())\n</code></pre> <pre><code>[0.0474 0.1192 0.2689 0.5    0.7311 0.8808 0.9526]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sigmoid(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Sigmoid function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Sigmoid_function\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sigmoid().numpy())\n  ```\n  \"\"\"\n  return (1 + (self * (-1/math.log(2))).exp2()).reciprocal()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.logsigmoid","title":"logsigmoid","text":"<pre><code>logsigmoid() -&gt; Tensor\n</code></pre> <p>Applies the LogSigmoid function element-wise.</p> <ul> <li>See: https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.logsigmoid.html</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).logsigmoid().numpy())\n</code></pre> <pre><code>[-3.0486 -2.1269 -1.3133 -0.6931 -0.3133 -0.1269 -0.0486]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logsigmoid(self) -&gt; Tensor:\n  \"\"\"\n  Applies the LogSigmoid function element-wise.\n\n  - See: https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.logsigmoid.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).logsigmoid().numpy())\n  ```\n  \"\"\"\n  return -(-self).softplus()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.hardsigmoid","title":"hardsigmoid","text":"<pre><code>hardsigmoid(\n    alpha: float = 1 / 6, beta: float = 0.5\n) -&gt; Tensor\n</code></pre> <p>Applies the Hardsigmoid function element-wise. NOTE: default <code>alpha</code> and <code>beta</code> values are taken from torch</p> <ul> <li>See: https://pytorch.org/docs/stable/generated/torch.nn.functional.hardsigmoid.html</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).hardsigmoid().numpy())\n</code></pre> <pre><code>[0.     0.1667 0.3333 0.5    0.6667 0.8333 1.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def hardsigmoid(self, alpha:float=1/6, beta:float=0.5) -&gt; Tensor:\n  \"\"\"\n  Applies the Hardsigmoid function element-wise.\n  NOTE: default `alpha` and `beta` values are taken from torch\n\n  - See: https://pytorch.org/docs/stable/generated/torch.nn.functional.hardsigmoid.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).hardsigmoid().numpy())\n  ```\n  \"\"\"\n  return (alpha * self + beta).relu() - (alpha * self + beta - 1).relu()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.elu","title":"elu","text":"<pre><code>elu(alpha=1.0) -&gt; Tensor\n</code></pre> <p>Applies the Exponential Linear Unit (ELU) function element-wise.</p> <ul> <li>Paper: https://arxiv.org/abs/1511.07289v5</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).elu().numpy())\n</code></pre> <pre><code>[-0.9502 -0.8647 -0.6321  0.      1.      2.      3.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def elu(self, alpha=1.0) -&gt; Tensor:\n  \"\"\"\n  Applies the Exponential Linear Unit (ELU) function element-wise.\n\n  - Paper: https://arxiv.org/abs/1511.07289v5\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).elu().numpy())\n  ```\n  \"\"\"\n  return self.relu() - alpha*(1-self.exp()).relu()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.celu","title":"celu","text":"<pre><code>celu(alpha=1.0) -&gt; Tensor\n</code></pre> <p>Applies the Continuously differentiable Exponential Linear Unit (CELU) function element-wise.</p> <ul> <li>Paper: https://arxiv.org/abs/1704.07483</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).celu().numpy())\n</code></pre> <pre><code>[-0.9502 -0.8647 -0.6321  0.      1.      2.      3.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def celu(self, alpha=1.0) -&gt; Tensor:\n  \"\"\"\n  Applies the Continuously differentiable Exponential Linear Unit (CELU) function element-wise.\n\n  - Paper: https://arxiv.org/abs/1704.07483\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).celu().numpy())\n  ```\n  \"\"\"\n  return self.maximum(0) + (alpha * ((self / alpha).exp() - 1)).minimum(0)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.selu","title":"selu","text":"<pre><code>selu(alpha=1.67326, gamma=1.0507) -&gt; Tensor\n</code></pre> <p>Applies the Scaled Exponential Linear Unit (SELU) function element-wise.</p> <ul> <li>Paper: https://arxiv.org/abs/1706.02515v5</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).selu().numpy())\n</code></pre> <pre><code>[-1.6706 -1.5202 -1.1113  0.      1.0507  2.1014  3.1521]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def selu(self, alpha=1.67326, gamma=1.0507) -&gt; Tensor:\n  \"\"\"\n  Applies the Scaled Exponential Linear Unit (SELU) function element-wise.\n\n  - Paper: https://arxiv.org/abs/1706.02515v5\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).selu().numpy())\n  ```\n  \"\"\"\n  return gamma * (self &gt;= 0).detach().where(self, alpha * (self.exp() - 1))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.swish","title":"swish","text":"<pre><code>swish() -&gt; Tensor\n</code></pre> <p>See <code>.silu()</code></p> <ul> <li>Paper: https://arxiv.org/abs/1710.05941v1</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).swish().numpy())\n</code></pre> <pre><code>[-0.1423 -0.2384 -0.2689  0.      0.7311  1.7616  2.8577]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def swish(self) -&gt; Tensor:\n  \"\"\"\n  See `.silu()`\n\n  - Paper: https://arxiv.org/abs/1710.05941v1\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).swish().numpy())\n  ```\n  \"\"\"\n  return self * self.sigmoid()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.silu","title":"silu","text":"<pre><code>silu() -&gt; Tensor\n</code></pre> <p>Applies the Sigmoid Linear Unit (SiLU) function element-wise.</p> <ul> <li>Paper: https://arxiv.org/abs/1606.08415</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).silu().numpy())\n</code></pre> <pre><code>[-0.1423 -0.2384 -0.2689  0.      0.7311  1.7616  2.8577]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def silu(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Sigmoid Linear Unit (SiLU) function element-wise.\n\n  - Paper: https://arxiv.org/abs/1606.08415\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).silu().numpy())\n  ```\n  \"\"\"\n  return self.swish()   # The SiLU function is also known as the swish function.\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.relu6","title":"relu6","text":"<pre><code>relu6() -&gt; Tensor\n</code></pre> <p>Applies the ReLU6 function element-wise.</p> <ul> <li>Paper: https://arxiv.org/abs/1704.04861v1</li> </ul> <pre><code>print(Tensor([-9., -6., -3., 0., 3., 6., 9.]).relu6().numpy())\n</code></pre> <pre><code>[0. 0. 0. 0. 3. 6. 6.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def relu6(self) -&gt; Tensor:\n  \"\"\"\n  Applies the ReLU6 function element-wise.\n\n  - Paper: https://arxiv.org/abs/1704.04861v1\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-9., -6., -3., 0., 3., 6., 9.]).relu6().numpy())\n  ```\n  \"\"\"\n  return self.relu() - (self-6).relu()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.hardswish","title":"hardswish","text":"<pre><code>hardswish() -&gt; Tensor\n</code></pre> <p>Applies the Hardswish function element-wise.</p> <ul> <li>Paper: https://arxiv.org/abs/1905.02244v5</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).hardswish().numpy())\n</code></pre> <pre><code>[-0.     -0.3333 -0.3333  0.      0.6667  1.6667  3.    ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def hardswish(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Hardswish function element-wise.\n\n  - Paper: https://arxiv.org/abs/1905.02244v5\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).hardswish().numpy())\n  ```\n  \"\"\"\n  return self * (self+3).relu6() * (1/6)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.tanh","title":"tanh","text":"<pre><code>tanh() -&gt; Tensor\n</code></pre> <p>Applies the Hyperbolic Tangent (tanh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Tanh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).tanh().numpy())\n</code></pre> <pre><code>[-0.9951 -0.964  -0.7616  0.      0.7616  0.964   0.9951]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tanh(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Hyperbolic Tangent (tanh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Tanh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).tanh().numpy())\n  ```\n  \"\"\"\n  return 2.0 * ((2.0 * self).sigmoid()) - 1.0\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sinh","title":"sinh","text":"<pre><code>sinh() -&gt; Tensor\n</code></pre> <p>Applies the Hyperbolic Sine (sinh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Sinh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sinh().numpy())\n</code></pre> <pre><code>[-10.0179  -3.6269  -1.1752   0.       1.1752   3.6269  10.0179]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sinh(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Hyperbolic Sine (sinh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Sinh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).sinh().numpy())\n  ```\n  \"\"\"\n  return (self.exp() - self.neg().exp()) / 2\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.cosh","title":"cosh","text":"<pre><code>cosh() -&gt; Tensor\n</code></pre> <p>Applies the Hyperbolic Cosine (cosh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Cosh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).cosh().numpy())\n</code></pre> <pre><code>[10.0677  3.7622  1.5431  1.      1.5431  3.7622 10.0677]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cosh(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Hyperbolic Cosine (cosh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Hyperbolic_functions#Cosh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).cosh().numpy())\n  ```\n  \"\"\"\n  return (self.exp() + self.neg().exp()) / 2\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.atanh","title":"atanh","text":"<pre><code>atanh() -&gt; Tensor\n</code></pre> <p>Applies the Inverse Hyperbolic Tangent (atanh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#atanh</li> </ul> <pre><code>print(Tensor([-0.9, -0.6, -0.3, 0., 0.3, 0.6, 0.9]).atanh().numpy())\n</code></pre> <pre><code>[-1.4722 -0.6931 -0.3095  0.      0.3095  0.6931  1.4722]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def atanh(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Inverse Hyperbolic Tangent (atanh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#atanh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-0.9, -0.6, -0.3, 0., 0.3, 0.6, 0.9]).atanh().numpy())\n  ```\n  \"\"\"\n  return ((1 + self)/(1 - self)).log() / 2\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.asinh","title":"asinh","text":"<pre><code>asinh() -&gt; Tensor\n</code></pre> <p>Applies the Inverse Hyperbolic Sine (asinh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#asinh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).asinh().numpy())\n</code></pre> <pre><code>[-1.8184 -1.4436 -0.8814  0.      0.8814  1.4436  1.8184]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def asinh(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Inverse Hyperbolic Sine (asinh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#asinh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).asinh().numpy())\n  ```\n  \"\"\"\n  return (self + (self.square() + 1).sqrt()).log()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.acosh","title":"acosh","text":"<pre><code>acosh() -&gt; Tensor\n</code></pre> <p>Applies the Inverse Hyperbolic Cosine (acosh) function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#acosh</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).acosh().numpy())\n</code></pre> <pre><code>[   nan    nan    nan    nan 0.     1.317  1.7627]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def acosh(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Inverse Hyperbolic Cosine (acosh) function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#acosh\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).acosh().numpy())\n  ```\n  \"\"\"\n  return (self + (self.square() - 1).sqrt()).log()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.hardtanh","title":"hardtanh","text":"<pre><code>hardtanh(min_val=-1, max_val=1) -&gt; Tensor\n</code></pre> <p>Applies the Hardtanh function element-wise.</p> <pre><code>print(Tensor([-1.5, -1.0, -0.5, 0., 0.5, 1.0, 1.5]).hardtanh().numpy())\n</code></pre> <pre><code>[-1.  -1.  -0.5  0.   0.5  1.   1. ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def hardtanh(self, min_val=-1, max_val=1) -&gt; Tensor:\n  \"\"\"\n  Applies the Hardtanh function element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1.5, -1.0, -0.5, 0., 0.5, 1.0, 1.5]).hardtanh().numpy())\n  ```\n  \"\"\"\n  return self.clip(min_val, max_val)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.erf","title":"erf","text":"<pre><code>erf() -&gt; Tensor\n</code></pre> <p>Applies error function element-wise.</p> <ul> <li>Described: https://en.wikipedia.org/wiki/Error_function</li> </ul> <pre><code>print(Tensor([-1.5, -1.0, -0.5, 0., 0.5, 1.0, 1.5]).erf().numpy())\n</code></pre> <pre><code>[-0.9661 -0.8427 -0.5205  0.      0.5205  0.8427  0.9661]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def erf(self) -&gt; Tensor:\n  \"\"\"\n  Applies error function element-wise.\n\n  - Described: https://en.wikipedia.org/wiki/Error_function\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1.5, -1.0, -0.5, 0., 0.5, 1.0, 1.5]).erf().numpy())\n  ```\n  \"\"\"\n  # https://personal.math.ubc.ca/~cbm/aands/page_299.htm 7.1.26\n  t = 1.0 / (1.0 + 0.3275911 * self.abs())\n  return self.sign() * (1.0 - t * polyN(t, [1.061405429, -1.453152027, 1.421413741, -0.284496736, 0.254829592]) * (-self.square()).exp())\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.gelu","title":"gelu","text":"<pre><code>gelu() -&gt; Tensor\n</code></pre> <p>Applies the Gaussian Error Linear Unit (GELU) function element-wise.</p> <ul> <li>Paper: https://arxiv.org/abs/1606.08415v5</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).gelu().numpy())\n</code></pre> <pre><code>[-0.0036 -0.0454 -0.1588  0.      0.8412  1.9546  2.9964]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def gelu(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Gaussian Error Linear Unit (GELU) function element-wise.\n\n  - Paper: https://arxiv.org/abs/1606.08415v5\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).gelu().numpy())\n  ```\n  \"\"\"\n  return 0.5 * self * (1 + (math.sqrt(2 / math.pi) * (self + 0.044715 * self ** 3)).tanh())\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.quick_gelu","title":"quick_gelu","text":"<pre><code>quick_gelu() -&gt; Tensor\n</code></pre> <p>Applies the Sigmoid GELU approximation element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).quick_gelu().numpy())\n</code></pre> <pre><code>[-0.0181 -0.0643 -0.1542  0.      0.8458  1.9357  2.9819]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def quick_gelu(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Sigmoid GELU approximation element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).quick_gelu().numpy())\n  ```\n  \"\"\"\n  return self * (self * 1.702).sigmoid()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.leaky_relu","title":"leaky_relu","text":"<pre><code>leaky_relu(neg_slope=0.01) -&gt; Tensor\n</code></pre> <p>Applies the Leaky ReLU function element-wise.</p> <p><pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).leaky_relu().numpy())\n</code></pre> <pre><code>[-0.03 -0.02 -0.01  0.    1.    2.    3.  ]\n</code></pre> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).leaky_relu(neg_slope=0.42).numpy())\n</code></pre> <pre><code>[-1.26 -0.84 -0.42  0.    1.    2.    3.  ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def leaky_relu(self, neg_slope=0.01) -&gt; Tensor:\n  \"\"\"\n  Applies the Leaky ReLU function element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).leaky_relu().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).leaky_relu(neg_slope=0.42).numpy())\n  ```\n  \"\"\"\n  return (self&lt;0).where(neg_slope*self, self)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.mish","title":"mish","text":"<pre><code>mish() -&gt; Tensor\n</code></pre> <p>Applies the Mish function element-wise.</p> <ul> <li>Paper: https://arxiv.org/abs/1908.08681v3</li> </ul> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).mish().numpy())\n</code></pre> <pre><code>[-0.1456 -0.2525 -0.3034  0.      0.8651  1.944   2.9865]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def mish(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Mish function element-wise.\n\n  - Paper: https://arxiv.org/abs/1908.08681v3\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).mish().numpy())\n  ```\n  \"\"\"\n  return self * self.softplus().tanh()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.softplus","title":"softplus","text":"<pre><code>softplus(beta=1.0) -&gt; Tensor\n</code></pre> <p>Applies the Softplus function element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).softplus().numpy())\n</code></pre> <pre><code>[0.0486 0.1269 0.3133 0.6931 1.3133 2.1269 3.0486]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def softplus(self, beta=1.0) -&gt; Tensor:\n  \"\"\"\n  Applies the Softplus function element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).softplus().numpy())\n  ```\n  \"\"\"\n  return (1/beta) * (self*beta).logaddexp(0.0)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.softsign","title":"softsign","text":"<pre><code>softsign() -&gt; Tensor\n</code></pre> <p>Applies the Softsign function element-wise.</p> <pre><code>print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).softsign().numpy())\n</code></pre> <pre><code>[-0.75   -0.6667 -0.5     0.      0.5     0.6667  0.75  ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def softsign(self) -&gt; Tensor:\n  \"\"\"\n  Applies the Softsign function element-wise.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-3., -2., -1., 0., 1., 2., 3.]).softsign().numpy())\n  ```\n  \"\"\"\n  return self / (1 + self.abs())\n</code></pre>"},{"location":"tensor/elementwise/#elementwise-ops-broadcasted","title":"Elementwise Ops (broadcasted)","text":""},{"location":"tensor/elementwise/#tinygrad.Tensor.add","title":"add","text":"<pre><code>add(x: TMT | ConstType, reverse: bool = False)\n</code></pre> <p>Adds <code>self</code> and <code>x</code>. Equivalent to <code>self + x</code>. Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs. <pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(4)\nprint(t.numpy())\n</code></pre> <pre><code>[-0.5144  1.085   0.9089 -0.0841]\n</code></pre> <pre><code>print(t.add(20).numpy())\n</code></pre> <pre><code>[19.4856 21.085  20.9089 19.9159]\n</code></pre> <pre><code>print(t.add(Tensor([[2.0], [3.5]])).numpy())\n</code></pre> <pre><code>[[1.4856 3.085  2.9089 1.9159]\n [2.9856 4.585  4.4089 3.4159]]\n</code></pre></p> Source code in <code>tinygrad/uop/mathtraits.py</code> <pre><code>def add(self:TMT, x:TMT|ConstType, reverse:bool=False):\n  \"\"\"\n  Adds `self` and `x`.\n  Equivalent to `self + x`.\n  Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.add(20).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.add(Tensor([[2.0], [3.5]])).numpy())\n  ```\n  \"\"\"\n  return self._binop(Ops.ADD, x, reverse)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.sub","title":"sub","text":"<pre><code>sub(x: Tensor | ConstType, reverse=False) -&gt; Tensor\n</code></pre> <p>Subtracts <code>x</code> from <code>self</code>. Equivalent to <code>self - x</code>. Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(4)\nprint(t.numpy())\n</code></pre> <pre><code>[-0.5144  1.085   0.9089 -0.0841]\n</code></pre> <pre><code>print(t.sub(20).numpy())\n</code></pre> <pre><code>[-20.5144 -18.915  -19.0911 -20.0841]\n</code></pre> <pre><code>print(t.sub(Tensor([[2.0], [3.5]])).numpy())\n</code></pre> <pre><code>[[-2.5144 -0.915  -1.0911 -2.0841]\n [-4.0144 -2.415  -2.5911 -3.5841]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sub(self, x:Tensor|ConstType, reverse=False) -&gt; Tensor:\n  \"\"\"\n  Subtracts `x` from `self`.\n  Equivalent to `self - x`.\n  Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sub(20).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sub(Tensor([[2.0], [3.5]])).numpy())\n  ```\n  \"\"\"\n  a, b = self._broadcasted(x, reverse)\n  return a + (-b)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.mul","title":"mul","text":"<pre><code>mul(x: TMT | ConstType, reverse: bool = False)\n</code></pre> <p>Multiplies <code>self</code> and <code>x</code>. Equivalent to <code>self * x</code>. Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(4)\nprint(t.numpy())\n</code></pre> <pre><code>[-0.5144  1.085   0.9089 -0.0841]\n</code></pre> <pre><code>print(t.mul(3).numpy())\n</code></pre> <pre><code>[-1.5431  3.2549  2.7267 -0.2523]\n</code></pre> <pre><code>print(t.mul(Tensor([[-1.0], [2.0]])).numpy())\n</code></pre> <pre><code>[[ 0.5144 -1.085  -0.9089  0.0841]\n [-1.0287  2.17    1.8178 -0.1682]]\n</code></pre></p> Source code in <code>tinygrad/uop/mathtraits.py</code> <pre><code>def mul(self:TMT, x:TMT|ConstType, reverse:bool=False):\n  \"\"\"\n  Multiplies `self` and `x`.\n  Equivalent to `self * x`.\n  Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mul(3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mul(Tensor([[-1.0], [2.0]])).numpy())\n  ```\n  \"\"\"\n  return self._binop(Ops.MUL, x, reverse)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.div","title":"div","text":"<pre><code>div(\n    x: Tensor | ConstType,\n    reverse=False,\n    rounding_mode: Literal[\"trunc\", \"floor\"] | None = None,\n) -&gt; Tensor\n</code></pre> <p>Divides <code>self</code> by <code>x</code>. Equivalent to <code>self / x</code>. Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs. <code>div</code> performs true division.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(4)\nprint(t.numpy())\n</code></pre> <pre><code>[-0.5144  1.085   0.9089 -0.0841]\n</code></pre> <pre><code>print(t.div(3).numpy())\n</code></pre> <pre><code>[-0.1715  0.3617  0.303  -0.028 ]\n</code></pre> <pre><code>print(Tensor([1, 4, 10]).div(Tensor([2, 3, 4])).numpy())\n</code></pre> <pre><code>[0.5    1.3333 2.5   ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def div(self, x:Tensor|ConstType, reverse=False, rounding_mode:Literal[\"trunc\", \"floor\"]|None=None) -&gt; Tensor:\n  \"\"\"\n  Divides `self` by `x`.\n  Equivalent to `self / x`.\n  Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.\n  `div` performs true division.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.div(3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, 4, 10]).div(Tensor([2, 3, 4])).numpy())\n  ```\n  \"\"\"\n  numerator, denominator = self._broadcasted(x, reverse)\n  d = numerator.cast(least_upper_float(numerator.dtype)) * denominator.cast(least_upper_float(denominator.dtype)).reciprocal()\n  output_dtype = numerator.dtype if dtypes.is_int(numerator.dtype) else d.dtype\n  if dtypes.is_int(dt:=least_upper_dtype(numerator.dtype, denominator.dtype)) and rounding_mode is not None:\n    numerator, denominator = numerator.cast(dt), denominator.cast(dt)\n    if rounding_mode == \"trunc\": return numerator.idiv(denominator)\n    if rounding_mode == \"floor\":\n      truncate_div, truncate_mod = numerator.idiv(denominator), numerator._apply_broadcasted_uop(UOp.mod, denominator)\n      opposite_sign = ((numerator&gt;0)&amp;(denominator&lt;0)) | ((numerator&lt;0)&amp;(denominator&gt;0))\n      return (opposite_sign&amp;(truncate_mod!=0)).where(truncate_div-1, truncate_div)\n  if rounding_mode == \"trunc\": return d.trunc().cast(output_dtype)\n  if rounding_mode == \"floor\": return d.floor().cast(output_dtype)\n  if rounding_mode is not None: raise RuntimeError(f\"{rounding_mode=} is not supported\")\n  return d\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.idiv","title":"idiv","text":"<pre><code>idiv(x: TMT | ConstType, reverse: bool = False)\n</code></pre> <p>Divides <code>self</code> by <code>x</code>. Equivalent to <code>self // x</code>. Supports broadcasting to a common shape, type promotion, and integer inputs. <code>idiv</code> performs integer division (truncate towards zero).</p> <pre><code>print(Tensor([-4, 7, 5, 4, -7, 8]).idiv(Tensor([2, -3, 8, -2, 3, 5])).numpy())\n</code></pre> <pre><code>[-2 -2  0 -2 -2  1]\n</code></pre> Source code in <code>tinygrad/uop/mathtraits.py</code> <pre><code>def idiv(self:TMT, x:TMT|ConstType, reverse:bool=False):\n  \"\"\"\n  Divides `self` by `x`.\n  Equivalent to `self // x`.\n  Supports broadcasting to a common shape, type promotion, and integer inputs.\n  `idiv` performs integer division (truncate towards zero).\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-4, 7, 5, 4, -7, 8]).idiv(Tensor([2, -3, 8, -2, 3, 5])).numpy())\n  ```\n  \"\"\"\n  return self._binop(Ops.IDIV, x, reverse)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.mod","title":"mod","text":"<pre><code>mod(x: Tensor | ConstType, reverse=False) -&gt; Tensor\n</code></pre> <p>Mod <code>self</code> by <code>x</code>. Equivalent to <code>self % x</code>. Supports broadcasting to a common shape, type promotion, and integer inputs.</p> <pre><code>print(Tensor([-4, 7, 5, 4, -7, 8]).mod(Tensor([2, -3, 8, -2, 3, 5])).numpy())\n</code></pre> <pre><code>[ 0 -2  5  0  2  3]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def mod(self, x:Tensor|ConstType, reverse=False) -&gt; Tensor:\n  \"\"\"\n  Mod `self` by `x`.\n  Equivalent to `self % x`.\n  Supports broadcasting to a common shape, type promotion, and integer inputs.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-4, 7, 5, 4, -7, 8]).mod(Tensor([2, -3, 8, -2, 3, 5])).numpy())\n  ```\n  \"\"\"\n  a, b = self._broadcasted(x, reverse)\n  return a - a.div(b, rounding_mode=\"floor\") * b\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.bitwise_xor","title":"bitwise_xor","text":"<pre><code>bitwise_xor(x: TMT | ConstType, reverse: bool = False)\n</code></pre> <p>Computes bitwise xor of <code>self</code> and <code>x</code>. Equivalent to <code>self ^ x</code>. Supports broadcasting to a common shape, type promotion, and integer, boolean inputs.</p> <p><pre><code>print(Tensor([-1, -2, 3]).bitwise_xor(Tensor([1, 0, 3])).numpy())\n</code></pre> <pre><code>[-2 -2  0]\n</code></pre> <pre><code>print(Tensor([True, True, False, False]).bitwise_xor(Tensor([True, False, True, False])).numpy())\n</code></pre> <pre><code>[False  True  True False]\n</code></pre></p> Source code in <code>tinygrad/uop/mathtraits.py</code> <pre><code>def bitwise_xor(self:TMT, x:TMT|ConstType, reverse:bool=False):\n  \"\"\"\n  Computes bitwise xor of `self` and `x`.\n  Equivalent to `self ^ x`.\n  Supports broadcasting to a common shape, type promotion, and integer, boolean inputs.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, -2, 3]).bitwise_xor(Tensor([1, 0, 3])).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([True, True, False, False]).bitwise_xor(Tensor([True, False, True, False])).numpy())\n  ```\n  \"\"\"\n  self._check_dtype()\n  return self._binop(Ops.XOR, x, reverse)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.bitwise_and","title":"bitwise_and","text":"<pre><code>bitwise_and(x: TMT | ConstType, reverse: bool = False)\n</code></pre> <p>Computes the bitwise AND of <code>self</code> and <code>x</code>. Equivalent to <code>self &amp; x</code>. Supports broadcasting to a common shape, type promotion, and integer, boolean inputs. <pre><code>print(Tensor([2, 5, 255]).bitwise_and(Tensor([3, 14, 16])).numpy())\n</code></pre> <pre><code>[ 2  4 16]\n</code></pre> <pre><code>print(Tensor([True, True, False, False]).bitwise_and(Tensor([True, False, True, False])).numpy())\n</code></pre> <pre><code>[ True False False False]\n</code></pre></p> Source code in <code>tinygrad/uop/mathtraits.py</code> <pre><code>def bitwise_and(self:TMT, x:TMT|ConstType, reverse:bool=False):\n  \"\"\"\n  Computes the bitwise AND of `self` and `x`.\n  Equivalent to `self &amp; x`.\n  Supports broadcasting to a common shape, type promotion, and integer, boolean inputs.\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([2, 5, 255]).bitwise_and(Tensor([3, 14, 16])).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([True, True, False, False]).bitwise_and(Tensor([True, False, True, False])).numpy())\n  ```\n  \"\"\"\n  self._check_dtype()\n  return self._binop(Ops.AND, x, reverse)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.bitwise_or","title":"bitwise_or","text":"<pre><code>bitwise_or(x: TMT | ConstType, reverse: bool = False)\n</code></pre> <p>Computes the bitwise OR of <code>self</code> and <code>x</code>. Equivalent to <code>self | x</code>. Supports broadcasting to a common shape, type promotion, and integer, boolean inputs. <pre><code>print(Tensor([2, 5, 255]).bitwise_or(Tensor([4, 4, 4])).numpy())\n</code></pre> <pre><code>[  6   5 255]\n</code></pre> <pre><code>print(Tensor([True, True, False, False]).bitwise_or(Tensor([True, False, True, False])).numpy())\n</code></pre> <pre><code>[ True  True  True False]\n</code></pre></p> Source code in <code>tinygrad/uop/mathtraits.py</code> <pre><code>def bitwise_or(self:TMT, x:TMT|ConstType, reverse:bool=False):\n  \"\"\"\n  Computes the bitwise OR of `self` and `x`.\n  Equivalent to `self | x`.\n  Supports broadcasting to a common shape, type promotion, and integer, boolean inputs.\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([2, 5, 255]).bitwise_or(Tensor([4, 4, 4])).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([True, True, False, False]).bitwise_or(Tensor([True, False, True, False])).numpy())\n  ```\n  \"\"\"\n  self._check_dtype()\n  return self._binop(Ops.OR, x, reverse)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.bitwise_not","title":"bitwise_not","text":"<pre><code>bitwise_not() -&gt; Tensor\n</code></pre> <p>Computes the bitwise NOT of <code>self</code>. Equivalent to <code>~self</code>. <pre><code>print(Tensor([0, 2, 5, 255], dtype=\"int8\").bitwise_not().numpy())\n</code></pre> <pre><code>[-1 -3 -6  0]\n</code></pre> <pre><code>print(Tensor([True, False]).bitwise_not().numpy())\n</code></pre> <pre><code>[False  True]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def bitwise_not(self) -&gt; Tensor:\n  \"\"\"\n  Computes the bitwise NOT of `self`.\n  Equivalent to `~self`.\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([0, 2, 5, 255], dtype=\"int8\").bitwise_not().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([True, False]).bitwise_not().numpy())\n  ```\n  \"\"\"\n  if self.dtype != dtypes.bool and not dtypes.is_int(self.dtype): raise RuntimeError(f\"{self.dtype} is not supported\")\n  return self.logical_not() if self.dtype == dtypes.bool else self ^ -1\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.lshift","title":"lshift","text":"<pre><code>lshift(x: Tensor | int, reverse=False) -&gt; Tensor\n</code></pre> <p>Computes left arithmetic shift of <code>self</code> by <code>x</code> bits. <code>self</code> must have unsigned dtype. Equivalent to <code>self &lt;&lt; x</code>.</p> <pre><code>print(Tensor([1, 3, 31], dtype=dtypes.uint8).lshift(2).numpy())\n</code></pre> <pre><code>[  4  12 124]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def lshift(self, x:Tensor|int, reverse=False) -&gt; Tensor:\n  \"\"\"\n  Computes left arithmetic shift of `self` by `x` bits. `self` must have unsigned dtype.\n  Equivalent to `self &lt;&lt; x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, 3, 31], dtype=dtypes.uint8).lshift(2).numpy())\n  ```\n  \"\"\"\n  assert dtypes.is_unsigned(self.dtype) and isinstance(x, int) and x &gt;= 0 and not reverse, f\"not supported {self.dtype=} {x=}\"\n  return self.mul(2 ** x, reverse)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.rshift","title":"rshift","text":"<pre><code>rshift(x: Tensor | int, reverse=False) -&gt; Tensor\n</code></pre> <p>Computes right arithmetic shift of <code>self</code> by <code>x</code> bits. <code>self</code> must have unsigned dtype. Equivalent to <code>self &gt;&gt; x</code>.</p> <pre><code>print(Tensor([4, 13, 125], dtype=dtypes.uint8).rshift(2).numpy())\n</code></pre> <pre><code>[ 1  3 31]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def rshift(self, x:Tensor|int, reverse=False) -&gt; Tensor:\n  \"\"\"\n  Computes right arithmetic shift of `self` by `x` bits. `self` must have unsigned dtype.\n  Equivalent to `self &gt;&gt; x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([4, 13, 125], dtype=dtypes.uint8).rshift(2).numpy())\n  ```\n  \"\"\"\n  assert dtypes.is_unsigned(self.dtype) and isinstance(x, int) and x &gt;= 0 and not reverse, f\"not supported {self.dtype=} {x=}\"\n  return self.idiv(2 ** x, reverse)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.pow","title":"pow","text":"<pre><code>pow(x: Tensor | ConstType, reverse=False) -&gt; Tensor\n</code></pre> <p>Computes power of <code>self</code> with <code>x</code>. Equivalent to <code>self ** x</code>.</p> <p><pre><code>print(Tensor([-1, 2, 3]).pow(2.0).numpy())\n</code></pre> <pre><code>[1 4 9]\n</code></pre> <pre><code>print(Tensor([-1, 2, 3]).pow(Tensor([-1.5, 0.5, 1.5])).numpy())\n</code></pre> <pre><code>[-2147483648           1           5]\n</code></pre> <pre><code>print((2.0 ** Tensor([-1, 2, 3])).numpy())\n</code></pre> <pre><code>[0.5 4.  8. ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def pow(self, x:Tensor|ConstType, reverse=False) -&gt; Tensor:\n  \"\"\"\n  Computes power of `self` with `x`.\n  Equivalent to `self ** x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).pow(2.0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).pow(Tensor([-1.5, 0.5, 1.5])).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print((2.0 ** Tensor([-1, 2, 3])).numpy())\n  ```\n  \"\"\"\n  base, exponent = self._broadcasted(x, reverse=reverse)\n  # TODO: int pow\n  if not base.is_floating_point() and not (isinstance(x, int) and x &gt;= 0): raise RuntimeError(\"base needs to be float\")\n\n  ret = base._apply_uop(UOp.pow, exponent)\n  # NOTE: pow(int, float) -&gt; int\n  return ret.round().cast(self.dtype) if not reverse and not dtypes.is_float(self.dtype) and dtypes.is_float(exponent.dtype) else ret\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.maximum","title":"maximum","text":"<pre><code>maximum(x: Tensor | ConstType) -&gt; Tensor\n</code></pre> <p>Computes element-wise maximum of <code>self</code> and <code>x</code>.</p> <p><pre><code>print(Tensor([-1, 2, 3]).maximum(1).numpy())\n</code></pre> <pre><code>[1 2 3]\n</code></pre> <pre><code>print(Tensor([-1, 2, 3]).maximum(Tensor([-4, -2, 9])).numpy())\n</code></pre> <pre><code>[-1  2  9]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def maximum(self, x:Tensor|ConstType) -&gt; Tensor:\n  \"\"\"\n  Computes element-wise maximum of `self` and `x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).maximum(1).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).maximum(Tensor([-4, -2, 9])).numpy())\n  ```\n  \"\"\"\n  return self._apply_broadcasted_uop(UOp.maximum, x)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.minimum","title":"minimum","text":"<pre><code>minimum(x: Tensor | ConstType) -&gt; Tensor\n</code></pre> <p>Computes element-wise minimum of <code>self</code> and <code>x</code>.</p> <p><pre><code>print(Tensor([-1, 2, 3]).minimum(1).numpy())\n</code></pre> <pre><code>[-1  1  1]\n</code></pre> <pre><code>print(Tensor([-1, 2, 3]).minimum(Tensor([-4, -2, 9])).numpy())\n</code></pre> <pre><code>[-4 -2  3]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def minimum(self, x:Tensor|ConstType) -&gt; Tensor:\n  \"\"\"\n  Computes element-wise minimum of `self` and `x`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).minimum(1).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([-1, 2, 3]).minimum(Tensor([-4, -2, 9])).numpy())\n  ```\n  \"\"\"\n  t, x = self._broadcasted(x)\n  return t._inverse().maximum(x._inverse())._inverse()\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.where","title":"where","text":"<pre><code>where(\n    x: Tensor | ConstType | sint,\n    y: Tensor | ConstType | sint,\n) -&gt; Tensor\n</code></pre> <p>Returns a tensor of elements selected from either <code>x</code> or <code>y</code>, depending on <code>self</code>. <code>output_i = x_i if self_i else y_i</code>.</p> <p><pre><code>cond = Tensor([[True, True, False], [True, False, False]])\nprint(cond.where(1, 3).numpy())\n</code></pre> <pre><code>[[1 1 3]\n [1 3 3]]\n</code></pre> <pre><code>Tensor.manual_seed(42)\ncond = Tensor.randn(2, 3)\nprint(cond.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print((cond &gt; 0).where(cond, -float(\"inf\")).numpy())\n</code></pre> <pre><code>[[0.9779 0.4678 0.5526]\n [  -inf   -inf 0.2753]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def where(self:Tensor, x:Tensor|ConstType|sint, y:Tensor|ConstType|sint) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor of elements selected from either `x` or `y`, depending on `self`.\n  `output_i = x_i if self_i else y_i`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  cond = Tensor([[True, True, False], [True, False, False]])\n  print(cond.where(1, 3).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  cond = Tensor.randn(2, 3)\n  print(cond.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print((cond &gt; 0).where(cond, -float(\"inf\")).numpy())\n  ```\n  \"\"\"\n  if isinstance(x, Tensor): x, y = x._broadcasted(y)\n  elif isinstance(y, Tensor): y, x = y._broadcasted(x)\n  cond, x = self._broadcasted(x, match_dtype=False)\n  cond, y = cond._broadcasted(y, match_dtype=False)\n  return cond.cast(dtypes.bool)._apply_uop(UOp.where, *x._broadcasted(y))\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.copysign","title":"copysign","text":"<pre><code>copysign(other) -&gt; Tensor\n</code></pre> <p>Returns a tensor of with the magnitude of <code>self</code> and the sign of <code>other</code>, elementwise.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def copysign(self, other) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor of with the magnitude of `self` and the sign of `other`, elementwise.\n  \"\"\"\n  # NOTE: torch always return in float, we return based on the broadcasting rule.\n  other = self._broadcasted(other)[1]\n  # TODO: remove other*0?\n  return (other &lt; 0).where(-self.abs(), self.abs()) + other*0\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.logaddexp","title":"logaddexp","text":"<pre><code>logaddexp(other) -&gt; Tensor\n</code></pre> <p>Calculates (self.exp()+other.exp()).log(), elementwise.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logaddexp(self, other) -&gt; Tensor:\n  \"\"\"\n  Calculates (self.exp()+other.exp()).log(), elementwise.\n  \"\"\"\n  m = self.maximum(other)\n  return ((self-m).exp() + (self._broadcasted(other)[1]-m).exp()).log() + m\n</code></pre>"},{"location":"tensor/elementwise/#casting-ops","title":"Casting Ops","text":""},{"location":"tensor/elementwise/#tinygrad.Tensor.cast","title":"cast","text":"<pre><code>cast(dtype: DTypeLike) -&gt; Tensor\n</code></pre> <p>Casts <code>self</code> to the given <code>dtype</code>.</p> <p><pre><code>t = Tensor([-1, 2.5, 3], dtype=dtypes.float)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.float [-1.   2.5  3. ]\n</code></pre> <pre><code>t = t.cast(dtypes.int32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  2  3]\n</code></pre> <pre><code>t = t.cast(dtypes.uint8)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.uchar [255   2   3]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cast(self, dtype:DTypeLike) -&gt; Tensor:\n  \"\"\"\n  Casts `self` to the given `dtype`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2.5, 3], dtype=dtypes.float)\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.cast(dtypes.int32)\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.cast(dtypes.uint8)\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  if (dt:=to_dtype(dtype)) in {dtypes.uint8, dtypes.uint16} and dtypes.is_float(self.dtype):\n    # NOTE: values within the int32 range and outside the unsigned dtype range will cause values to wrap around\n    return self._apply_uop(UOp.cast, dtype=dtypes.int32)._apply_uop(UOp.cast, dtype=dt)\n  return self if self.dtype == dt else self._apply_uop(UOp.cast, dtype=dt)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.bitcast","title":"bitcast","text":"<pre><code>bitcast(dtype: DTypeLike) -&gt; Tensor\n</code></pre> <p>Bitcasts <code>self</code> to the given <code>dtype</code> of the same itemsize.</p> <p><code>self</code> must not require a gradient.</p> <p><pre><code>t = Tensor([-1, 2, 3], dtype=dtypes.int32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  2  3]\n</code></pre> <pre><code>t = t.bitcast(dtypes.uint32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.uint [4294967295          2          3]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def bitcast(self, dtype:DTypeLike) -&gt; Tensor:\n  \"\"\"\n  Bitcasts `self` to the given `dtype` of the same itemsize.\n\n  `self` must not require a gradient.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2, 3], dtype=dtypes.int32)\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.bitcast(dtypes.uint32)\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  if self.requires_grad: raise RuntimeError(\"can't backprop through bitcast\")\n  dt = to_dtype(dtype)\n  if (ns:=dt.itemsize) != (os:=self.dtype.itemsize) and (self.shape[-1]*os) % ns != 0: raise RuntimeError(\"unsupported size in bitcast\")\n  if (not isinstance(self.device, str) or not self.device.startswith(\"DISK\")) and ns != os:\n    new_uint, old_uint = to_dtype(f\"uint{8*ns}\"), to_dtype(f\"uint{8*os}\")\n    tmp = self.bitcast(old_uint)\n    if ns &gt; os:\n      tmp = tmp.reshape(self.shape[:-1] + (self.shape[-1]//(rate := ns//os), rate))\n      nones = (None,) * (tmp.ndim - 1)\n      return functools.reduce(Tensor.add, (tmp.shrink(nones + ((i, i+1),)).cast(new_uint)&lt;&lt;8*i*os for i in range(rate))).squeeze(-1).bitcast(dtype)\n    return Tensor.stack(*(tmp&gt;&gt;8*i*ns for i in range(os//ns)), dim=-1).flatten(-2).cast(new_uint).bitcast(dtype)\n  return self._apply_uop(UOp.bitcast, dtype=dt) if self.dtype != dt else self\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.float","title":"float","text":"<pre><code>float() -&gt; Tensor\n</code></pre> <p>Convenience method to cast <code>self</code> to a <code>float32</code> Tensor.</p> <p><pre><code>t = Tensor([-1, 2, 3], dtype=dtypes.int32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  2  3]\n</code></pre> <pre><code>t = t.float()\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.float [-1.  2.  3.]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def float(self) -&gt; Tensor:\n  \"\"\"\n  Convenience method to cast `self` to a `float32` Tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2, 3], dtype=dtypes.int32)\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.float()\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.float32)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.half","title":"half","text":"<pre><code>half() -&gt; Tensor\n</code></pre> <p>Convenience method to cast <code>self</code> to a <code>float16</code> Tensor.</p> <p><pre><code>t = Tensor([-1, 2, 3], dtype=dtypes.int32)\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  2  3]\n</code></pre> <pre><code>t = t.half()\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.half [-1.  2.  3.]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def half(self) -&gt; Tensor:\n  \"\"\"\n  Convenience method to cast `self` to a `float16` Tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2, 3], dtype=dtypes.int32)\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.half()\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.float16)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.int","title":"int","text":"<pre><code>int() -&gt; Tensor\n</code></pre> <p>Convenience method to cast <code>self</code> to a <code>int32</code> Tensor.</p> <p><pre><code>t = Tensor([-1.5, -0.5, 0.0, 0.5, 1.5])\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.float [-1.5 -0.5  0.   0.5  1.5]\n</code></pre> <pre><code>t = t.int()\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  0  0  0  1]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def int(self) -&gt; Tensor:\n  \"\"\"\n  Convenience method to cast `self` to a `int32` Tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1.5, -0.5, 0.0, 0.5, 1.5])\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.int()\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.int32)\n</code></pre>"},{"location":"tensor/elementwise/#tinygrad.Tensor.bool","title":"bool","text":"<pre><code>bool() -&gt; Tensor\n</code></pre> <p>Convenience method to cast <code>self</code> to a <code>bool</code> Tensor.</p> <p><pre><code>t = Tensor([-1, 0, 1])\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.int [-1  0  1]\n</code></pre> <pre><code>t = t.bool()\nprint(t.dtype, t.numpy())\n</code></pre> <pre><code>dtypes.bool [ True False  True]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def bool(self) -&gt; Tensor:\n  \"\"\"\n  Convenience method to cast `self` to a `bool` Tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 0, 1])\n  print(t.dtype, t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.bool()\n  print(t.dtype, t.numpy())\n  ```\n  \"\"\"\n  return self.cast(dtypes.bool)\n</code></pre>"},{"location":"tensor/movement/","title":"Movement","text":""},{"location":"tensor/movement/#movement-low-level","title":"Movement (low level)","text":""},{"location":"tensor/movement/#tinygrad.Tensor.view","title":"view","text":"<pre><code>view(shape: tuple[sint, ...], *args) -&gt; Tensor\n</code></pre> <p><code>.view</code> is an alias for <code>.reshape</code>.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def view(self, shape:tuple[sint, ...], *args) -&gt; Tensor:\n  \"\"\"`.view` is an alias for `.reshape`.\"\"\"\n  return self.reshape(shape, *args)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.reshape","title":"reshape","text":"<pre><code>reshape(shape, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor with the same data as the original tensor but with a different shape. <code>shape</code> can be passed as a tuple or as separate arguments.</p> <pre><code>t = Tensor.arange(6)\nprint(t.reshape(2, 3).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def reshape(self, shape, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with the same data as the original tensor but with a different shape.\n  `shape` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6)\n  print(t.reshape(2, 3).numpy())\n  ```\n  \"\"\"\n  # resolve None and args\n  new_shape = tuple([s if s is not None else self.shape[i] for i,s in enumerate(argfix(shape, *args))])\n  # resolve -1\n  if (c := new_shape.count(-1)) &gt; 1: raise RuntimeError(f\"only one dimension can be inferred using -1, getting {new_shape}\")\n  if c: new_shape = tuple([-prod(self.shape) // prod(new_shape) if s == -1 else s for s in new_shape])\n  if resolve(prod(self.shape) != prod(new_shape), True):\n    raise ValueError(f\"size mismatch, can't reshape ({', '.join(srender(d) for d in self.shape)}) -&gt; ({', '.join(srender(d) for d in new_shape)})\")\n  return self._apply_uop(UOp.reshape, arg=new_shape) if new_shape != self.shape else self\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.expand","title":"expand","text":"<pre><code>expand(shape, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor that is expanded to the shape that is specified. Expand can also increase the number of dimensions that a tensor has.</p> <p>Passing a <code>-1</code> or <code>None</code> to a dimension means that its size will not be changed.</p> <pre><code>t = Tensor([1, 2, 3])\nprint(t.expand(4, -1).numpy())\n</code></pre> <pre><code>[[1 2 3]\n [1 2 3]\n [1 2 3]\n [1 2 3]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def expand(self, shape, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that is expanded to the shape that is specified.\n  Expand can also increase the number of dimensions that a tensor has.\n\n  Passing a `-1` or `None` to a dimension means that its size will not be changed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t.expand(4, -1).numpy())\n  ```\n  \"\"\"\n  new_shape = tuple(from_ if to == -1 or to is None else to for from_, to in zip(*(_align_left(self.shape, argfix(shape, *args)))))\n  return self._broadcast_to(new_shape)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.permute","title":"permute","text":"<pre><code>permute(order, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor that is a permutation of the original tensor. The new tensor has the same data as the original tensor but with the dimensions permuted according to the order specified. <code>order</code> can be passed as a tuple or as separate arguments.</p> <p><pre><code>t = Tensor.empty(2, 3, 5)\nprint(t.shape)\n</code></pre> <pre><code>(2, 3, 5)\n</code></pre> <pre><code>print(t.permute(2, 0, 1).shape)\n</code></pre> <pre><code>(5, 2, 3)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def permute(self, order, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that is a permutation of the original tensor.\n  The new tensor has the same data as the original tensor but with the dimensions permuted according to the order specified.\n  `order` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.empty(2, 3, 5)\n  print(t.shape)\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.permute(2, 0, 1).shape)\n  ```\n  \"\"\"\n  order_arg = tuple(self._resolve_dim(x) for x in argfix(order, *args))\n  if sorted(order_arg) != list(range(self.ndim)): raise RuntimeError(f\"order is not a valid permutation, getting {order_arg}\")\n  return self._apply_uop(UOp.permute, arg=order_arg)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.flip","title":"flip","text":"<pre><code>flip(axis, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor that reverses the order of the original tensor along given <code>axis</code>. <code>axis</code> can be passed as a tuple or as separate arguments.</p> <p><pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> <pre><code>print(t.flip(0).numpy())\n</code></pre> <pre><code>[[3 4 5]\n [0 1 2]]\n</code></pre> <pre><code>print(t.flip((0, 1)).numpy())\n</code></pre> <pre><code>[[5 4 3]\n [2 1 0]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def flip(self, axis, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that reverses the order of the original tensor along given `axis`.\n  `axis` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.flip(0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.flip((0, 1)).numpy())\n  ```\n  \"\"\"\n  axis_arg = tuple(self._resolve_dim(x) for x in argfix(axis, *args))\n  if len(axis_arg) != len(dedup(axis_arg)): raise RuntimeError(f\"dim can appear at most once, getting {axis_arg}\")\n  return self._apply_uop(UOp.flip, arg=tuple([i in axis_arg for i in range(len(self.shape))]))\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.shrink","title":"shrink","text":"<pre><code>shrink(arg: tuple[tuple[sint, sint] | None, ...]) -&gt; Tensor\n</code></pre> <p>Returns a tensor that shrinks the each axis based on input arg. <code>arg</code> must have the same length as <code>self.ndim</code>. For each axis, it can be <code>None</code>, which means no shrink, or a tuple <code>(start, end)</code> that works the same as Python slice.</p> <p><pre><code>t = Tensor.arange(9).reshape(3, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]\n [6 7 8]]\n</code></pre> <pre><code>print(t.shrink(((None, (1, 3)))).numpy())\n</code></pre> <pre><code>[[1 2]\n [4 5]\n [7 8]]\n</code></pre> <pre><code>print(t.shrink((((0, 2), (0, 2)))).numpy())\n</code></pre> <pre><code>[[0 1]\n [3 4]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def shrink(self, arg:tuple[tuple[sint, sint]|None, ...]) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that shrinks the each axis based on input arg.\n  `arg` must have the same length as `self.ndim`.\n  For each axis, it can be `None`, which means no shrink, or a tuple `(start, end)` that works the same as Python slice.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(3, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.shrink(((None, (1, 3)))).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.shrink((((0, 2), (0, 2)))).numpy())\n  ```\n  \"\"\"\n  if self.ndim != len(arg): raise ValueError(f\"{self.ndim=} != {len(arg)=}\")\n  if (shrink_arg:=[x if x is not None else (0,s) for x,s in zip(arg, self.shape)]) == [(0,s) for s in self.shape]: return self\n  return self._apply_uop(UOp.shrink, arg=tuple(shrink_arg))\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.pad","title":"pad","text":"<pre><code>pad(\n    padding: (\n        Sequence[sint] | Sequence[tuple[sint, sint] | None]\n    ),\n    mode: str = \"constant\",\n    value: float = 0.0,\n) -&gt; Tensor\n</code></pre> <p>Returns a tensor with padding applied based on the input <code>padding</code>.</p> <p><code>padding</code> supports two padding structures:</p> <ol> <li> <p>Flat padding: <code>(padding_left, padding_right, padding_top, padding_bottom, ...)</code></p> <ul> <li>This structure matches PyTorch's pad.</li> <li><code>padding</code> length must be even.</li> </ul> </li> <li> <p>Group padding: <code>(..., (padding_top, padding_bottom), (padding_left, padding_right))</code></p> <ul> <li>This structure matches pad for JAX, NumPy, TensorFlow, and others.</li> <li>For each axis, padding can be <code>None</code>, meaning no padding, or a tuple <code>(start, end)</code>.</li> <li><code>padding</code> must have the same length as <code>self.ndim</code>.</li> </ul> </li> </ol> <p>Padding values can be negative, resulting in dimension shrinks that work similarly to Python negative slices. Padding modes is selected with <code>mode</code> which supports <code>constant</code>, <code>reflect</code> and <code>replicate</code>.</p> <p><pre><code>t = Tensor.arange(9).reshape(1, 1, 3, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[0 1 2]\n   [3 4 5]\n   [6 7 8]]]]\n</code></pre> <pre><code>print(t.pad((1, 2, 0, -1)).numpy())\n</code></pre> <pre><code>[[[[0 0 1 2 0 0]\n   [0 3 4 5 0 0]]]]\n</code></pre> <pre><code>print(t.pad(((None, None, (0, -1), (1, 2)))).numpy())\n</code></pre> <pre><code>[[[[0 0 1 2 0 0]\n   [0 3 4 5 0 0]]]]\n</code></pre> <pre><code>print(t.pad((1, 2, 0, -1), value=-float('inf')).numpy())\n</code></pre> <pre><code>[[[[-inf   0.   1.   2. -inf -inf]\n   [-inf   3.   4.   5. -inf -inf]]]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def pad(self, padding:Sequence[sint]|Sequence[tuple[sint, sint]|None], mode:str=\"constant\", value:float=0.0) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with padding applied based on the input `padding`.\n\n  `padding` supports two padding structures:\n\n  1. Flat padding: `(padding_left, padding_right, padding_top, padding_bottom, ...)`\n      - This structure matches PyTorch's pad.\n      - `padding` length must be even.\n\n  2. Group padding: `(..., (padding_top, padding_bottom), (padding_left, padding_right))`\n      - This structure matches pad for JAX, NumPy, TensorFlow, and others.\n      - For each axis, padding can be `None`, meaning no padding, or a tuple `(start, end)`.\n      - `padding` must have the same length as `self.ndim`.\n\n  Padding values can be negative, resulting in dimension shrinks that work similarly to Python negative slices.\n  Padding modes is selected with `mode` which supports `constant`, `reflect` and `replicate`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(1, 1, 3, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.pad((1, 2, 0, -1)).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.pad(((None, None, (0, -1), (1, 2)))).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.pad((1, 2, 0, -1), value=-float('inf')).numpy())\n  ```\n  \"\"\"\n  if mode not in {\"constant\", \"reflect\", \"replicate\", \"circular\"}: raise NotImplementedError(f\"{mode=} is not supported\")\n  # flat padding\n  if all(isinstance(p, (int,UOp)) for p in padding):\n    if len(padding)%2 != 0: raise ValueError(\"Flat padding must have even number of pads\")\n    pX = _flat_to_grouped(tuple(cast(Sequence[sint], padding)) + (0,0)*(self.ndim - len(padding)//2))\n  # group padding\n  else: pX = tuple((0,0) if p is None else p for p in cast(Sequence[tuple[sint, sint]|None], padding))\n  if len(pX) != self.ndim: raise ValueError(f\"padding length is improper, {padding=} {self.ndim=}\")\n  X, pads = self, tuple((smax(pB,0), smax(pA,0)) for pB,pA in pX)\n  if mode == \"constant\":\n    def _constant(x:Tensor,px,v) -&gt; Tensor:\n      return x._apply_uop(UOp.pad, arg=px) if v == 0 else (x._apply_uop(UOp.pad, arg=px)+Tensor.ones_like(x)._apply_uop(UOp.pad, arg=px).where(0,v))\n    return _constant(X, pX, value) if all(resolve(p &gt;= 0) for p in flatten(pX)) else \\\n           _constant(X.shrink(tuple((-smin(pB,0),smin(pA+s,s)) for (pB,pA),s in zip(pX, X.shape))), pads, value)\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  if mode == \"circular\":\n    if any(pB&gt;sh or pA&gt;sh for (pB,pA),sh in zip(pX, X.shape)): raise ValueError('Padding value causes wrapping around more than once.')\n    if any(pB&lt;0 or pA&lt;0 for pB,pA in pX): raise NotImplementedError(\"Negative pads with circular pads is not supported\")\n    orig_shape, X = X.shape, X.repeat(tuple(1 + bool(pB) + bool(pA) for pB,pA in pads))\n    return X.shrink(tuple((0 if pB == 0 else osh-pB, xsh if pA == 0 else xsh-osh+pA) for (pB,pA),osh,xsh in zip(pads, orig_shape, X.shape)))\n  for d,(pB,pA) in enumerate(pads):\n    if mode == \"reflect\":\n      if pB &gt;= (s:=X.shape[d]) or pA&gt;=s: raise ValueError(f\"Padding ({pB}, {pA}) should be less than the input size={s} for dim={d}.\")\n      slcB, slcA, = slice(pB,0,-1), slice(s-2 if s-2&gt;=0 else None, s-2-pA if s-2-pA&gt;=0 else None, -1)\n      xB, xA = (X[[slc if i == d else slice(None) for i in range(X.ndim)]] if p &gt; 0 else None for slc, p in ((slcB, pB), (slcA, pA)))\n    if mode == \"replicate\":\n      shrB, shrA, = tuple((0,1) if i==d else None for i in range(X.ndim)), tuple((X.shape[i]-1,X.shape[i]) if i==d else None for i in range(X.ndim))\n      xB, xA = (X.shrink(shr).expand(tuple(p if i==d else None for i in range(X.ndim))) if p &gt; 0 else None for shr, p in ((shrB, pB), (shrA, pA)))\n    X = Tensor.cat(*(X_ for X_ in (xB, X, xA) if X_ is not None), dim=d)\n  return X.shrink(tuple((-min(pB,0), min(pA+s,s)) for (pB,pA),s in zip(pX, X.shape)))\n</code></pre>"},{"location":"tensor/movement/#movement-high-level","title":"Movement (high level)","text":""},{"location":"tensor/movement/#tinygrad.Tensor.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(indices) -&gt; Tensor\n</code></pre> <p>Retrieves a sub-tensor using indexing.</p> <p>Supported Index Types: <code>int | slice | Tensor | None | list | tuple | Ellipsis</code></p> <p>Examples: <pre><code>t = Tensor.arange(12).reshape(3, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n</code></pre></p> <ul> <li> <p>Int Indexing: Select an element or sub-tensor using integers for each dimension.   <pre><code>print(t[1, 2].numpy())\n</code></pre> <pre><code>6\n</code></pre></p> </li> <li> <p>Slice Indexing: Select a range of elements using slice notation (<code>start:end:stride</code>).   <pre><code>print(t[0:2, ::2].numpy())\n</code></pre> <pre><code>[[0 2]\n [4 6]]\n</code></pre></p> </li> <li> <p>Tensor Indexing: Use another tensor as indices for advanced indexing. Using <code>tuple</code> or <code>list</code> here also works.   <pre><code>print(t[Tensor([2, 0, 1]), Tensor([1, 2, 3])].numpy())\n</code></pre> <pre><code>[9 2 7]\n</code></pre></p> </li> <li> <p><code>None</code> Indexing: Add a new dimension to the tensor.   <pre><code>print(t[:, None].shape)\n</code></pre> <pre><code>(3, 1, 4)\n</code></pre></p> </li> </ul> <p>Note</p> <p>Out-of-bounds indexing results in a value of <code>0</code>. <pre><code>t = Tensor([1, 2, 3])\nprint(t[Tensor([4, 3, 2])].numpy())\n</code></pre> <pre><code>[0 0 3]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def __getitem__(self, indices) -&gt; Tensor:\n  \"\"\"\n  Retrieves a sub-tensor using indexing.\n\n  Supported Index Types: `int | slice | Tensor | None | list | tuple | Ellipsis`\n\n  Examples:\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(12).reshape(3, 4)\n  print(t.numpy())\n  ```\n\n  - Int Indexing: Select an element or sub-tensor using integers for each dimension.\n    ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n    print(t[1, 2].numpy())\n    ```\n\n  - Slice Indexing: Select a range of elements using slice notation (`start:end:stride`).\n    ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n    print(t[0:2, ::2].numpy())\n    ```\n\n  - Tensor Indexing: Use another tensor as indices for advanced indexing. Using `tuple` or `list` here also works.\n    ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n    print(t[Tensor([2, 0, 1]), Tensor([1, 2, 3])].numpy())\n    ```\n\n  - `None` Indexing: Add a new dimension to the tensor.\n    ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n    print(t[:, None].shape)\n    ```\n\n  NOTE: Out-of-bounds indexing results in a value of `0`.\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t[Tensor([4, 3, 2])].numpy())\n  ```\n  \"\"\"\n  return self._getitem(indices)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.gather","title":"gather","text":"<pre><code>gather(dim: int, index: Tensor) -&gt; Tensor\n</code></pre> <p>Gathers values along an axis specified by <code>dim</code>.</p> <p><pre><code>t = Tensor([[1, 2], [3, 4]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 2]\n [3 4]]\n</code></pre> <pre><code>print(t.gather(1, Tensor([[0, 0], [1, 0]])).numpy())\n</code></pre> <pre><code>[[1 1]\n [4 3]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def gather(self:Tensor, dim:int, index:Tensor) -&gt; Tensor:\n  \"\"\"\n  Gathers values along an axis specified by `dim`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2], [3, 4]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.gather(1, Tensor([[0, 0], [1, 0]])).numpy())\n  ```\n  \"\"\"\n  assert index.ndim == self.ndim, f\"self.ndim must equal index.ndim, {self.ndim=}, {index.ndim=}\"\n  dim = self._resolve_dim(dim)\n  assert all(s &gt;= i for d,(s,i) in enumerate(zip(self.shape, index.shape)) if d != dim), \"requires self.shape[d] &gt;= index.shape[d] for all d != dim\"\n  index = index.to(self.device)\n  x = self.shrink(tuple((0, i) if d != dim else None for d,i in enumerate(index.shape))).unsqueeze(-1).transpose(-1, dim)\n  return (index.unsqueeze(-1)._one_hot_along_dim(self.shape[dim]).where(x, 0)).sum(-1, dtype=self.dtype)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.cat","title":"cat","text":"<pre><code>cat(*args: Tensor, dim: int = 0) -&gt; Tensor\n</code></pre> <p>Concatenates self with other <code>Tensor</code> in <code>args</code> along an axis specified by <code>dim</code>. All tensors must have the same shape except in the concatenating dimension.</p> <p><pre><code>t0, t1, t2 = Tensor([[1, 2]]), Tensor([[3, 4]]), Tensor([[5, 6]])\nprint(t0.cat(t1, t2, dim=0).numpy())\n</code></pre> <pre><code>[[1 2]\n [3 4]\n [5 6]]\n</code></pre> <pre><code>print(t0.cat(t1, t2, dim=1).numpy())\n</code></pre> <pre><code>[[1 2 3 4 5 6]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cat(self:Tensor, *args:Tensor, dim:int=0) -&gt; Tensor:\n  \"\"\"\n  Concatenates self with other `Tensor` in `args` along an axis specified by `dim`.\n  All tensors must have the same shape except in the concatenating dimension.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t0, t1, t2 = Tensor([[1, 2]]), Tensor([[3, 4]]), Tensor([[5, 6]])\n  print(t0.cat(t1, t2, dim=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t0.cat(t1, t2, dim=1).numpy())\n  ```\n  \"\"\"\n  dim = self._resolve_dim(dim)\n  for arg in args: assert arg.ndim==self.ndim and all(ti==ai for i,(ti,ai) in enumerate(zip(self.shape, arg.shape)) if i!=dim)\n  tensors = [self, *args]\n  dim_cumsum = list(itertools.accumulate([t.shape[dim] for t in tensors], initial=0))\n  for i,t in enumerate(tensors): tensors[i] = t.pad([(dim_cumsum[i], dim_cumsum[-1]-dim_cumsum[i+1]) if j==dim else None for j in range(t.ndim)])\n  return functools.reduce(Tensor.add, tensors)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.stack","title":"stack","text":"<pre><code>stack(*args: Tensor, dim: int = 0) -&gt; Tensor\n</code></pre> <p>Concatenates self with other <code>Tensor</code> in <code>args</code> along a new dimension specified by <code>dim</code>.</p> <p><pre><code>t0, t1, t2 = Tensor([1, 2]), Tensor([3, 4]), Tensor([5, 6])\nprint(t0.stack(t1, t2, dim=0).numpy())\n</code></pre> <pre><code>[[1 2]\n [3 4]\n [5 6]]\n</code></pre> <pre><code>print(t0.stack(t1, t2, dim=1).numpy())\n</code></pre> <pre><code>[[1 3 5]\n [2 4 6]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def stack(self:Tensor, *args:Tensor, dim:int=0) -&gt; Tensor:\n  \"\"\"\n  Concatenates self with other `Tensor` in `args` along a new dimension specified by `dim`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t0, t1, t2 = Tensor([1, 2]), Tensor([3, 4]), Tensor([5, 6])\n  print(t0.stack(t1, t2, dim=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t0.stack(t1, t2, dim=1).numpy())\n  ```\n  \"\"\"\n  # checks for shapes and number of dimensions delegated to cat\n  return Tensor.cat(*[t.unsqueeze(dim) for t in argfix(self, *args)], dim=dim)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.repeat","title":"repeat","text":"<pre><code>repeat(repeats, *args) -&gt; Tensor\n</code></pre> <p>Repeats tensor number of times along each dimension specified by <code>repeats</code>. <code>repeats</code> can be passed as a tuple or as separate arguments.</p> <p><pre><code>t = Tensor([1, 2, 3])\nprint(t.repeat(4, 2).numpy())\n</code></pre> <pre><code>[[1 2 3 1 2 3]\n [1 2 3 1 2 3]\n [1 2 3 1 2 3]\n [1 2 3 1 2 3]]\n</code></pre> <pre><code>print(t.repeat(4, 2, 1).shape)\n</code></pre> <pre><code>(4, 2, 3)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def repeat(self, repeats, *args) -&gt; Tensor:\n  \"\"\"\n  Repeats tensor number of times along each dimension specified by `repeats`.\n  `repeats` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t.repeat(4, 2).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.repeat(4, 2, 1).shape)\n  ```\n  \"\"\"\n  repeats = argfix(repeats, *args)\n  base_shape = _align_left(self.shape, repeats)[0]\n  unsqueezed_shape = flatten([[1, s] for s in base_shape])\n  expanded_shape = flatten([[r, s] for r,s in zip(repeats, base_shape)])\n  final_shape = [r*s for r,s in zip(repeats, base_shape)]\n  return self.reshape(unsqueezed_shape).expand(expanded_shape).reshape(final_shape)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.repeat_interleave","title":"repeat_interleave","text":"<pre><code>repeat_interleave(\n    repeats: int, dim: int | None = None\n) -&gt; Tensor\n</code></pre> <p>Repeats elements of a tensor.</p> <pre><code>t = Tensor([1, 2, 3])\nprint(t.repeat_interleave(2).numpy())\n</code></pre> <pre><code>[1 1 2 2 3 3]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def repeat_interleave(self, repeats:int, dim:int|None=None) -&gt; Tensor:\n  \"\"\"\n  Repeats elements of a tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t.repeat_interleave(2).numpy())\n  ```\n  \"\"\"\n  x, dim = (self.flatten(), 0) if dim is None else (self, self._resolve_dim(dim))\n  shp = x.shape\n  return x.reshape(*shp[:dim+1], 1, *shp[dim+1:]).expand(*shp[:dim+1], repeats, *shp[dim+1:]).reshape(*shp[:dim], shp[dim]*repeats, *shp[dim+1:])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.split","title":"split","text":"<pre><code>split(\n    sizes: int | Sequence[int], dim: int = 0\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Splits the tensor into chunks along the dimension specified by <code>dim</code>. If <code>sizes</code> is an integer, it splits into equally sized chunks if possible, otherwise the last chunk will be smaller. If <code>sizes</code> is a list, it splits into <code>len(sizes)</code> chunks with size in <code>dim</code> according to <code>size</code>.</p> <p><pre><code>t = Tensor.arange(10).reshape(5, 2)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1]\n [2 3]\n [4 5]\n [6 7]\n [8 9]]\n</code></pre> <pre><code>split = t.split(2)\nprint(\"\\n\".join([repr(x.numpy()) for x in split]))\n</code></pre> <pre><code>array([[0, 1],\n       [2, 3]], dtype=int32)\narray([[4, 5],\n       [6, 7]], dtype=int32)\narray([[8, 9]], dtype=int32)\n</code></pre> <pre><code>split = t.split([1, 4])\nprint(\"\\n\".join([repr(x.numpy()) for x in split]))\n</code></pre> <pre><code>array([[0, 1]], dtype=int32)\narray([[2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]], dtype=int32)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def split(self, sizes:int|Sequence[int], dim:int=0) -&gt; tuple[Tensor, ...]:\n  \"\"\"\n  Splits the tensor into chunks along the dimension specified by `dim`.\n  If `sizes` is an integer, it splits into equally sized chunks if possible, otherwise the last chunk will be smaller.\n  If `sizes` is a list, it splits into `len(sizes)` chunks with size in `dim` according to `size`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(10).reshape(5, 2)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  split = t.split(2)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in split]))\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  split = t.split([1, 4])\n  print(\"\\\\n\".join([repr(x.numpy()) for x in split]))\n  ```\n  \"\"\"\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  dim = self._resolve_dim(dim)\n  if isinstance(sizes, int): sizes = [min(sizes, self.shape[dim]-i) for i in range(0, max(1, self.shape[dim]), max(1, sizes))]\n  assert sum(sizes) == self.shape[dim], f\"expect sizes to sum exactly to {self.shape[dim]}, but got {sum(sizes)}\"\n  return tuple(self[sl] for sl in [tuple([slice(None)]*dim + [slice(sum(sizes[:i]), sum(sizes[:i + 1]))]) for i in range(len(sizes))])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.chunk","title":"chunk","text":"<pre><code>chunk(chunks: int, dim: int = 0) -&gt; list[Tensor]\n</code></pre> <p>Splits the tensor into <code>chunks</code> number of chunks along the dimension <code>dim</code>. If the tensor size along <code>dim</code> is not divisible by <code>chunks</code>, all returned chunks will be the same size except the last one. The function may return fewer than the specified number of chunks.</p> <p><pre><code>chunked = Tensor.arange(11).chunk(6)\nprint(\"\\n\".join([repr(x.numpy()) for x in chunked]))\n</code></pre> <pre><code>array([0, 1], dtype=int32)\narray([2, 3], dtype=int32)\narray([4, 5], dtype=int32)\narray([6, 7], dtype=int32)\narray([8, 9], dtype=int32)\narray([10], dtype=int32)\n</code></pre> <pre><code>chunked = Tensor.arange(12).chunk(6)\nprint(\"\\n\".join([repr(x.numpy()) for x in chunked]))\n</code></pre> <pre><code>array([0, 1], dtype=int32)\narray([2, 3], dtype=int32)\narray([4, 5], dtype=int32)\narray([6, 7], dtype=int32)\narray([8, 9], dtype=int32)\narray([10, 11], dtype=int32)\n</code></pre> <pre><code>chunked = Tensor.arange(13).chunk(6)\nprint(\"\\n\".join([repr(x.numpy()) for x in chunked]))\n</code></pre> <pre><code>array([0, 1, 2], dtype=int32)\narray([3, 4, 5], dtype=int32)\narray([6, 7, 8], dtype=int32)\narray([ 9, 10, 11], dtype=int32)\narray([12], dtype=int32)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def chunk(self, chunks:int, dim:int=0) -&gt; list[Tensor]:\n  \"\"\"\n  Splits the tensor into `chunks` number of chunks along the dimension `dim`.\n  If the tensor size along `dim` is not divisible by `chunks`, all returned chunks will be the same size except the last one.\n  The function may return fewer than the specified number of chunks.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  chunked = Tensor.arange(11).chunk(6)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in chunked]))\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  chunked = Tensor.arange(12).chunk(6)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in chunked]))\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  chunked = Tensor.arange(13).chunk(6)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in chunked]))\n  ```\n  \"\"\"\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  assert chunks &gt; 0, f\"expect chunks to be greater than 0, got: {chunks}\"\n  dim = self._resolve_dim(dim)\n  return list(self.split(ceildiv(self.shape[dim], chunks) if self.shape[dim] else [0]*chunks, dim=dim))\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.unfold","title":"unfold","text":"<pre><code>unfold(dim: int, size: sint, step: int) -&gt; Tensor\n</code></pre> <p>Unfolds the tensor along dimension <code>dim</code> into overlapping windows.</p> <p>Each window has length <code>size</code> and begins every <code>step</code> elements of <code>self</code>. Returns the input tensor with dimension <code>dim</code> replaced by dims <code>(n_windows, size)</code> where <code>n_windows = (self.shape[dim] - size) // step + 1</code>.</p> <p><pre><code>unfolded = Tensor.arange(8).unfold(0,2,2)\nprint(\"\\n\".join([repr(x.numpy()) for x in unfolded]))\n</code></pre> <pre><code>array([0, 1], dtype=int32)\narray([2, 3], dtype=int32)\narray([4, 5], dtype=int32)\narray([6, 7], dtype=int32)\n</code></pre> <pre><code>unfolded = Tensor.arange(27).reshape(3,3,3).unfold(-1,2,3)\nprint(\"\\n\".join([repr(x.numpy()) for x in unfolded]))\n</code></pre> <pre><code>array([[[0, 1]],\n\n       [[3, 4]],\n\n       [[6, 7]]], dtype=int32)\narray([[[ 9, 10]],\n\n       [[12, 13]],\n\n       [[15, 16]]], dtype=int32)\narray([[[18, 19]],\n\n       [[21, 22]],\n\n       [[24, 25]]], dtype=int32)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def unfold(self, dim:int, size:sint, step:int) -&gt; Tensor:\n  \"\"\"\n  Unfolds the tensor along dimension `dim` into overlapping windows.\n\n  Each window has length `size` and begins every `step` elements of `self`.\n  Returns the input tensor with dimension `dim` replaced by dims `(n_windows, size)`\n  where `n_windows = (self.shape[dim] - size) // step + 1`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  unfolded = Tensor.arange(8).unfold(0,2,2)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in unfolded]))\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  unfolded = Tensor.arange(27).reshape(3,3,3).unfold(-1,2,3)\n  print(\"\\\\n\".join([repr(x.numpy()) for x in unfolded]))\n  ```\n  \"\"\"\n  if size &lt; 0: raise RuntimeError(f'size must be &gt;= 0 but got {size=}')\n  if step &lt;= 0: raise RuntimeError(f'step must be &gt; 0 but got {step=}')\n  if size &gt; self.shape[dim]: raise RuntimeError(f'maximum size for tensor at dimension {dim} is {self.shape[dim]} but size is {size}')\n  dim = self._resolve_dim(dim)\n  perm_to_last = tuple(i for i in range(self.ndim) if i != dim) + (dim,)\n  return self.permute(perm_to_last)._pool((size,), step).permute(argsort(perm_to_last) + (self.ndim,))\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.meshgrid","title":"meshgrid","text":"<pre><code>meshgrid(\n    *args: Tensor, indexing: Literal[\"ij\", \"xy\"] = \"ij\"\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Generates coordinate matrices from coordinate vectors. Input tensors can be scalars or 1D tensors.</p> <p><code>indexing</code> determines how the output grids are aligned. <code>ij</code> indexing follows matrix-style indexing and <code>xy</code> indexing follows Cartesian-style indexing.</p> <p><pre><code>x, y = Tensor([1, 2, 3]), Tensor([4, 5, 6])\ngrid_x, grid_y = x.meshgrid(y)\nprint(grid_x.numpy())\nprint(grid_y.numpy())\n</code></pre> <pre><code>[[1 1 1]\n [2 2 2]\n [3 3 3]]\n[[4 5 6]\n [4 5 6]\n [4 5 6]]\n</code></pre> <pre><code>grid_x, grid_y = x.meshgrid(y, indexing=\"xy\")\nprint(grid_x.numpy())\nprint(grid_y.numpy())\n</code></pre> <pre><code>[[1 2 3]\n [1 2 3]\n [1 2 3]]\n[[4 4 4]\n [5 5 5]\n [6 6 6]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def meshgrid(self:Tensor, *args:Tensor, indexing:Literal[\"ij\", \"xy\"]=\"ij\") -&gt; tuple[Tensor, ...]:\n  \"\"\"\n  Generates coordinate matrices from coordinate vectors.\n  Input tensors can be scalars or 1D tensors.\n\n  `indexing` determines how the output grids are aligned.\n  `ij` indexing follows matrix-style indexing and `xy` indexing follows Cartesian-style indexing.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  x, y = Tensor([1, 2, 3]), Tensor([4, 5, 6])\n  grid_x, grid_y = x.meshgrid(y)\n  print(grid_x.numpy())\n  print(grid_y.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  grid_x, grid_y = x.meshgrid(y, indexing=\"xy\")\n  print(grid_x.numpy())\n  print(grid_y.numpy())\n  ```\n  \"\"\"\n  if indexing not in (\"ij\", \"xy\"): raise RuntimeError(f'indexing must be in (\"ij\", \"xy\"), got {indexing}')\n  if len(tensors:=(self, *args)) == 1: return tensors\n  basis = tuple(range(len(tensors))) if indexing == \"ij\" else (1, 0) + tuple(range(2, len(tensors)))\n  tensors = tuple(t.reshape((-1,) + (1,)*(len(args) - i)) for i,t in zip(basis, tensors))\n  output_shape = _broadcast_shape(*(t.shape for t in tensors))\n  return tuple(t._broadcast_to(output_shape) for t in tensors)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.squeeze","title":"squeeze","text":"<pre><code>squeeze(dim: int | None = None) -&gt; Tensor\n</code></pre> <p>Returns a tensor with specified dimensions of input of size 1 removed. If <code>dim</code> is not specified, all dimensions with size 1 are removed.</p> <p><pre><code>t = Tensor.zeros(2, 1, 2, 1, 2)\nprint(t.squeeze().shape)\n</code></pre> <pre><code>(2, 2, 2)\n</code></pre> <pre><code>print(t.squeeze(0).shape)\n</code></pre> <pre><code>(2, 1, 2, 1, 2)\n</code></pre> <pre><code>print(t.squeeze(1).shape)\n</code></pre> <pre><code>(2, 2, 1, 2)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def squeeze(self, dim:int|None=None) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with specified dimensions of input of size 1 removed.\n  If `dim` is not specified, all dimensions with size 1 are removed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.zeros(2, 1, 2, 1, 2)\n  print(t.squeeze().shape)\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.squeeze(0).shape)\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.squeeze(1).shape)\n  ```\n  \"\"\"\n  if dim is None: return self.reshape(tuple(dim for dim in self.shape if dim != 1))\n  dim = self._resolve_dim(dim)\n  return self if not self.ndim or self.shape[dim] != 1 else self.reshape(self.shape[:dim] + self.shape[dim+1:])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.unsqueeze","title":"unsqueeze","text":"<pre><code>unsqueeze(dim: int) -&gt; Tensor\n</code></pre> <p>Returns a tensor with a new dimension of size 1 inserted at the specified <code>dim</code>.</p> <p><pre><code>t = Tensor([1, 2, 3, 4])\nprint(t.unsqueeze(0).numpy())\n</code></pre> <pre><code>[[1 2 3 4]]\n</code></pre> <pre><code>print(t.unsqueeze(1).numpy())\n</code></pre> <pre><code>[[1]\n [2]\n [3]\n [4]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def unsqueeze(self, dim:int) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with a new dimension of size 1 inserted at the specified `dim`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(t.unsqueeze(0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.unsqueeze(1).numpy())\n  ```\n  \"\"\"\n  dim = self._resolve_dim(dim, extra=True)\n  return self.reshape(self.shape[:dim] + (1,) + self.shape[dim:])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.T","title":"T  <code>property</code>","text":"<pre><code>T: Tensor\n</code></pre> <p><code>.T</code> is an alias for <code>.transpose()</code>.</p>"},{"location":"tensor/movement/#tinygrad.Tensor.transpose","title":"transpose","text":"<pre><code>transpose(dim0=1, dim1=0) -&gt; Tensor\n</code></pre> <p>Returns a tensor that is a transposed version of the original tensor. The given dimensions <code>dim0</code> and <code>dim1</code> are swapped.</p> <p><pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> <pre><code>print(t.transpose(0, 1).numpy())\n</code></pre> <pre><code>[[0 3]\n [1 4]\n [2 5]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def transpose(self, dim0=1, dim1=0) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that is a transposed version of the original tensor.\n  The given dimensions `dim0` and `dim1` are swapped.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.transpose(0, 1).numpy())\n  ```\n  \"\"\"\n  order = list(range(self.ndim))\n  order[dim0], order[dim1] = order[dim1], order[dim0]\n  return self.permute(order)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.flatten","title":"flatten","text":"<pre><code>flatten(start_dim=0, end_dim=-1) -&gt; Tensor\n</code></pre> <p>Flattens the tensor by reshaping it into a one-dimensional tensor. If <code>start_dim</code> or <code>end_dim</code> are passed, only dimensions starting with <code>start_dim</code> and ending with <code>end_dim</code> are flattened.</p> <p><pre><code>t = Tensor.arange(8).reshape(2, 2, 2)\nprint(t.flatten().numpy())\n</code></pre> <pre><code>[0 1 2 3 4 5 6 7]\n</code></pre> <pre><code>print(t.flatten(start_dim=1).numpy())\n</code></pre> <pre><code>[[0 1 2 3]\n [4 5 6 7]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def flatten(self, start_dim=0, end_dim=-1) -&gt; Tensor:\n  \"\"\"\n  Flattens the tensor by reshaping it into a one-dimensional tensor.\n  If `start_dim` or `end_dim` are passed, only dimensions starting with `start_dim` and ending with `end_dim` are flattened.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(8).reshape(2, 2, 2)\n  print(t.flatten().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.flatten(start_dim=1).numpy())\n  ```\n  \"\"\"\n  start_dim, end_dim = self._resolve_dim(start_dim), self._resolve_dim(end_dim)\n  return self.reshape(self.shape[:start_dim] + (prod(self.shape[start_dim:end_dim+1]), ) + self.shape[end_dim+1:])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.unflatten","title":"unflatten","text":"<pre><code>unflatten(dim: int, sizes: tuple[int, ...]) -&gt; Tensor\n</code></pre> <p>Unflattens dimension <code>dim</code> of the tensor into multiple dimensions specified by <code>sizes</code>. <code>Tensor.flatten()</code> is the inverse of this function.</p> <p><pre><code>print(Tensor.ones(3, 4, 1).unflatten(1, (2, 2)).shape)\n</code></pre> <pre><code>(3, 2, 2, 1)\n</code></pre> <pre><code>print(Tensor.ones(3, 4, 1).unflatten(1, (-1, 2)).shape)\n</code></pre> <pre><code>(3, 2, 2, 1)\n</code></pre> <pre><code>print(Tensor.ones(5, 12, 3).unflatten(-2, (2, 2, 3, 1, 1)).shape)\n</code></pre> <pre><code>(5, 2, 2, 3, 1, 1, 3)\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def unflatten(self, dim:int, sizes:tuple[int,...]) -&gt; Tensor:\n  \"\"\"\n  Unflattens dimension `dim` of the tensor into multiple dimensions specified by `sizes`. `Tensor.flatten()` is the inverse of this function.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(3, 4, 1).unflatten(1, (2, 2)).shape)\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(3, 4, 1).unflatten(1, (-1, 2)).shape)\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(5, 12, 3).unflatten(-2, (2, 2, 3, 1, 1)).shape)\n  ```\n  \"\"\"\n  dim = self._resolve_dim(dim)\n  return self.reshape(self.shape[:dim] + sizes + self.shape[dim+1:])\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.diag","title":"diag","text":"<pre><code>diag() -&gt; Tensor\n</code></pre> <p>Returns a 2-D square tensor with the elements of input as the main diagonal.</p> <pre><code>print(Tensor([1, 2, 3]).diag().numpy())\n</code></pre> <pre><code>[[1 0 0]\n [0 2 0]\n [0 0 3]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def diag(self) -&gt; Tensor:\n  \"\"\"\n  Returns a 2-D square tensor with the elements of input as the main diagonal.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1, 2, 3]).diag().numpy())\n  ```\n  \"\"\"\n  if self.ndim != 1: raise ValueError(f\"expect input to be 1-D, getting {self.ndim}-D\")\n  return self.unsqueeze(-1).pad((None,(0,n:=self.shape[0]))).flatten().shrink(((0,n*n),)).reshape(n,n)\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.roll","title":"roll","text":"<pre><code>roll(\n    shifts: int | tuple[int, ...],\n    dims: int | tuple[int, ...] | None = None,\n) -&gt; Tensor\n</code></pre> <p>Rolls the tensor along specified dimension(s). The rolling operation is circular, meaning that elements that go beyond the edge are wrapped around to the beginning of the dimension.</p> <p><pre><code>t = Tensor.arange(4)\nprint(t.roll(shifts=1, dims=0).numpy())\n</code></pre> <pre><code>[3 0 1 2]\n</code></pre> <pre><code>print(t.roll(shifts=-1, dims=0).numpy())\n</code></pre> <pre><code>[1 2 3 0]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def roll(self, shifts:int|tuple[int, ...], dims:int|tuple[int, ...]|None=None) -&gt; Tensor:\n  \"\"\"\n  Rolls the tensor along specified dimension(s).\n  The rolling operation is circular, meaning that elements that go beyond the edge are wrapped around to the beginning of the dimension.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(4)\n  print(t.roll(shifts=1, dims=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.roll(shifts=-1, dims=0).numpy())\n  ```\n  \"\"\"\n  if dims is None: return self.flatten().roll(shifts, 0).reshape(self.shape)\n  dims, shifts, slices = tuple(self._resolve_dim(d) for d in make_tuple(dims, 1)), make_tuple(shifts, 1), [slice(None)] * self.ndim\n  if len(dims) != len(shifts): raise RuntimeError(f\"{len(dims)=} != {len(shifts)=}\")\n  for dim, shift in zip(dims, shifts): slices[dim] = slice(delta:=self.shape[dim]-shift%self.shape[dim], delta+self.shape[dim])\n  return self.repeat(*tuple(2 if i in dims else 1 for i in range(self.ndim)))[slices]\n</code></pre>"},{"location":"tensor/movement/#tinygrad.Tensor.rearrange","title":"rearrange","text":"<pre><code>rearrange(formula: str, **sizes) -&gt; Tensor\n</code></pre> <p>Rearranges input according to formula</p> <p>See: https://einops.rocks/api/rearrange/</p> <pre><code>x = Tensor([[1, 2], [3, 4]])\nprint(Tensor.rearrange(x, \"batch channel -&gt; (batch channel)\").numpy())\n</code></pre> <pre><code>[1 2 3 4]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def rearrange(self, formula:str, **sizes) -&gt; Tensor:\n  \"\"\"\n  Rearranges input according to formula\n\n  See: https://einops.rocks/api/rearrange/\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  x = Tensor([[1, 2], [3, 4]])\n  print(Tensor.rearrange(x, \"batch channel -&gt; (batch channel)\").numpy())\n  ```\n  \"\"\"\n  def parse_formula(formula: str):\n    tokens = f\" {formula} \".replace(\"\u2026\", \"...\").replace(\"(\", \" ( \").replace(\")\", \" ) \").replace(\" \", \"  \").replace(\" 1 \", \" ( ) \").split()\n    lparens, rparens = map(lambda x: [i for i, ch in enumerate(tokens) if ch == x], (\"(\", \")\"))\n    pairs = list(zip(lparens, rparens))\n    assert len(lparens) == len(rparens) and sorted(flatten(pairs)) == flatten(pairs), \"bracket mismatch\"\n    return [name for name in tokens if name not in (\"(\", \")\")], [(s - 2*i, e - 1 - 2*i) for i, (s, e) in enumerate(pairs)]\n\n  assert formula.count(\"-&gt;\") == 1, 'need exactly one \"-&gt;\" in formula'\n\n  (lhs, unflatten_dims), (rhs, flatten_dims) = map(parse_formula, formula.split(\"-&gt;\"))\n\n  for name in sizes: assert name in lhs, f\"axis {name} is not used in transform\"\n  assert sorted(lhs) == sorted(rhs) and len(lhs) == len(set(lhs)), f\"name mismatch in {formula}\"\n  for name in flatten((lhs, rhs)): assert name == \"...\" or (name.isidentifier() and \"_\" not in (name[0], name[-1])), f\"invalid axis name {name}\"\n  assert \"...\" not in flatten([lhs[s:e] for s, e in unflatten_dims]), f\"cannot have collapsed ellipsis (...) in lhs of {formula}\"\n  assert lhs.count(\"...\") &lt;= 1, f\"too many ellipses in {formula}\"\n\n  # resolve ellipsis\n  if \"...\" in lhs: ell_len = len(self.shape) - len(lhs) + 1 + sum(e - s - 1 for s, e in unflatten_dims)\n  lhs, rhs = map(lambda l: l[:(i:=l.index(\"...\"))] + [f\"...{j}\" for j in range(ell_len)] + l[i + 1:] if \"...\" in l else l, (lhs, rhs))\n  unflatten_dims = [(s + (ell_len - 1 if \"...0\" in lhs[:s] else 0), e + (ell_len - 1 if \"...0\" in lhs[:e] else 0)) for s, e in unflatten_dims]\n  flatten_dims = [(s + (ell_len - 1 if \"...0\" in rhs[:s] else 0), e + (ell_len - 1 if \"...0\" in rhs[:e] else 0)) for s, e in flatten_dims]\n\n  # apply movement ops in order unflatten -&gt; permute -&gt; flatten/unsqueeze\n  t = functools.reduce(lambda x, dims: x.unflatten(dims[0], tuple(sizes.get(lhs[d], -1) for d in range(*dims))), unflatten_dims, self)\n  for i, name in enumerate(lhs): assert (name not in sizes) or sizes[name] == t.shape[i], f\"size provided for dimension {name} incorrect\"\n  t = t.permute([lhs.index(name) for name in rhs])\n  return functools.reduce(lambda x, dims: x.flatten(dims[0], dims[1] - 1) if dims[0]&lt;dims[1] else x.unsqueeze(dims[0]), reversed(flatten_dims), t)\n</code></pre>"},{"location":"tensor/ops/","title":"Complex Ops","text":""},{"location":"tensor/ops/#reduce","title":"Reduce","text":""},{"location":"tensor/ops/#tinygrad.Tensor.sum","title":"sum","text":"<pre><code>sum(\n    axis: int | Sequence[int] | None = None,\n    keepdim=False,\n    dtype: DTypeLike | None = None,\n) -&gt; Tensor\n</code></pre> <p>Returns the sum of the elements of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <p>You can pass in <code>dtype</code> keyword argument to control the data type of the accumulation. If not specified, the accumulation data type is chosen based on the input tensor's data type.</p> <p><pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> <pre><code>print(t.sum().numpy())\n</code></pre> <pre><code>15\n</code></pre> <pre><code>print(t.sum(axis=0).numpy())\n</code></pre> <pre><code>[3 5 7]\n</code></pre> <pre><code>print(t.sum(axis=1).numpy())\n</code></pre> <pre><code>[ 3 12]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sum(self, axis:int|Sequence[int]|None=None, keepdim=False, dtype:DTypeLike|None=None) -&gt; Tensor:\n  \"\"\"\n  Returns the sum of the elements of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  You can pass in `dtype` keyword argument to control the data type of the accumulation.\n  If not specified, the accumulation data type is chosen based on the input tensor's data type.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sum().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sum(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.sum(axis=1).numpy())\n  ```\n  \"\"\"\n  ret = self.cast(sum_acc_dtype(self.dtype) if dtype is None else dtype)._reduce(Ops.ADD, axis, keepdim)\n  return ret.cast(self.dtype) if dtype is None and self.dtype in (dtypes.float16, dtypes.bfloat16, *dtypes.fp8s) else ret\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.prod","title":"prod","text":"<pre><code>prod(\n    axis: int | Sequence[int] | None = None,\n    keepdim=False,\n    dtype: DTypeLike | None = None,\n) -&gt; Tensor\n</code></pre> <p>Returns the product of the elements of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <p>You can pass in <code>dtype</code> keyword argument to control the data type of the accumulation. If not specified, the accumulation data type is chosen based on the input tensor's data type.</p> <p><pre><code>t = Tensor([-1, -2, -3, 1, 2, 3]).reshape(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[-1 -2 -3]\n [ 1  2  3]]\n</code></pre> <pre><code>print(t.prod().numpy())\n</code></pre> <pre><code>-36\n</code></pre> <pre><code>print(t.prod(axis=0).numpy())\n</code></pre> <pre><code>[-1 -4 -9]\n</code></pre> <pre><code>print(t.prod(axis=1).numpy())\n</code></pre> <pre><code>[-6  6]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def prod(self, axis:int|Sequence[int]|None=None, keepdim=False, dtype:DTypeLike|None=None) -&gt; Tensor:\n  \"\"\"\n  Returns the product of the elements of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  You can pass in `dtype` keyword argument to control the data type of the accumulation.\n  If not specified, the accumulation data type is chosen based on the input tensor's data type.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, -2, -3, 1, 2, 3]).reshape(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.prod().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.prod(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.prod(axis=1).numpy())\n  ```\n  \"\"\"\n  return self.cast(dtype if dtype is not None else self.dtype)._reduce(Ops.MUL, axis, keepdim)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.max","title":"max","text":"<pre><code>max(\n    axis: int | Sequence[int] | None = None, keepdim=False\n) -&gt; Tensor\n</code></pre> <p>Returns the maximum value of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[1, 0, 2], [5, 4, 3]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 0 2]\n [5 4 3]]\n</code></pre> <pre><code>print(t.max().numpy())\n</code></pre> <pre><code>5\n</code></pre> <pre><code>print(t.max(axis=0).numpy())\n</code></pre> <pre><code>[5 4 3]\n</code></pre> <pre><code>print(t.max(axis=1, keepdim=True).numpy())\n</code></pre> <pre><code>[[2]\n [5]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def max(self, axis:int|Sequence[int]|None=None, keepdim=False) -&gt; Tensor:\n  \"\"\"\n  Returns the maximum value of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 0, 2], [5, 4, 3]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.max().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.max(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.max(axis=1, keepdim=True).numpy())\n  ```\n  \"\"\"\n  return self._reduce(Ops.MAX, axis, keepdim)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.min","title":"min","text":"<pre><code>min(\n    axis: int | Sequence[int] | None = None, keepdim=False\n) -&gt; Tensor\n</code></pre> <p>Returns the minimum value of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the minimum is computed and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[1, 0, 2], [5, 4, 3]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 0 2]\n [5 4 3]]\n</code></pre> <pre><code>print(t.min().numpy())\n</code></pre> <pre><code>0\n</code></pre> <pre><code>print(t.min(axis=0).numpy())\n</code></pre> <pre><code>[1 0 2]\n</code></pre> <pre><code>print(t.min(axis=1, keepdim=True).numpy())\n</code></pre> <pre><code>[[0]\n [3]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def min(self, axis:int|Sequence[int]|None=None, keepdim=False) -&gt; Tensor:\n  \"\"\"\n  Returns the minimum value of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the minimum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 0, 2], [5, 4, 3]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.min().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.min(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.min(axis=1, keepdim=True).numpy())\n  ```\n  \"\"\"\n  return self._inverse().max(axis=axis, keepdim=keepdim)._inverse()\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.any","title":"any","text":"<pre><code>any(\n    axis: int | Sequence[int] | None = None, keepdim=False\n) -&gt; Tensor\n</code></pre> <p>Tests if any element evaluates to <code>True</code> along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the reduce axis and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[True, True], [True, False], [False, False]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ True  True]\n [ True False]\n [False False]]\n</code></pre> <pre><code>print(t.any().numpy())\n</code></pre> <pre><code>True\n</code></pre> <pre><code>print(t.any(axis=0).numpy())\n</code></pre> <pre><code>[ True  True]\n</code></pre> <pre><code>print(t.any(axis=1, keepdim=True).numpy())\n</code></pre> <pre><code>[[ True]\n [ True]\n [False]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def any(self, axis:int|Sequence[int]|None=None, keepdim=False) -&gt; Tensor:\n  \"\"\"\n  Tests if any element evaluates to `True` along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the reduce axis and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[True, True], [True, False], [False, False]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.any().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.any(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.any(axis=1, keepdim=True).numpy())\n  ```\n  \"\"\"\n  return self.bool().max(axis, keepdim)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.all","title":"all","text":"<pre><code>all(\n    axis: int | Sequence[int] | None = None, keepdim=False\n) -&gt; Tensor\n</code></pre> <p>Tests if all element evaluates to <code>True</code> along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the reduce axis and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[True, True], [True, False], [False, False]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ True  True]\n [ True False]\n [False False]]\n</code></pre> <pre><code>print(t.all().numpy())\n</code></pre> <pre><code>False\n</code></pre> <pre><code>print(t.all(axis=0).numpy())\n</code></pre> <pre><code>[False False]\n</code></pre> <pre><code>print(t.all(axis=1, keepdim=True).numpy())\n</code></pre> <pre><code>[[ True]\n [False]\n [False]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def all(self, axis:int|Sequence[int]|None=None, keepdim=False) -&gt; Tensor:\n  \"\"\"\n  Tests if all element evaluates to `True` along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the reduce axis and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[True, True], [True, False], [False, False]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.all().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.all(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.all(axis=1, keepdim=True).numpy())\n  ```\n  \"\"\"\n  return self.logical_not().any(axis, keepdim).logical_not()\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.isclose","title":"isclose","text":"<pre><code>isclose(\n    other: Tensor,\n    rtol: float = 1e-05,\n    atol: float = 1e-08,\n    equal_nan=False,\n) -&gt; Tensor\n</code></pre> <p>Returns a new tensor with element-wise comparison of closeness to <code>other</code> within a tolerance.</p> <p>The <code>rtol</code> and <code>atol</code> keyword arguments control the relative and absolute tolerance of the comparison.</p> <p>By default, two <code>NaN</code> values are not close to each other. If <code>equal_nan</code> is <code>True</code>, two <code>NaN</code> values are considered close.</p> <p><pre><code>print(Tensor([1e-7, 1e-8, 1e-9, float('nan')]).isclose(Tensor([0.0, 0.0, 0.0, float('nan')])).numpy())\n</code></pre> <pre><code>[False  True  True False]\n</code></pre> <pre><code>print(Tensor([float('nan')]).isclose(Tensor([float('nan')]), equal_nan=True).numpy())\n</code></pre> <pre><code>[ True]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def isclose(self, other:Tensor, rtol:float=1e-05, atol:float=1e-08, equal_nan=False) -&gt; Tensor:\n  \"\"\"\n  Returns a new tensor with element-wise comparison of closeness to `other` within a tolerance.\n\n  The `rtol` and `atol` keyword arguments control the relative and absolute tolerance of the comparison.\n\n  By default, two `NaN` values are not close to each other. If `equal_nan` is `True`, two `NaN` values are considered close.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([1e-7, 1e-8, 1e-9, float('nan')]).isclose(Tensor([0.0, 0.0, 0.0, float('nan')])).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([float('nan')]).isclose(Tensor([float('nan')]), equal_nan=True).numpy())\n  ```\n  \"\"\"\n  is_finite_close = self.isfinite() &amp; other.isfinite() &amp; ((self - other).abs() &lt;= atol + rtol * other.abs())\n  is_infinite_close = (self.isinf() | other.isinf()) &amp; (self == other)\n  is_nan_close = (self.isnan() &amp; other.isnan()) &amp; equal_nan\n  return is_finite_close | is_infinite_close | is_nan_close\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.mean","title":"mean","text":"<pre><code>mean(\n    axis: int | Sequence[int] | None = None, keepdim=False\n) -&gt; Tensor\n</code></pre> <p>Returns the mean value of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the mean is computed and whether the reduced dimensions are retained.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=2.5, std=0.5)\nprint(t.numpy())\n</code></pre> <pre><code>[[2.9889 2.7339 2.7763]\n [2.3356 2.0722 2.6376]]\n</code></pre> <pre><code>print(t.mean().numpy())\n</code></pre> <pre><code>2.5907671\n</code></pre> <pre><code>print(t.mean(axis=0).numpy())\n</code></pre> <pre><code>[2.6623 2.4031 2.707 ]\n</code></pre> <pre><code>print(t.mean(axis=1).numpy())\n</code></pre> <pre><code>[2.833  2.3485]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def mean(self, axis:int|Sequence[int]|None=None, keepdim=False) -&gt; Tensor:\n  \"\"\"\n  Returns the mean value of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the mean is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=2.5, std=0.5)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mean().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mean(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.mean(axis=1).numpy())\n  ```\n  \"\"\"\n  output_dtype = self.dtype if dtypes.is_float(self.dtype) else dtypes.float32\n  numerator = self.cast(sum_acc_dtype(self.dtype)).sum(axis=axis, keepdim=keepdim)\n  return numerator.div(prod([cast(int, si) for si, so in zip(self.shape, self.sum(axis=axis, keepdim=True).shape) if resolve(si != so)])) \\\n    .cast(output_dtype)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.var","title":"var","text":"<pre><code>var(\n    axis: int | Sequence[int] | None = None,\n    keepdim=False,\n    correction=1,\n) -&gt; Tensor\n</code></pre> <p>Returns the variance of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code>, <code>keepdim</code>, and <code>correction</code> keyword arguments to control the axis along which the variance is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=2.5, std=0.5)\nprint(t.numpy())\n</code></pre> <pre><code>[[2.9889 2.7339 2.7763]\n [2.3356 2.0722 2.6376]]\n</code></pre> <pre><code>print(t.var().numpy())\n</code></pre> <pre><code>0.10992539\n</code></pre> <pre><code>print(t.var(axis=0).numpy())\n</code></pre> <pre><code>[0.2134 0.2189 0.0096]\n</code></pre> <pre><code>print(t.var(axis=1).numpy())\n</code></pre> <pre><code>[0.0187 0.08  ]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def var(self, axis:int|Sequence[int]|None=None, keepdim=False, correction=1) -&gt; Tensor:\n  \"\"\"\n  Returns the variance of the tensor along the specified axis or axes.\n\n  You can pass in `axis`, `keepdim`, and `correction` keyword arguments to control the axis along\n  which the variance is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=2.5, std=0.5)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.var().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.var(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.var(axis=1).numpy())\n  ```\n  \"\"\"\n  squares = (self - self.mean(axis=axis, keepdim=True)).square()\n  n = prod([si for si, so in zip(self.shape, squares.sum(axis=axis, keepdim=True).shape) if resolve(si != so)])\n  return squares.sum(axis=axis, keepdim=keepdim).div(smax([0, n-correction]))\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.var_mean","title":"var_mean","text":"<pre><code>var_mean(\n    axis: int | Sequence[int] | None = None,\n    keepdim=False,\n    correction=1,\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Calculates the variance and mean over the dimensions specified by dim. Syntactic sugar around <code>Tensor.var</code> and <code>Tensor.mean</code> to match <code>torch.var_mean</code>.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=2.5, std=0.5)\nprint(t.numpy())\n</code></pre> <pre><code>[[2.9889 2.7339 2.7763]\n [2.3356 2.0722 2.6376]]\n</code></pre> <pre><code>var, mean = t.var_mean()\nprint(var.numpy(), mean.numpy())\n</code></pre> <pre><code>0.10992539 2.5907671\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def var_mean(self, axis:int|Sequence[int]|None=None, keepdim=False, correction=1) -&gt; tuple[Tensor, Tensor]:\n  \"\"\"\n  Calculates the variance and mean over the dimensions specified by dim.\n  Syntactic sugar around `Tensor.var` and `Tensor.mean` to match `torch.var_mean`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=2.5, std=0.5)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  var, mean = t.var_mean()\n  print(var.numpy(), mean.numpy())\n  ```\n  \"\"\"\n  return self.var(axis, keepdim, correction), self.mean(axis, keepdim)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.std","title":"std","text":"<pre><code>std(\n    axis: int | Sequence[int] | None = None,\n    keepdim=False,\n    correction=1,\n) -&gt; Tensor\n</code></pre> <p>Returns the standard deviation of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code>, <code>keepdim</code>, and <code>correction</code> keyword arguments to control the axis along which the standard deviation is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=2.5, std=0.5)\nprint(t.numpy())\n</code></pre> <pre><code>[[2.9889 2.7339 2.7763]\n [2.3356 2.0722 2.6376]]\n</code></pre> <pre><code>print(t.std().numpy())\n</code></pre> <pre><code>0.33154997\n</code></pre> <pre><code>print(t.std(axis=0).numpy())\n</code></pre> <pre><code>[0.462  0.4679 0.0981]\n</code></pre> <pre><code>print(t.std(axis=1).numpy())\n</code></pre> <pre><code>[0.1367 0.2829]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def std(self, axis:int|Sequence[int]|None=None, keepdim=False, correction=1) -&gt; Tensor:\n  \"\"\"\n  Returns the standard deviation of the tensor along the specified axis or axes.\n\n  You can pass in `axis`, `keepdim`, and `correction` keyword arguments to control the axis along\n  which the standard deviation is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=2.5, std=0.5)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.std().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.std(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.std(axis=1).numpy())\n  ```\n  \"\"\"\n  return self.var(axis, keepdim, correction).sqrt()\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.std_mean","title":"std_mean","text":"<pre><code>std_mean(\n    axis: int | Sequence[int] | None = None,\n    keepdim=False,\n    correction=1,\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Calculates the standard deviation and mean over the dimensions specified by dim. Syntactic sugar around <code>Tensor.std</code> and <code>Tensor.mean</code> to match <code>torch.std_mean</code>.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=2.5, std=0.5)\nprint(t.numpy())\n</code></pre> <pre><code>[[2.9889 2.7339 2.7763]\n [2.3356 2.0722 2.6376]]\n</code></pre> <pre><code>std, mean = t.std_mean()\nprint(std.numpy(), mean.numpy())\n</code></pre> <pre><code>0.33154997 2.5907671\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def std_mean(self, axis:int|Sequence[int]|None=None, keepdim=False, correction=1) -&gt; tuple[Tensor, Tensor]:\n  \"\"\"\n  Calculates the standard deviation and mean over the dimensions specified by dim.\n  Syntactic sugar around `Tensor.std` and `Tensor.mean` to match `torch.std_mean`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=2.5, std=0.5)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  std, mean = t.std_mean()\n  print(std.numpy(), mean.numpy())\n  ```\n  \"\"\"\n  return self.std(axis, keepdim, correction), self.mean(axis, keepdim)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.softmax","title":"softmax","text":"<pre><code>softmax(\n    axis=-1,\n    dtype: DTypeLike | None = None,\n    _single_kernel=getenv(\"SINGLE_KERNEL_SOFTMAX\"),\n) -&gt; Tensor\n</code></pre> <p>Applies the softmax function to the tensor along the specified axis.</p> <p>Rescales the elements of the tensor such that they lie in the range [0, 1] and sum to 1.</p> <p>You can pass in the <code>axis</code> keyword argument to control the axis along which the softmax is computed.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print(t.softmax().numpy())\n</code></pre> <pre><code>[[0.4436 0.2664 0.29  ]\n [0.2924 0.1727 0.5349]]\n</code></pre> <pre><code>print(t.softmax(axis=0).numpy())\n</code></pre> <pre><code>[[0.787  0.7897 0.5689]\n [0.213  0.2103 0.4311]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def softmax(self, axis=-1, dtype:DTypeLike|None=None, _single_kernel=getenv(\"SINGLE_KERNEL_SOFTMAX\")) -&gt; Tensor:\n  \"\"\"\n  Applies the softmax function to the tensor along the specified axis.\n\n  Rescales the elements of the tensor such that they lie in the range [0, 1] and sum to 1.\n\n  You can pass in the `axis` keyword argument to control the axis along which the softmax is computed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.softmax().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.softmax(axis=0).numpy())\n  ```\n  \"\"\"\n  if _single_kernel:\n    _, e, ss = self.contiguous()._softmax(axis, dtype)\n    return e.div(ss).fuse()\n  _, e, ss = self._softmax(axis, dtype)\n  return e.div(ss)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.log_softmax","title":"log_softmax","text":"<pre><code>log_softmax(\n    axis=-1, dtype: DTypeLike | None = None\n) -&gt; Tensor\n</code></pre> <p>Applies the log-softmax function to the tensor along the specified axis.</p> <p>The log-softmax function is a numerically stable alternative to the softmax function in log space.</p> <p>You can pass in the <code>axis</code> keyword argument to control the axis along which the log-softmax is computed.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print(t.log_softmax().numpy())\n</code></pre> <pre><code>[[-0.8127 -1.3228 -1.238 ]\n [-1.2297 -1.7564 -0.6256]]\n</code></pre> <pre><code>print(t.log_softmax(axis=0).numpy())\n</code></pre> <pre><code>[[-0.2396 -0.2361 -0.564 ]\n [-1.5463 -1.5594 -0.8414]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def log_softmax(self, axis=-1, dtype:DTypeLike|None=None) -&gt; Tensor:\n  \"\"\"\n  Applies the log-softmax function to the tensor along the specified axis.\n\n  The log-softmax function is a numerically stable alternative to the softmax function in log space.\n\n  You can pass in the `axis` keyword argument to control the axis along which the log-softmax is computed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.log_softmax().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.log_softmax(axis=0).numpy())\n  ```\n  \"\"\"\n  m, _, ss = self._softmax(axis, dtype)\n  return m - ss.log()\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.logsumexp","title":"logsumexp","text":"<pre><code>logsumexp(axis=None, keepdim=False) -&gt; Tensor\n</code></pre> <p>Computes the log-sum-exp of the tensor along the specified axis or axes.</p> <p>The log-sum-exp function is a numerically stable way to compute the logarithm of the sum of exponentials.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the log-sum-exp is computed and whether the reduced dimensions are retained.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print(t.logsumexp().numpy())\n</code></pre> <pre><code>2.1347282\n</code></pre> <pre><code>print(t.logsumexp(axis=0).numpy())\n</code></pre> <pre><code>[1.2174 0.7039 1.1167]\n</code></pre> <pre><code>print(t.logsumexp(axis=1).numpy())\n</code></pre> <pre><code>[1.7906 0.9009]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logsumexp(self, axis=None, keepdim=False) -&gt; Tensor:\n  \"\"\"\n  Computes the log-sum-exp of the tensor along the specified axis or axes.\n\n  The log-sum-exp function is a numerically stable way to compute the logarithm of the sum of exponentials.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the log-sum-exp is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logsumexp().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logsumexp(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logsumexp(axis=1).numpy())\n  ```\n  \"\"\"\n  m = self.max(axis=axis, keepdim=True)\n  return (self - m).exp().sum(axis=axis, keepdim=keepdim).log() + m.squeeze(axis)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.logcumsumexp","title":"logcumsumexp","text":"<pre><code>logcumsumexp(axis=0) -&gt; Tensor\n</code></pre> <p>Computes the log-cumsum-exp of the tensor along the specified axis or axes.</p> <p>The log-cumsum-exp function is a numerically stable way to compute the logarithm of the cumulative sum of exponentials.</p> <p>You can pass in the <code>axis</code> keyword argument to control the axis along which the log-cumsum-exp is computed.</p> <p><pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.9779  0.4678  0.5526]\n [-0.3288 -0.8555  0.2753]]\n</code></pre> <pre><code>print(t.logcumsumexp().numpy())\n</code></pre> <pre><code>[[0.9779 0.4678 0.5526]\n [1.2174 0.7039 1.1167]]\n</code></pre> <pre><code>print(t.logcumsumexp(axis=0).numpy())\n</code></pre> <pre><code>[[0.9779 0.4678 0.5526]\n [1.2174 0.7039 1.1167]]\n</code></pre> <pre><code>print(t.logcumsumexp(axis=1).numpy())\n</code></pre> <pre><code>[[ 0.9779  1.4481  1.7906]\n [-0.3288  0.1353  0.9009]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logcumsumexp(self, axis=0) -&gt; Tensor:\n  \"\"\"\n  Computes the log-cumsum-exp of the tensor along the specified axis or axes.\n\n  The log-cumsum-exp function is a numerically stable way to compute the logarithm of the cumulative sum of exponentials.\n\n  You can pass in the `axis` keyword argument to control the axis along which\n  the log-cumsum-exp is computed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logcumsumexp().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logcumsumexp(axis=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.logcumsumexp(axis=1).numpy())\n  ```\n  \"\"\"\n  if self.ndim == 0: return self\n  x = self.transpose(axis, -1)\n  last_dim_size = x.shape[-1]\n  x_unsqueezed = x.unsqueeze(-2).expand((None,)*(self.ndim-1)+(last_dim_size, None))\n  x_cummax = x.cummax(-1)\n  mask = Tensor.ones(last_dim_size, last_dim_size, requires_grad=False, device=self.device).tril()\n  ret = mask.where(x_unsqueezed - x_cummax.unsqueeze(-1), dtypes.min(self.dtype)).exp().sum(-1).log() + x_cummax\n  return ret.transpose(-1, axis)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.argmax","title":"argmax","text":"<pre><code>argmax(axis=None, keepdim=False) -&gt; Tensor\n</code></pre> <p>Returns the indices of the maximum value of the tensor along the specified axis.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[1, 0, 2], [5, 4, 3]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 0 2]\n [5 4 3]]\n</code></pre> <pre><code>print(t.argmax().numpy()) # Returns the index of the maximum value in the flattened tensor.\n</code></pre> <pre><code>3\n</code></pre> <pre><code>print(t.argmax(axis=0).numpy()) # Returns the indices of the maximum values along axis 0.\n</code></pre> <pre><code>[1 1 1]\n</code></pre> <pre><code>print(t.argmax(axis=1).numpy()) # Returns the indices of the maximum values along axis 1.\n</code></pre> <pre><code>[2 0]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def argmax(self, axis=None, keepdim=False) -&gt; Tensor:\n  \"\"\"\n  Returns the indices of the maximum value of the tensor along the specified axis.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 0, 2], [5, 4, 3]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmax().numpy()) # Returns the index of the maximum value in the flattened tensor.\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmax(axis=0).numpy()) # Returns the indices of the maximum values along axis 0.\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmax(axis=1).numpy()) # Returns the indices of the maximum values along axis 1.\n  ```\n  \"\"\"\n  if axis is None: return self.flatten().argmax(0)\n  axis = self._resolve_dim(axis)\n  m = self == self.max(axis=axis, keepdim=True)\n  idx = m * Tensor.arange(self.shape[axis],0,-1, requires_grad=False, device=self.device).reshape(self.shape[axis], *[1]*(self.ndim-axis-1))\n  return (self.shape[axis]-idx.max(axis=axis, keepdim=keepdim)).cast(dtypes.int32)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.argmin","title":"argmin","text":"<pre><code>argmin(axis=None, keepdim=False) -&gt; Tensor\n</code></pre> <p>Returns the indices of the minimum value of the tensor along the specified axis.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the minimum is computed and whether the reduced dimensions are retained.</p> <p><pre><code>t = Tensor([[1, 0, 2], [5, 4, 3]])\nprint(t.numpy())\n</code></pre> <pre><code>[[1 0 2]\n [5 4 3]]\n</code></pre> <pre><code>print(t.argmin().numpy()) # Returns the index of the minimum value in the flattened tensor.\n</code></pre> <pre><code>1\n</code></pre> <pre><code>print(t.argmin(axis=0).numpy()) # Returns the indices of the minimum values along axis 0.\n</code></pre> <pre><code>[0 0 0]\n</code></pre> <pre><code>print(t.argmin(axis=1).numpy()) # Returns the indices of the minimum values along axis 1.\n</code></pre> <pre><code>[1 2]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def argmin(self, axis=None, keepdim=False) -&gt; Tensor:\n  \"\"\"\n  Returns the indices of the minimum value of the tensor along the specified axis.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the minimum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 0, 2], [5, 4, 3]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmin().numpy()) # Returns the index of the minimum value in the flattened tensor.\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmin(axis=0).numpy()) # Returns the indices of the minimum values along axis 0.\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.argmin(axis=1).numpy()) # Returns the indices of the minimum values along axis 1.\n  ```\n  \"\"\"\n  return self._inverse().argmax(axis=axis, keepdim=keepdim)\n</code></pre>"},{"location":"tensor/ops/#processing","title":"Processing","text":""},{"location":"tensor/ops/#tinygrad.Tensor.avg_pool2d","title":"avg_pool2d","text":"<pre><code>avg_pool2d(\n    kernel_size: tuple[int, ...] = (2, 2),\n    stride=None,\n    dilation=1,\n    padding: int | tuple[int, ...] = 0,\n    ceil_mode=False,\n    count_include_pad=True,\n) -&gt; Tensor\n</code></pre> <p>Applies average pooling over a tensor.</p> <p>This function supports three different types of <code>padding</code></p> <ol> <li> <p><code>int</code> (single value):   Applies the same padding value uniformly to all spatial dimensions.</p> </li> <li> <p><code>tuple[int, ...]</code> (length = number of spatial dimensions):   Specifies a distinct padding value for each spatial dimension in the form <code>(padding_height, padding_width, ...)</code>.</p> </li> <li> <p><code>tuple[int, ...]</code> (length = 2 * number of spatial dimensions):   Specifies explicit padding for each side of each spatial dimension in the form   <code>(padding_left, padding_right, padding_top, padding_bottom, ...)</code>.</p> </li> </ol> <p>When <code>ceil_mode</code> is set to <code>True</code>, output shape will be determined using ceil division. When <code>count_include_pad</code> is set to <code>False</code>, zero padding will not be included in the averaging calculation.</p> <p>Note</p> <p>unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.</p> <p><pre><code>t = Tensor.arange(25).reshape(1, 1, 5, 5)\nprint(t.avg_pool2d().numpy())\n</code></pre> <pre><code>[[[[ 3.  5.]\n   [13. 15.]]]]\n</code></pre> <pre><code>print(t.avg_pool2d(ceil_mode=True).numpy())\n</code></pre> <pre><code>[[[[ 3.   5.   6.5]\n   [13.  15.  16.5]\n   [20.5 22.5 24. ]]]]\n</code></pre> <pre><code>print(t.avg_pool2d(padding=1).numpy())\n</code></pre> <pre><code>[[[[ 0.    0.75  1.75]\n   [ 3.75  9.   11.  ]\n   [ 8.75 19.   21.  ]]]]\n</code></pre> <pre><code>print(t.avg_pool2d(padding=1, count_include_pad=False).numpy())\n</code></pre> <pre><code>[[[[ 0.   1.5  3.5]\n   [ 7.5  9.  11. ]\n   [17.5 19.  21. ]]]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def avg_pool2d(self, kernel_size:tuple[int, ...]=(2,2), stride=None, dilation=1, padding:int|tuple[int, ...]=0,\n               ceil_mode=False, count_include_pad=True) -&gt; Tensor:\n  \"\"\"\n  Applies average pooling over a tensor.\n\n  This function supports three different types of `padding`\n\n  1. `int` (single value):\n    Applies the same padding value uniformly to all spatial dimensions.\n\n  2. `tuple[int, ...]` (length = number of spatial dimensions):\n    Specifies a distinct padding value for each spatial dimension in the form `(padding_height, padding_width, ...)`.\n\n  3. `tuple[int, ...]` (length = 2 * number of spatial dimensions):\n    Specifies explicit padding for each side of each spatial dimension in the form\n    `(padding_left, padding_right, padding_top, padding_bottom, ...)`.\n\n  When `ceil_mode` is set to `True`, output shape will be determined using ceil division.\n  When `count_include_pad` is set to `False`, zero padding will not be included in the averaging calculation.\n\n  NOTE: unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(25).reshape(1, 1, 5, 5)\n  print(t.avg_pool2d().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.avg_pool2d(ceil_mode=True).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.avg_pool2d(padding=1).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.avg_pool2d(padding=1, count_include_pad=False).numpy())\n  ```\n  \"\"\"\n  axis = tuple(range(-len(k_ := make_tuple(kernel_size, 2)), 0))\n  def pool(x:Tensor, padding_:Sequence[int]) -&gt; Tensor: return x.pad(padding_)._pool(k_, stride if stride is not None else k_, dilation)\n  reg_pads = self._resolve_pool_pads(padding, len(k_))\n  ceil_pads = self._apply_ceil_mode(reg_pads, k_, stride if stride is not None else k_, dilation)\n  if not count_include_pad:\n    pads = ceil_pads if ceil_mode else reg_pads\n    return pool(self, pads).sum(axis) / pool(self.ones_like(), pads).sum(axis)\n  if not ceil_mode: return pool(self, reg_pads).mean(axis)\n  return pool(self, ceil_pads).sum(axis) / pool(self.pad(reg_pads).ones_like(), tuple(cp-rp for cp,rp in zip(ceil_pads, reg_pads))).sum(axis)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.max_pool2d","title":"max_pool2d","text":"<pre><code>max_pool2d(\n    kernel_size: tuple[int, ...] = (2, 2),\n    stride=None,\n    dilation=1,\n    padding: int | tuple[int, ...] = 0,\n    ceil_mode=False,\n    return_indices=False,\n) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Applies max pooling over a tensor.</p> <p>This function supports three different types of <code>padding</code></p> <ol> <li> <p><code>int</code> (single value):   Applies the same padding value uniformly to all spatial dimensions.</p> </li> <li> <p><code>tuple[int, ...]</code> (length = number of spatial dimensions):   Specifies a distinct padding value for each spatial dimension in the form <code>(padding_height, padding_width, ...)</code>.</p> </li> <li> <p><code>tuple[int, ...]</code> (length = 2 * number of spatial dimensions):   Specifies explicit padding for each side of each spatial dimension in the form   <code>(padding_left, padding_right, padding_top, padding_bottom, ...)</code>.</p> </li> </ol> <p>When <code>ceil_mode</code> is set to <code>True</code>, output shape will be determined using ceil division. When <code>return_indices</code> is set to <code>True</code>, the argmax will be returned along with the max values.</p> <p>Note</p> <p>unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.</p> <p><pre><code>t = Tensor.arange(25).reshape(1, 1, 5, 5)\nprint(t.max_pool2d().numpy())\n</code></pre> <pre><code>[[[[ 6  8]\n   [16 18]]]]\n</code></pre> <pre><code>print(t.max_pool2d(ceil_mode=True).numpy())\n</code></pre> <pre><code>[[[[ 6  8  9]\n   [16 18 19]\n   [21 23 24]]]]\n</code></pre> <pre><code>print(t.max_pool2d(padding=1).numpy())\n</code></pre> <pre><code>[[[[ 0  2  4]\n   [10 12 14]\n   [20 22 24]]]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def max_pool2d(self, kernel_size:tuple[int, ...]=(2,2), stride=None, dilation=1, padding:int|tuple[int, ...]=0,\n               ceil_mode=False, return_indices=False) -&gt; Tensor | tuple[Tensor, Tensor]:\n  \"\"\"\n  Applies max pooling over a tensor.\n\n  This function supports three different types of `padding`\n\n  1. `int` (single value):\n    Applies the same padding value uniformly to all spatial dimensions.\n\n  2. `tuple[int, ...]` (length = number of spatial dimensions):\n    Specifies a distinct padding value for each spatial dimension in the form `(padding_height, padding_width, ...)`.\n\n  3. `tuple[int, ...]` (length = 2 * number of spatial dimensions):\n    Specifies explicit padding for each side of each spatial dimension in the form\n    `(padding_left, padding_right, padding_top, padding_bottom, ...)`.\n\n  When `ceil_mode` is set to `True`, output shape will be determined using ceil division.\n  When `return_indices` is set to `True`, the argmax will be returned along with the max values.\n\n  NOTE: unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(25).reshape(1, 1, 5, 5)\n  print(t.max_pool2d().numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.max_pool2d(ceil_mode=True).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.max_pool2d(padding=1).numpy())\n  ```\n  \"\"\"\n  axis = tuple(range(-len(k_ := make_tuple(kernel_size, 2)), 0))\n  pads = self._resolve_pool_pads(padding, len(k_))\n  if ceil_mode: pads = self._apply_ceil_mode(pads, k_, stride if stride is not None else k_, dilation)\n  pooled = self.pad(pads, value=dtypes.min(self.dtype))._pool(k_, stride if stride is not None else k_, dilation)\n  if not return_indices: return pooled.max(axis)\n  spatial_sz = math.prod(spatial_shape := self.shape[-len(k_):])\n  idx = Tensor.arange(spatial_sz,0,-1, requires_grad=False, device=self.device).reshape(spatial_shape)\n  m = pooled == pooled.max(axis, keepdim=True)\n  idx = m * idx.pad(pads, value=dtypes.min(idx.dtype))._pool(k_, stride if stride is not None else k_, dilation)\n  return pooled.max(axis), spatial_sz - idx.max(axis)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.max_unpool2d","title":"max_unpool2d","text":"<pre><code>max_unpool2d(\n    indices: Tensor,\n    kernel_size: tuple[int, ...] = (2, 2),\n    stride=None,\n    dilation=1,\n    padding: int | tuple[int, ...] = 0,\n    output_size=None,\n)\n</code></pre> <p>Performs a partial inverse of <code>max_pool2d</code> using the indices from the argmax.</p> <p>When <code>output_size</code> is provided, the output shape disambiguates to the provided shape.</p> <p>Note</p> <p>unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.</p> <p><pre><code>t = Tensor.arange(1, 17).reshape(1, 1, 4, 4)\nprint(t.numpy())\n</code></pre> <pre><code>[[[[ 1  2  3  4]\n   [ 5  6  7  8]\n   [ 9 10 11 12]\n   [13 14 15 16]]]]\n</code></pre> <pre><code>output, indices = Tensor.max_pool2d(t, return_indices=True)\nprint(output.numpy())\nprint(indices.numpy())\n</code></pre> <pre><code>[[[[ 6  8]\n   [14 16]]]]\n[[[[ 5  7]\n   [13 15]]]]\n</code></pre> <pre><code>print(Tensor.max_unpool2d(output, indices).numpy())\n</code></pre> <pre><code>[[[[ 0  0  0  0]\n   [ 0  6  0  8]\n   [ 0  0  0  0]\n   [ 0 14  0 16]]]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def max_unpool2d(self, indices:Tensor, kernel_size:tuple[int, ...]=(2,2), stride=None, dilation=1, padding:int|tuple[int, ...]=0, output_size=None):\n  \"\"\"\n  Performs a partial inverse of `max_pool2d` using the indices from the argmax.\n\n  When `output_size` is provided, the output shape disambiguates to the provided shape.\n\n  NOTE: unlike PyTorch, this implementation is not limited to only 2d pooling and instead works for any number of dimensions.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(1, 17).reshape(1, 1, 4, 4)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  output, indices = Tensor.max_pool2d(t, return_indices=True)\n  print(output.numpy())\n  print(indices.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.max_unpool2d(output, indices).numpy())\n  ```\n  \"\"\"\n  bs,c,*spatial_shape = self.shape\n  if output_size is None:\n    k_,d_,s_ = (make_tuple(x, len(spatial_shape)) for x in (kernel_size, dilation, stride if stride is not None else kernel_size))\n    p_ = _flat_to_grouped(self._resolve_pool_pads(padding, len(spatial_shape)))\n    # https://arxiv.org/pdf/1603.07285 inverse of relationship 15 in section 5.1.\n    output_size = tuple((i-1)*s - (pB+pA) + (d*(k-1)+1) for i,k,d,s,(pA,pB) in zip(spatial_shape,k_,d_,s_,p_))\n  else: output_size = output_size[-len(spatial_shape):]\n  ret = (indices.reshape(bs,c,1,-1)._one_hot_along_dim(prod(output_size), 2).where(self.reshape(bs,c,1,-1), 0)).sum(3)\n  return ret.reshape(bs,c,*output_size)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.conv2d","title":"conv2d","text":"<pre><code>conv2d(\n    weight: Tensor,\n    bias: Tensor | None = None,\n    groups=1,\n    stride=1,\n    dilation=1,\n    padding: int | tuple[int, ...] = 0,\n    dtype: DTypeLike | None = None,\n) -&gt; Tensor\n</code></pre> <p>Applies a convolution over a tensor with a given <code>weight</code> and optional <code>bias</code>.</p> <p>This function supports three different types of <code>padding</code></p> <ol> <li> <p><code>int</code> (single value):   Applies the same padding value uniformly to all spatial dimensions.</p> </li> <li> <p><code>tuple[int, ...]</code> (length = number of spatial dimensions):   Specifies a distinct padding value for each spatial dimension in the form <code>(padding_height, padding_width, ...)</code>.</p> </li> <li> <p><code>tuple[int, ...]</code> (length = 2 * number of spatial dimensions):   Specifies explicit padding for each side of each spatial dimension in the form   <code>(padding_left, padding_right, padding_top, padding_bottom, ...)</code>.</p> </li> </ol> <p>Note</p> <p>unlike PyTorch, this implementation is not limited to only 2d convolutions and instead works for any number of dimensions.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html</p> <pre><code>t = Tensor.arange(9).reshape(1, 1, 3, 3)\nw = Tensor.ones(1, 1, 2, 2)\nprint(t.conv2d(w).numpy())\n</code></pre> <pre><code>[[[[ 8. 12.]\n   [20. 24.]]]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def conv2d(self, weight:Tensor, bias:Tensor|None=None, groups=1, stride=1, dilation=1, padding:int|tuple[int, ...]=0,\n           dtype:DTypeLike|None=None) -&gt; Tensor:\n  \"\"\"\n  Applies a convolution over a tensor with a given `weight` and optional `bias`.\n\n  This function supports three different types of `padding`\n\n  1. `int` (single value):\n    Applies the same padding value uniformly to all spatial dimensions.\n\n  2. `tuple[int, ...]` (length = number of spatial dimensions):\n    Specifies a distinct padding value for each spatial dimension in the form `(padding_height, padding_width, ...)`.\n\n  3. `tuple[int, ...]` (length = 2 * number of spatial dimensions):\n    Specifies explicit padding for each side of each spatial dimension in the form\n    `(padding_left, padding_right, padding_top, padding_bottom, ...)`.\n\n  NOTE: unlike PyTorch, this implementation is not limited to only 2d convolutions and instead works for any number of dimensions.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(1, 1, 3, 3)\n  w = Tensor.ones(1, 1, 2, 2)\n  print(t.conv2d(w).numpy())\n  ```\n  \"\"\"\n  if IMAGE: return self.image_conv2d(weight, bias, groups, stride, dilation, padding, dtype)\n  (bs,cin_), (cout,cin), HW = self.shape[:2], weight.shape[:2], weight.shape[2:]\n  padding_ = self._resolve_pool_pads(padding, len(HW))\n  assert groups*cin == cin_ and len(self.shape) == len(weight.shape),\\\n      f\"Input Tensor shape {self.shape} does not match the shape of the weights {weight.shape}. ({groups*cin} vs. {cin_})\"\n\n  # conv2d is a pooling op (with padding)\n  x = self.pad(padding_)._pool(HW, stride, dilation)   # (bs, groups*cin, oy, ox, H, W)\n  rcout, oyx = cout//groups, x.shape[2:-len(HW)]\n  if not all(x == 3 for x in HW) or stride != 1 or dilation != 1 or not WINO:\n    # normal conv\n    x = x.reshape(bs, groups, cin, 1, *oyx, *HW).expand(bs, groups, cin, rcout, *oyx, *HW)\\\n      .permute(0,1,3,*[4+i for i in range(len(oyx))],2,*[4+len(oyx)+i for i in range(len(HW))])\n\n    # conv! broadcasted to (bs, groups, rcout, *oyx, cin, *HW)\n    ret = (x * weight.reshape(1, groups, rcout, *[1] * len(oyx), cin, *HW))\\\n      .sum([-1-i for i in range(1+len(oyx))], keepdim=True, dtype=dtype).reshape(bs, cout, *oyx)\n    return ret if bias is None else ret.add(bias.reshape(1, -1, *[1] * len(HW)))\n\n  HWI, HWO = (6,) * len(HW), (4,) * len(HW)  # F(4x4,3x3) winograd tiles\n  winograd_G = [[1/4, 0, 0], [-1/6, -1/6, -1/6], [-1/6, 1/6, -1/6], [1/24, 1/12, 1/6], [1/24, -1/12, 1/6], [0, 0, 1]]\n  winograd_Bt = [[4, 0, -5, 0, 1, 0], [0, -4, -4, 1, 1, 0], [0, 4, -4, -1, 1, 0], [0, -2, -1, 2, 1, 0], [0, 2, -1, -2, 1, 0], [0, 4, 0, -5, 0, 1]]\n  winograd_At = [[1, 1, 1, 1, 1, 0], [0, 1, -1, 2, -2, 0], [0, 1, 1, 4, 4, 0], [0, 1, -1, 8, -8, 1]] # applying At in pre-order doubles compile time\n\n  # TODO: stride == dilation\n  # use padding to round up to 4x4 output tiles\n  # (bs, cin_, tyx, HWI)\n  pads = [[padding_[i*2], padding_[i*2+1] + (-(dim + sum(padding_[i * 2:(i + 1) * 2]) - 2) % 4)] for i, dim in enumerate(self.shape[-len(HW):])]\n  d = self.pad(sum(pads, []))._pool(HWI, HWO)\n  # move HW to the front: # (HWI, bs, cin_, tyx)\n  d = d.permute(*range(len(d.shape)-len(HW),len(d.shape)), *range(len(d.shape)-len(HW)))\n  tyx = d.shape[-len(HWI):]  # dim of tiling\n\n  g = weight.permute(*range(len(weight.shape)-len(HW),len(weight.shape)), *range(len(weight.shape)-len(HW)))  # move HW to the front\n\n  # compute 6x6 winograd tiles: GgGt, BtdB\n  # (HWI, groups * rcout, cin) -&gt; (HWI, bs=1, groups, rcout, cin, tyx=(1,1))\n  gfactors = _apply_winograd_matrix(winograd_G, g, len(HW)).reshape(*HWI, 1, groups, rcout, cin, *([1]*len(tyx)))\n  # (HWI, bs, cin_, tyx) -&gt; (HWI, bs, groups, 1 ,cin, *tyx)\n  dfactors = _apply_winograd_matrix(winograd_Bt, d, len(HW)).reshape(*HWI, bs, groups, 1, cin, *tyx)\n\n  # matmul; sum across cin: (HWI, bs, groups, rcout, *tyx); then HWI -&gt; HWO: (HWO, bs, groups, rcout, *tyx)\n  ret = _apply_winograd_matrix(winograd_At, (gfactors * dfactors).sum(axis=-1-len(HW), dtype=dtype), len(HW))\n\n  # interleave tyx and HWO: (bs, groups, rcout, oy, HO, ox, WO)\n  ret = ret.permute([*range(len(HW), len(ret.shape)-len(HW)), *[i+o for i in range(len(HW)) for o in [len(ret.shape)-len(HW),0]]])\n  # merge groups and rcout, tyx and HWO: (bs, groups, cout, *yx), shrink to final\n  ret = ret.reshape(bs, cout, *[c * HWO[i] for i, c in enumerate(tyx)]).shrink(tuple((0, s) for s in [bs, cout, *oyx]))\n\n  return (ret if bias is None else ret.add(bias.reshape(1, -1, *[1 for _ in range(len(HW))]))).contiguous().contiguous_backward()\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.conv_transpose2d","title":"conv_transpose2d","text":"<pre><code>conv_transpose2d(\n    weight: Tensor,\n    bias: Tensor | None = None,\n    groups=1,\n    stride=1,\n    dilation=1,\n    padding=0,\n    output_padding=0,\n) -&gt; Tensor\n</code></pre> <p>Applies a transposed convolution over a tensor with a given <code>weight</code> and optional <code>bias</code>.</p> <p>This function supports three different types of <code>padding</code></p> <ol> <li> <p><code>int</code> (single value):   Applies the same padding value uniformly to all spatial dimensions.</p> </li> <li> <p><code>tuple[int, ...]</code> (length = number of spatial dimensions):   Specifies a distinct padding value for each spatial dimension in the form <code>(padding_height, padding_width, ...)</code>.</p> </li> <li> <p><code>tuple[int, ...]</code> (length = 2 * number of spatial dimensions):   Specifies explicit padding for each side of each spatial dimension in the form   <code>(padding_left, padding_right, padding_top, padding_bottom, ...)</code>.</p> </li> </ol> <p>Note</p> <p>unlike PyTorch, this implementation is not limited to only 2d transposed convolutions and instead works for any number of dimensions.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html</p> <pre><code>t = Tensor.arange(9).reshape(1, 1, 3, 3)\nw = Tensor.ones(1, 1, 2, 2)\nprint(t.conv_transpose2d(w).numpy())\n</code></pre> <pre><code>[[[[ 0.  1.  3.  2.]\n   [ 3.  8. 12.  7.]\n   [ 9. 20. 24. 13.]\n   [ 6. 13. 15.  8.]]]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def conv_transpose2d(self, weight:Tensor, bias:Tensor|None=None, groups=1, stride=1, dilation=1, padding=0, output_padding=0) -&gt; Tensor:\n  \"\"\"\n  Applies a transposed convolution over a tensor with a given `weight` and optional `bias`.\n\n  This function supports three different types of `padding`\n\n  1. `int` (single value):\n    Applies the same padding value uniformly to all spatial dimensions.\n\n  2. `tuple[int, ...]` (length = number of spatial dimensions):\n    Specifies a distinct padding value for each spatial dimension in the form `(padding_height, padding_width, ...)`.\n\n  3. `tuple[int, ...]` (length = 2 * number of spatial dimensions):\n    Specifies explicit padding for each side of each spatial dimension in the form\n    `(padding_left, padding_right, padding_top, padding_bottom, ...)`.\n\n  NOTE: unlike PyTorch, this implementation is not limited to only 2d transposed convolutions and instead works for any number of dimensions.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(1, 1, 3, 3)\n  w = Tensor.ones(1, 1, 2, 2)\n  print(t.conv_transpose2d(w).numpy())\n  ```\n  \"\"\"\n  x, w = self, weight.unflatten(0, (groups, -1)).transpose(1, 2).flip(*range(3, len(weight.shape)+1))\n  HW = weight.shape[2:]\n  padding = _flat_to_grouped(self._resolve_pool_pads(padding, len(HW)))\n  stride, dilation, output_padding = [make_tuple(x, len(HW)) for x in (stride, dilation, output_padding)]\n  if any(s&gt;1 for s in stride):\n    # handle strides: (k) -&gt; reshape -&gt; (k,1) -&gt; pad -&gt; (k,s) -&gt; reshape -&gt; (k*s) -&gt; shrink (k-(s-1))\n    x = x.reshape(None, None, *flatten((k,1) for k in x.shape[2:]))\n    x = x.pad((None, None, *flatten((None,(0,s-1)) for s in stride)))\n    x = x.reshape(None, None, *[k*s for k,s in zip(x.shape[2::2], stride)])\n    x = x.shrink((None, None, *[(0,k-(s-1)) for k,s in zip(x.shape[2:], stride)]))\n  padding = flatten((((k-1)*d-pB,(k-1)*d-pA+op) for k,d,(pB,pA),op in reversed(list(zip(HW, dilation, padding, output_padding)))))\n  return x.conv2d(w.flatten(end_dim=1), groups=groups, bias=bias, dilation=dilation, padding=padding)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.dot","title":"dot","text":"<pre><code>dot(w: Tensor, dtype: DTypeLike | None = None) -&gt; Tensor\n</code></pre> <p>Performs dot product between two tensors. If <code>w</code> is 1-D, it's a sum product over the last axis of <code>self</code> and <code>w</code>. If <code>w</code> is N-D with N&gt;=2, it's a sum product over the last axis of <code>self</code> and the second-to-last axis of <code>w</code>.</p> <p>You can pass in the optional <code>dtype</code> keyword argument to control the data type of the accumulation.</p> <p><pre><code>a = Tensor([1, 2, 3])\nb = Tensor([1, 1, 0])\nprint(a.dot(b).numpy())\n</code></pre> <pre><code>3\n</code></pre> <pre><code>a = Tensor([[1, 2], [3, 4]])\nb = Tensor([[5, 6], [7, 8]])\nprint(a.dot(b).numpy())\n</code></pre> <pre><code>[[19 22]\n [43 50]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def dot(self, w:Tensor, dtype:DTypeLike|None=None) -&gt; Tensor:\n\n  \"\"\"\n  Performs dot product between two tensors.\n  If `w` is 1-D, it's a sum product over the last axis of `self` and `w`.\n  If `w` is N-D with N&gt;=2, it's a sum product over the last axis of `self` and the second-to-last axis of `w`.\n\n  You can pass in the optional `dtype` keyword argument to control the data type of the accumulation.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  a = Tensor([1, 2, 3])\n  b = Tensor([1, 1, 0])\n  print(a.dot(b).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  a = Tensor([[1, 2], [3, 4]])\n  b = Tensor([[5, 6], [7, 8]])\n  print(a.dot(b).numpy())\n  ```\n  \"\"\"\n  if IMAGE: return self.image_dot(w, dtype)\n  x, dx, dw = self, self.ndim, w.ndim\n  if not (dx &gt; 0 and dw &gt; 0): raise RuntimeError(f\"both tensors need to be at least 1D, got {dx}D and {dw}D\")\n  if x.shape[-1] != w.shape[axis_w:=-min(w.ndim,2)]: raise RuntimeError(f\"cannot dot {x.shape} and {w.shape}\")\n  x = x.reshape(*x.shape[0:-1], *[1]*min(dx-1, dw-1, 1), x.shape[-1])\n  w = w.reshape(*w.shape[0:-2], *[1]*min(dx-1, dw-1, 1), *w.shape[axis_w:]).transpose(-1, axis_w)\n  return (x*w).sum(-1, dtype=dtype).cast(least_upper_dtype(x.dtype, w.dtype) if dtype is None else dtype)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.matmul","title":"matmul","text":"<pre><code>matmul(\n    x: Tensor, reverse=False, dtype: DTypeLike | None = None\n) -&gt; Tensor\n</code></pre> <p>Performs matrix multiplication between two tensors.</p> <p>You can pass in the <code>reverse</code> keyword argument to control the order of the matrix multiplication. You can pass in the optional <code>dtype</code> keyword argument to control the data type of the accumulation.</p> <pre><code>a = Tensor([[1, 2], [3, 4]])\nb = Tensor([[5, 6], [7, 8]])\nprint(a.matmul(b).numpy())\n</code></pre> <pre><code>[[19 22]\n [43 50]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def matmul(self, x:Tensor, reverse=False, dtype:DTypeLike|None=None) -&gt; Tensor:\n  \"\"\"\n  Performs matrix multiplication between two tensors.\n\n  You can pass in the `reverse` keyword argument to control the order of the matrix multiplication.\n  You can pass in the optional `dtype` keyword argument to control the data type of the accumulation.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  a = Tensor([[1, 2], [3, 4]])\n  b = Tensor([[5, 6], [7, 8]])\n  print(a.matmul(b).numpy())\n  ```\n  \"\"\"\n  return x.dot(self, dtype=dtype) if reverse else self.dot(x, dtype=dtype)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.einsum","title":"einsum  <code>staticmethod</code>","text":"<pre><code>einsum(\n    formula: str,\n    *operands: Tensor | Sequence[Tensor],\n    dtype: DTypeLike | None = None\n) -&gt; Tensor\n</code></pre> <p>Sums the product of the elements of the input tensors according to a formula based on the Einstein summation convention.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.einsum.html</p> <pre><code>x = Tensor([[1, 2], [3, 4]])\ny = Tensor([[5, 6], [7, 8]])\nprint(Tensor.einsum(\"ij,ij-&gt;\", x, y).numpy())\n</code></pre> <pre><code>70\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef einsum(formula:str, *operands:Tensor|Sequence[Tensor], dtype:DTypeLike|None=None) -&gt; Tensor:\n  \"\"\"\n  Sums the product of the elements of the input tensors according to a formula based on the Einstein summation convention.\n\n  See: https://pytorch.org/docs/stable/generated/torch.einsum.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  x = Tensor([[1, 2], [3, 4]])\n  y = Tensor([[5, 6], [7, 8]])\n  print(Tensor.einsum(\"ij,ij-&gt;\", x, y).numpy())\n  ```\n  \"\"\"\n  def parse_formula(formula:str, *operands:Tensor):\n    if \"...\" in (formula := formula.replace(\" \", \"\")):\n      ell_chars, ell_longest = \"\".join(c for c in string.ascii_letters if c not in formula), 0\n      for i, inp in enumerate(filter(lambda x: \"...\" in x, inputs := formula.split(\"-&gt;\")[0].split(\",\"))):\n        if (ell_count := max(operands[i].ndim, 1) - (len(inp) - len(\"...\"))) &gt; ell_longest: ell_longest = ell_count\n        inputs[i] = inp.replace(\"...\", ell_chars[-ell_count:])\n      inputs_str, out_ellipse = \",\".join(inputs), ell_chars[-ell_longest:]\n      return (inputs_str, formula.split(\"-&gt;\")[1].replace(\"...\", out_ellipse)) if \"-&gt;\" in formula else \\\n        (inputs_str, out_ellipse + ''.join(sorted(c for c in inputs_str if inputs_str.count(c) == 1 and c.isalpha() and c not in out_ellipse)))\n    return formula.split(\"-&gt;\") if \"-&gt;\" in formula else (formula, ''.join(c for c in sorted(formula) if formula.count(c) == 1 and c.isalpha()))\n\n  xs:tuple[Tensor, ...] = argfix(*operands)\n  inputs_str, output = parse_formula(formula, *xs)\n  inputs = inputs_str.split(\",\")\n  if len(xs)!=len(inputs): raise ValueError(f\"number of inputs doesn't match number of operands in formula, expected {len(inputs)}, got {len(xs)}\")\n\n  # map the value of each letter in the formula\n  letter_val = sorted(merge_dicts([dict(zip(letters, tensor.shape)) for letters, tensor in zip(inputs, xs)]).items())\n\n  xs_:list[Tensor] = []\n  lhs = [sorted(enumerate(s), key=lambda e:e[1]) for s in inputs]\n  for x,(order,letters) in zip(xs, [list(zip(*l)) for l in lhs]):\n    # permute to the sorted letter order, then reshape/expand to create dimensions for the missing letters\n    xs_.append(x.permute(order).reshape([val if letter in letters else 1 for letter,val in letter_val]).expand([val for _,val in letter_val]))\n\n  # ordinal encode the output alphabet\n  rhs_order = argsort(argsort(list(output)))\n\n  # sum over all axes that's not in the output, then permute to the output order\n  return functools.reduce(lambda a,b:a*b, xs_) \\\n    .sum(axis=[axis for axis,(letter,_) in enumerate(letter_val) if letter not in output], dtype=dtype).permute(rhs_order)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.cumsum","title":"cumsum","text":"<pre><code>cumsum(axis: int = 0) -&gt; Tensor\n</code></pre> <p>Computes the cumulative sum of the tensor along the specified <code>axis</code>.</p> <p><pre><code>t = Tensor.ones(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[1. 1. 1.]\n [1. 1. 1.]]\n</code></pre> <pre><code>print(t.cumsum(1).numpy())\n</code></pre> <pre><code>[[1. 2. 3.]\n [1. 2. 3.]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cumsum(self, axis:int=0) -&gt; Tensor:\n  \"\"\"\n  Computes the cumulative sum of the tensor along the specified `axis`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.ones(2, 3)\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.cumsum(1).numpy())\n  ```\n  \"\"\"\n  return self._split_cumalu(axis, Ops.ADD)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.cummax","title":"cummax","text":"<pre><code>cummax(axis: int = 0) -&gt; Tensor\n</code></pre> <p>Computes the cumulative max of the tensor along the specified <code>axis</code>.</p> <p><pre><code>t = Tensor([0, 1, -1, 2, -2, 3, -3])\nprint(t.numpy())\n</code></pre> <pre><code>[ 0  1 -1  2 -2  3 -3]\n</code></pre> <pre><code>print(t.cummax(0).numpy())\n</code></pre> <pre><code>[0 1 1 2 2 3 3]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cummax(self, axis:int=0) -&gt; Tensor:\n  \"\"\"\n  Computes the cumulative max of the tensor along the specified `axis`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([0, 1, -1, 2, -2, 3, -3])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.cummax(0).numpy())\n  ```\n  \"\"\"\n  return self._split_cumalu(axis, Ops.MAX)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.triu","title":"triu","text":"<pre><code>triu(diagonal: int = 0) -&gt; Tensor\n</code></pre> <p>Returns the upper triangular part of the tensor, the other elements are set to 0.</p> <p>The argument <code>diagonal</code> determines which diagonal is on the boundary. <code>diagonal = 0</code> means the main diagonal. Positive <code>diagonal</code> means above the main diagonal, and negative <code>diagonal</code> means below the main diagonal.</p> <p><pre><code>t = Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n</code></pre> <pre><code>print(t.triu(diagonal=0).numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [ 0  6  7  8]\n [ 0  0 11 12]]\n</code></pre> <pre><code>print(t.triu(diagonal=1).numpy())\n</code></pre> <pre><code>[[ 0  2  3  4]\n [ 0  0  7  8]\n [ 0  0  0 12]]\n</code></pre> <pre><code>print(t.triu(diagonal=-1).numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 0 10 11 12]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def triu(self, diagonal:int=0) -&gt; Tensor:\n  \"\"\"\n  Returns the upper triangular part of the tensor, the other elements are set to 0.\n\n  The argument `diagonal` determines which diagonal is on the boundary. `diagonal = 0` means the main diagonal.\n  Positive `diagonal` means above the main diagonal, and negative `diagonal` means below the main diagonal.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.triu(diagonal=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.triu(diagonal=1).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.triu(diagonal=-1).numpy())\n  ```\n  \"\"\"\n  return Tensor._tri(self.shape[-2], self.shape[-1], diagonal=diagonal, device=self.device, dtype=dtypes.bool).where(self, self.zeros_like())\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.tril","title":"tril","text":"<pre><code>tril(diagonal: int = 0) -&gt; Tensor\n</code></pre> <p>Returns the lower triangular part of the tensor, the other elements are set to 0.</p> <p>The argument <code>diagonal</code> determines which diagonal is on the boundary. <code>diagonal = 0</code> means the main diagonal. Positive <code>diagonal</code> means above the main diagonal, and negative <code>diagonal</code> means below the main diagonal.</p> <p><pre><code>t = Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n</code></pre> <pre><code>print(t.tril(diagonal=0).numpy())\n</code></pre> <pre><code>[[ 1  0  0  0]\n [ 5  6  0  0]\n [ 9 10 11  0]]\n</code></pre> <pre><code>print(t.tril(diagonal=1).numpy())\n</code></pre> <pre><code>[[ 1  2  0  0]\n [ 5  6  7  0]\n [ 9 10 11 12]]\n</code></pre> <pre><code>print(t.tril(diagonal=-1).numpy())\n</code></pre> <pre><code>[[ 0  0  0  0]\n [ 5  0  0  0]\n [ 9 10  0  0]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tril(self, diagonal:int=0) -&gt; Tensor:\n  \"\"\"\n  Returns the lower triangular part of the tensor, the other elements are set to 0.\n\n  The argument `diagonal` determines which diagonal is on the boundary. `diagonal = 0` means the main diagonal.\n  Positive `diagonal` means above the main diagonal, and negative `diagonal` means below the main diagonal.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.tril(diagonal=0).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.tril(diagonal=1).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.tril(diagonal=-1).numpy())\n  ```\n  \"\"\"\n  return Tensor._tri(self.shape[-2], self.shape[-1], diagonal=diagonal+1, device=self.device, dtype=dtypes.bool).where(self.zeros_like(), self)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.interpolate","title":"interpolate","text":"<pre><code>interpolate(\n    size: tuple[int, ...],\n    mode: str = \"linear\",\n    align_corners: bool = False,\n) -&gt; Tensor\n</code></pre> <p>Downsamples or Upsamples to the input <code>size</code>, accepts 0 to N batch dimensions.</p> <p>The interpolation algorithm is selected with <code>mode</code> which currently only supports <code>linear</code>, <code>nearest</code> and <code>nearest-exact</code>. To run <code>bilinear</code> or <code>trilinear</code>, pass in a 2D or 3D size.</p> <p><pre><code>t = Tensor([[1, 2, 3, 4], [21, 22, 23, 24], [41, 42, 43, 44]])\nprint(t.numpy())\n</code></pre> <pre><code>[[ 1  2  3  4]\n [21 22 23 24]\n [41 42 43 44]]\n</code></pre> <pre><code>print(t.interpolate(size=(2,3), mode=\"linear\").numpy())\n</code></pre> <pre><code>[[ 6  7  8]\n [36 37 38]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def interpolate(self, size:tuple[int, ...], mode:str=\"linear\", align_corners:bool=False) -&gt; Tensor:\n  \"\"\"\n  Downsamples or Upsamples to the input `size`, accepts 0 to N batch dimensions.\n\n  The interpolation algorithm is selected with `mode` which currently only supports `linear`, `nearest` and `nearest-exact`.\n  To run `bilinear` or `trilinear`, pass in a 2D or 3D size.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2, 3, 4], [21, 22, 23, 24], [41, 42, 43, 44]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.interpolate(size=(2,3), mode=\"linear\").numpy())\n  ```\n  \"\"\"\n  assert isinstance(size, (tuple,list)) and all_int(size) and 0 &lt; len(size) &lt;= self.ndim, f\"invalid {size=}\"\n  assert mode in (\"linear\", \"nearest\", \"nearest-exact\"), \"only supports linear, nearest or nearest-exact interpolate\"\n  assert not (align_corners and mode != \"linear\"), \"align_corners option can only be set with the interpolating mode linear\"\n  x, expand = self, list(self.shape)\n  for i in range(-1,-len(size)-1,-1):\n    scale = (self.shape[i] - int(align_corners)) / (size[i] - int(align_corners))\n    arr, reshape = Tensor.arange(size[i], dtype=dtypes.float32, device=self.device), [1] * self.ndim\n    reshape[i] = expand[i] = size[i]\n    if mode == \"linear\":\n      index = (scale*arr if align_corners else (scale*(arr+0.5))-0.5).clip(0, self.shape[i]-1)\n      low, high, perc = [y.reshape(reshape).expand(expand) for y in (index.floor().int(), index.ceil().int(), index - index.floor())]\n      x = x.gather(i, low).lerp(x.gather(i, high), perc)\n    else:\n      index = (scale*(arr+0.5) if mode==\"nearest-exact\" else scale*arr).cast(dtypes.int32).reshape(reshape).expand(expand)\n      x = x.gather(i, index)\n  return x.cast(self.dtype)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.scatter","title":"scatter","text":"<pre><code>scatter(\n    dim: int,\n    index: Tensor,\n    src: Tensor | ConstType,\n    reduce: Literal[\"multiply\", \"add\"] | None = None,\n) -&gt; Tensor\n</code></pre> <p>Scatters <code>src</code> values along an axis specified by <code>dim</code>. Apply <code>add</code> or <code>multiply</code> reduction operation with <code>reduce</code>.</p> <p>Note</p> <p>To use the <code>reduce</code> argument with a Tensor <code>src</code>, see <code>Tensor.scatter_reduce</code>.</p> <p><pre><code>src = Tensor.arange(1, 11).reshape(2, 5)\nprint(src.numpy())\n</code></pre> <pre><code>[[ 1  2  3  4  5]\n [ 6  7  8  9 10]]\n</code></pre> <pre><code>index = Tensor([[0, 1, 2, 0]])\nprint(Tensor.zeros(3, 5, dtype=src.dtype).scatter(0, index, src).numpy())\n</code></pre> <pre><code>[[1 0 0 4 0]\n [0 2 0 0 0]\n [0 0 3 0 0]]\n</code></pre> <pre><code>index = Tensor([[0, 1, 2], [0, 1, 4]])\nprint(Tensor.zeros(3, 5, dtype=src.dtype).scatter(1, index, src).numpy())\n</code></pre> <pre><code>[[1 2 3 0 0]\n [6 7 0 0 8]\n [0 0 0 0 0]]\n</code></pre> <pre><code>print(Tensor.full((2, 4), 2.0).scatter(1, Tensor([[2], [3]]), 1.23, reduce='multiply').numpy())\n</code></pre> <pre><code>[[2.   2.   2.46 2.  ]\n [2.   2.   2.   2.46]]\n</code></pre> <pre><code>print(Tensor.full((2, 4), 2.0).scatter(1, Tensor([[2], [3]]), 1.23, reduce='add').numpy())\n</code></pre> <pre><code>[[2.   2.   3.23 2.  ]\n [2.   2.   2.   3.23]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def scatter(self, dim:int, index:Tensor, src:Tensor|ConstType, reduce:Literal['multiply', 'add']|None=None) -&gt; Tensor:\n  \"\"\"\n  Scatters `src` values along an axis specified by `dim`.\n  Apply `add` or `multiply` reduction operation with `reduce`.\n\n  NOTE: To use the `reduce` argument with a Tensor `src`, see `Tensor.scatter_reduce`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  src = Tensor.arange(1, 11).reshape(2, 5)\n  print(src.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  index = Tensor([[0, 1, 2, 0]])\n  print(Tensor.zeros(3, 5, dtype=src.dtype).scatter(0, index, src).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  index = Tensor([[0, 1, 2], [0, 1, 4]])\n  print(Tensor.zeros(3, 5, dtype=src.dtype).scatter(1, index, src).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.full((2, 4), 2.0).scatter(1, Tensor([[2], [3]]), 1.23, reduce='multiply').numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.full((2, 4), 2.0).scatter(1, Tensor([[2], [3]]), 1.23, reduce='add').numpy())\n  ```\n  \"\"\"\n  if reduce not in {None, \"add\", \"multiply\"}: raise TypeError(f\"{reduce=} must be one of None, 'multiply', or 'add'\")\n  if reduce and isinstance(src, Tensor): raise TypeError(\"Tensor src is not supported with reduce arg. see scatter_reduce\")\n  if not isinstance(src, Tensor): src = index.full_like(src, device=self.device, dtype=self.dtype)\n  if reduce == \"add\": return self.scatter_reduce(dim, index, src, \"sum\", include_self=True)\n  if reduce == \"multiply\": return self.scatter_reduce(dim, index, src, \"prod\", include_self=True)\n  src, mask = self._pre_scatter(dim, index, src)\n  return _masked_setitem(self, src, mask, (-1,))\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.scatter_reduce","title":"scatter_reduce","text":"<pre><code>scatter_reduce(\n    dim: int,\n    index: Tensor,\n    src: Tensor,\n    reduce: Literal[\"sum\", \"prod\", \"mean\", \"amax\", \"amin\"],\n    include_self: bool = True,\n) -&gt; Tensor\n</code></pre> <p>Scatters <code>src</code> values along an axis specified by <code>dim</code>. Apply <code>\"sum\"</code>, <code>\"prod\"</code>, <code>\"mean\"</code>, <code>\"amax\"</code>, or <code>\"amin\"</code> reduction operations with <code>reduce</code>.</p> <p>Set <code>include_self=False</code> to exclude values in the <code>self</code> Tensor from the reduction.</p> <p><pre><code>src = Tensor.arange(1, 11).cast(dtypes.float).reshape(2, 5)\nprint(src.numpy())\nindex = Tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\nprint(index.numpy())\n</code></pre> <pre><code>[[ 1.  2.  3.  4.  5.]\n [ 6.  7.  8.  9. 10.]]\n[[0 0 0 0 0]\n [0 0 0 0 0]]\n</code></pre> <pre><code>print(Tensor.ones(1, 5, dtype=src.dtype).scatter_reduce(0, index, src, reduce='sum').numpy())\n</code></pre> <pre><code>[[ 8. 10. 12. 14. 16.]]\n</code></pre> <pre><code>print(Tensor.ones(1, 5, dtype=src.dtype).scatter_reduce(0, index, src, reduce='prod').numpy())\n</code></pre> <pre><code>[[ 6. 14. 24. 36. 50.]]\n</code></pre> <pre><code>print(Tensor.ones(1, 5, dtype=src.dtype).scatter_reduce(0, index, src, reduce='mean', include_self=False).numpy())\n</code></pre> <pre><code>[[3.5 4.5 5.5 6.5 7.5]]\n</code></pre> <pre><code>print(Tensor([[-10, 20, 0, 5, 10]], dtype=src.dtype).scatter_reduce(0, index, src, reduce='amax').numpy())\n</code></pre> <pre><code>[[ 6. 20.  8.  9. 10.]]\n</code></pre> <pre><code>print(Tensor([[-10, 20, 0, 5, 10]], dtype=src.dtype).scatter_reduce(0, index, src, reduce='amin').numpy())\n</code></pre> <pre><code>[[-10.   2.   0.   4.   5.]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def scatter_reduce(self, dim:int, index:Tensor, src:Tensor, reduce:Literal[\"sum\", \"prod\", \"mean\", \"amax\", \"amin\"],\n                   include_self:bool=True) -&gt; Tensor:\n  \"\"\"\n  Scatters `src` values along an axis specified by `dim`.\n  Apply `\"sum\"`, `\"prod\"`, `\"mean\"`, `\"amax\"`, or `\"amin\"` reduction operations with `reduce`.\n\n  Set `include_self=False` to exclude values in the `self` Tensor from the reduction.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  src = Tensor.arange(1, 11).cast(dtypes.float).reshape(2, 5)\n  print(src.numpy())\n  index = Tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\n  print(index.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(1, 5, dtype=src.dtype).scatter_reduce(0, index, src, reduce='sum').numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(1, 5, dtype=src.dtype).scatter_reduce(0, index, src, reduce='prod').numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor.ones(1, 5, dtype=src.dtype).scatter_reduce(0, index, src, reduce='mean', include_self=False).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([[-10, 20, 0, 5, 10]], dtype=src.dtype).scatter_reduce(0, index, src, reduce='amax').numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(Tensor([[-10, 20, 0, 5, 10]], dtype=src.dtype).scatter_reduce(0, index, src, reduce='amin').numpy())\n  ```\n  \"\"\"\n  src, mask = self._pre_scatter(dim, index, src)\n  def _inv_mask(a:Tensor|ConstType, b:Tensor|ConstType) -&gt; Tensor: return mask.any(-1).logical_not().where(a, b)\n  if reduce == \"sum\": return mask.where(src, 0).sum(-1).add(self if include_self else _inv_mask(self, 0))\n  if reduce == \"prod\": return mask.where(src, 1).prod(-1).mul(self if include_self else _inv_mask(self, 1))\n  if reduce == \"amax\": return mask.where(src, m := dtypes.min(src.dtype)).max(-1).maximum(self if include_self else _inv_mask(self, m))\n  if reduce == \"amin\": return mask.where(src, m := dtypes.max(src.dtype)).min(-1).minimum(self if include_self else _inv_mask(self, m))\n  if reduce == \"mean\":\n    count = mask.where(1, 0).sum(-1).add(1 if include_self else _inv_mask(1, 0))\n    return mask.where(src, 0).sum(-1).add(self if include_self else _inv_mask(self, 0)).div(count)\n  raise RuntimeError(f\"{reduce=} must be one of 'sum', 'prod', 'mean', 'amax', 'amin'\")\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.masked_select","title":"masked_select","text":"<pre><code>masked_select(mask)\n</code></pre> <p>Selects elements from <code>self</code> based on the boolean <code>mask</code>.</p> <p><pre><code>t = Tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\nmask = Tensor([[True, False, True], [False, True, False], [False, False, True]])\nprint(t.numpy())\nprint(mask.numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]\n [6 7 8]]\n[[ True False  True]\n [False  True False]\n [False False  True]]\n</code></pre> <pre><code>print(t.masked_select(mask).numpy())\n</code></pre> <pre><code>[0 2 4 8]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def masked_select(self, mask):\n  \"\"\"\n  Selects elements from `self` based on the boolean `mask`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n  mask = Tensor([[True, False, True], [False, True, False], [False, False, True]])\n  print(t.numpy())\n  print(mask.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.masked_select(mask).numpy())\n  ```\n  \"\"\"\n  if not dtypes.is_bool(mask.dtype): raise RuntimeError(f\"masked_select expects bool mask tensor, got {mask.dtype}\")\n  x, mask = self.flatten(), mask._broadcast_to(self.shape).flatten()\n  mask_cumsum = mask.cumsum()\n  counts = Tensor.zeros(mask_cumsum[-1].item(), dtype=dtypes.int32)\n  idxs = counts.scatter(0, mask_cumsum, 1, reduce='add').cumsum()\n  return x[idxs]\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.masked_fill","title":"masked_fill","text":"<pre><code>masked_fill(\n    mask: Tensor, value: Tensor | ConstType\n) -&gt; Tensor\n</code></pre> <p>Replaces <code>self</code> with <code>value</code> wherever the elements of <code>mask</code> are True.</p> <p><pre><code>t = Tensor([1, 2, 3, 4, 5])\nmask = Tensor([True, False, True, False, False])\nprint(t.masked_fill(mask, -12).numpy())\n</code></pre> <pre><code>[-12   2 -12   4   5]\n</code></pre> <pre><code>t = Tensor([1, 2, 3, 4, 5])\nmask = Tensor([True, False, True, False, False])\nvalue = Tensor([-1, -2, -3, -4, -5])\nprint(t.masked_fill(mask, value).numpy())\n</code></pre> <pre><code>[-1  2 -3  4  5]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def masked_fill(self:Tensor, mask:Tensor, value:Tensor|ConstType) -&gt; Tensor:\n  \"\"\"\n  Replaces `self` with `value` wherever the elements of `mask` are True.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4, 5])\n  mask = Tensor([True, False, True, False, False])\n  print(t.masked_fill(mask, -12).numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4, 5])\n  mask = Tensor([True, False, True, False, False])\n  value = Tensor([-1, -2, -3, -4, -5])\n  print(t.masked_fill(mask, value).numpy())\n  ```\n  \"\"\"\n  return mask.where(value, self)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.sort","title":"sort","text":"<pre><code>sort(\n    dim: int = -1, descending: bool = False\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Performs a bitonic sort on the tensor along the specified dimension.</p> <p>Order of indices for equivalent elements is always preserved.</p> <p>See: https://en.wikipedia.org/wiki/Bitonic_sorter</p> <p><pre><code>t = Tensor([[0.1, 0.5, 1.2, 3.4, 2.1], [2.2, 1.9, 0.3, 4.5, 0.8]])\nprint(t.numpy())\n</code></pre> <pre><code>[[0.1 0.5 1.2 3.4 2.1]\n [2.2 1.9 0.3 4.5 0.8]]\n</code></pre> <pre><code>sorted_values, indices = t.sort(dim=1, descending=True)\nprint(sorted_values.numpy())\nprint(indices.numpy())\n</code></pre> <pre><code>[[3.4 2.1 1.2 0.5 0.1]\n [4.5 2.2 1.9 0.8 0.3]]\n[[3 4 2 1 0]\n [3 0 1 4 2]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sort(self, dim:int=-1, descending:bool=False) -&gt; tuple[Tensor, Tensor]:\n  \"\"\"\n  Performs a bitonic sort on the tensor along the specified dimension.\n\n  Order of indices for equivalent elements is always preserved.\n\n  See: https://en.wikipedia.org/wiki/Bitonic_sorter\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[0.1, 0.5, 1.2, 3.4, 2.1], [2.2, 1.9, 0.3, 4.5, 0.8]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  sorted_values, indices = t.sort(dim=1, descending=True)\n  print(sorted_values.numpy())\n  print(indices.numpy())\n  ```\n  \"\"\"\n  x, dim = self, self._resolve_dim(dim)\n  if (orig_len:= x.shape[dim]) &lt;= 1: return x, x.zeros_like(dtype=dtypes.default_int)\n  # pad to power of 2\n  n_stages = (orig_len-1).bit_length()\n  pads = tuple((0, 2**n_stages - orig_len) if i == dim else None for i in range(x.ndim))\n  x = x.pad(pads, value=dtypes.min(x.dtype) if descending else dtypes.max(x.dtype)).unflatten(dim, (2,)*n_stages)\n  # https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort1.svg\n  for stage in range(1, n_stages+1):\n    if stage != n_stages:\n      # flip so arrows of green boxes point the same way as blue boxes\n      crossover_dim = dim + n_stages - stage - 1\n      blue_box, green_box = x.split(1, crossover_dim)\n      flip_dims = tuple(-i for i in range(1, stage+1+(self.ndim-dim)))\n      x = (blue_box.cat(green_box.flip(flip_dims), dim=crossover_dim)).contiguous()\n    for substage in range(stage-1, -1, -1):\n      partner_dim = dim + n_stages - substage - 1\n      x_top, x_bottom = x.split(1, partner_dim)\n      x_larger, x_smaller = x_top.maximum(x_bottom), x_top.minimum(x_bottom)\n      x = (x_larger.cat(x_smaller, dim=partner_dim) if descending else x_smaller.cat(x_larger, dim=partner_dim)).contiguous()\n    if stage != n_stages:\n      # flip wires back to undo the crossover\n      blue_box, flipped_green_box = x.split(1, crossover_dim)\n      x = blue_box.cat(flipped_green_box.flip(flip_dims), dim=crossover_dim)\n  x = x.flatten(dim, dim+n_stages-1).shrink(tuple((0, s) for s in self.shape))\n  # compute indices for sorted values\n  mask = Tensor.ones(orig_len, orig_len, dtype=dtypes.bool, device=self.device).tril().reshape((None, None) + (1,)*(self.ndim-dim-1))\n  def compute_counts(t:Tensor): return (mask &amp; (t.unsqueeze(dim) == t.unsqueeze(dim+1))).sum(dim+1)\n  count_orig, count_sorted = compute_counts(self), compute_counts(x)\n  cond = (self.unsqueeze(dim+1) == x.unsqueeze(dim)) &amp; (count_orig.unsqueeze(dim+1) == count_sorted.unsqueeze(dim))\n  idx = Tensor.arange(orig_len, device=self.device).reshape(tuple(orig_len if i == dim else 1 for i in range(x.ndim)))\n  idx = (cond * idx.unsqueeze(dim+1)).sum(dim)\n  return x, idx\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.topk","title":"topk","text":"<pre><code>topk(\n    k: int,\n    dim: int = -1,\n    largest: bool = True,\n    sorted_: bool = True,\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Computes the top-k elements of the tensor along the specified <code>dim</code>.</p> <p>Order of indices for equivalent elements is always preserved.</p> <p><pre><code>t = Tensor([[0.1, 0.5, 1.2, 3.4, 2.1], [2.2, 1.9, 0.3, 4.5, 0.8]])\nprint(t.numpy())\n</code></pre> <pre><code>[[0.1 0.5 1.2 3.4 2.1]\n [2.2 1.9 0.3 4.5 0.8]]\n</code></pre> <pre><code>topk_values, topk_indices = t.topk(2, dim=1)\nprint(topk_values.numpy())\nprint(topk_indices.numpy())\n</code></pre> <pre><code>[[3.4 2.1]\n [4.5 2.2]]\n[[3 4]\n [3 0]]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def topk(self, k:int, dim:int=-1, largest:bool=True, sorted_:bool=True) -&gt; tuple[Tensor, Tensor]:\n  \"\"\"\n  Computes the top-k elements of the tensor along the specified `dim`.\n\n  Order of indices for equivalent elements is always preserved.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[0.1, 0.5, 1.2, 3.4, 2.1], [2.2, 1.9, 0.3, 4.5, 0.8]])\n  print(t.numpy())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  topk_values, topk_indices = t.topk(2, dim=1)\n  print(topk_values.numpy())\n  print(topk_indices.numpy())\n  ```\n  \"\"\"\n  if not sorted_: raise NotImplementedError(\"topk with sorted_=False is not supported\")\n  if k &gt; self.shape[dim:=self._resolve_dim(dim)]: raise ValueError(f\"selected index {k=} is out of range\")\n  x, idx = self.sort(dim, descending=largest)\n  shrink_to_k = tuple((0, k) if i == dim else None for i in range(self.ndim))\n  return x.shrink(shrink_to_k), idx.shrink(shrink_to_k)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.multinomial","title":"multinomial","text":"<pre><code>multinomial(\n    num_samples: int = 1, replacement: bool = False\n) -&gt; Tensor\n</code></pre> <p>Returns a tensor with <code>num_samples</code> indices sampled from a multinomial distribution weighted by <code>self</code>.</p> <p>Note</p> <p><code>replacement=False</code> for <code>num_samples &gt; 1</code> is not supported yet. <pre><code>Tensor.manual_seed(42)\nt = Tensor([1, 2, 3, 4])\nprint(t.multinomial(20, replacement=True).numpy())\n</code></pre> <pre><code>[2 1 3 2 3 1 2 2 3 3 3 3 3 3 2 3 2 3 3 3]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def multinomial(self:Tensor, num_samples:int = 1, replacement:bool = False) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with `num_samples` indices sampled from a multinomial distribution weighted by `self`.\n\n  NOTE: `replacement=False` for `num_samples &gt; 1` is not supported yet.\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor([1, 2, 3, 4])\n  print(t.multinomial(20, replacement=True).numpy())\n  ```\n  \"\"\"\n  assert 1 &lt;= self.ndim &lt;= 2 and num_samples &gt; 0, f\"{self.ndim=} must be 1 or 2 dim, {num_samples=} must be positive\"\n  assert replacement or num_samples == 1, \"no replacement only supports num_samples = 1\"\n  weight = self.unsqueeze(0) if self.ndim == 1 else self\n  cdf = (cw := weight.cumsum(1).float()) / cw[:, -1].unsqueeze(1)\n  unif_samples = Tensor.rand(num_samples, cdf.shape[0], 1).to(self.device)\n  indices = (unif_samples.expand((-1, -1, cdf.shape[1])) &gt;= cdf).sum(2).permute((1, 0))\n  return (indices.squeeze(0) if self.ndim == 1 else indices).cast(dtypes.int32)\n</code></pre>"},{"location":"tensor/ops/#neural-network-functional","title":"Neural Network (functional)","text":""},{"location":"tensor/ops/#tinygrad.Tensor.linear","title":"linear","text":"<pre><code>linear(\n    weight: Tensor,\n    bias: Tensor | None = None,\n    dtype: DTypeLike | None = None,\n) -&gt; Tensor\n</code></pre> <p>Applies a linear transformation to <code>self</code> using <code>weight</code> and <code>bias</code>.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html</p> <pre><code>t = Tensor([[1, 2], [3, 4]])\nweight = Tensor([[1, 2], [3, 4]])\nbias = Tensor([1, 2])\nprint(t.linear(weight, bias).numpy())\n</code></pre> <pre><code>[[ 8 12]\n [16 24]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def linear(self, weight:Tensor, bias:Tensor|None=None, dtype:DTypeLike|None=None) -&gt; Tensor:\n  \"\"\"\n  Applies a linear transformation to `self` using `weight` and `bias`.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[1, 2], [3, 4]])\n  weight = Tensor([[1, 2], [3, 4]])\n  bias = Tensor([1, 2])\n  print(t.linear(weight, bias).numpy())\n  ```\n  \"\"\"\n  if dtype is not None: return self.cast(dtype).linear(weight.cast(dtype), bias.cast(dtype) if bias is not None else bias)\n  x = self.mul(weight) if len(weight.shape) == 1 else self.dot(weight)\n  return x.add(bias) if bias is not None else x\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.sequential","title":"sequential","text":"<pre><code>sequential(ll: list[Callable[[Tensor], Tensor]]) -&gt; Tensor\n</code></pre> <p>Applies a sequence of functions to <code>self</code> chaining the output of each function to the input of the next.</p> <pre><code>t = Tensor([1, 2, 3])\nprint(t.sequential([lambda x: x * 2, lambda x: x + 1]).numpy())\n</code></pre> <pre><code>[3 5 7]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sequential(self, ll:list[Callable[[Tensor], Tensor]]) -&gt; Tensor:\n  \"\"\"\n  Applies a sequence of functions to `self` chaining the output of each function to the input of the next.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t.sequential([lambda x: x * 2, lambda x: x + 1]).numpy())\n  ```\n  \"\"\"\n  return functools.reduce(lambda x,f: f(x), ll, self)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.layernorm","title":"layernorm","text":"<pre><code>layernorm(\n    axis: int | tuple[int, ...] = -1, eps: float = 1e-05\n) -&gt; Tensor\n</code></pre> <p>Applies Layer Normalization over a mini-batch of inputs.</p> <ul> <li>Paper: https://arxiv.org/abs/1607.06450v1</li> </ul> <p><pre><code>t = Tensor.randn(8, 10, 16) * 2 + 8\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>7.9793524742126465 2.074720621109009\n</code></pre> <pre><code>t = t.layernorm()\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>7.269673196752535e-10 1.0003894567489624\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def layernorm(self, axis:int|tuple[int,...]=-1, eps:float=1e-5) -&gt; Tensor:\n  \"\"\"\n  Applies Layer Normalization over a mini-batch of inputs.\n\n  - Paper: https://arxiv.org/abs/1607.06450v1\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.randn(8, 10, 16) * 2 + 8\n  print(t.mean().item(), t.std().item())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.layernorm()\n  print(t.mean().item(), t.std().item())\n  ```\n  \"\"\"\n  y = (self - self.mean(axis, keepdim=True))\n  return y.mul((y*y).mean(axis, keepdim=True).add(eps).rsqrt())\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.batchnorm","title":"batchnorm","text":"<pre><code>batchnorm(\n    weight: Tensor | None,\n    bias: Tensor | None,\n    mean: Tensor,\n    invstd: Tensor,\n    axis: int | tuple[int, ...] = 1,\n) -&gt; Tensor\n</code></pre> <p>Applies Batch Normalization over a mini-batch of inputs.</p> <ul> <li>Paper: https://arxiv.org/abs/1502.03167</li> </ul> <p><pre><code>t = Tensor.randn(8, 4, 16, 16) * 2 + 8\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>8.019729614257812 1.9927232265472412\n</code></pre> <pre><code>t = t.batchnorm(None, None, t.mean(axis=(0,2,3)), t.var(axis=(0,2,3)).add(1e-5).rsqrt())\nprint(t.mean().item(), t.std().item())\n</code></pre> <pre><code>6.119149134065083e-07 0.9998146891593933\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def batchnorm(self, weight:Tensor|None, bias:Tensor|None, mean:Tensor, invstd:Tensor, axis:int|tuple[int, ...]=1) -&gt; Tensor:\n  \"\"\"\n  Applies Batch Normalization over a mini-batch of inputs.\n\n  - Paper: https://arxiv.org/abs/1502.03167\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.randn(8, 4, 16, 16) * 2 + 8\n  print(t.mean().item(), t.std().item())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = t.batchnorm(None, None, t.mean(axis=(0,2,3)), t.var(axis=(0,2,3)).add(1e-5).rsqrt())\n  print(t.mean().item(), t.std().item())\n  ```\n  \"\"\"\n  axis_ = argfix(axis)\n  shape = tuple(s if ax in axis_ else 1 for ax, s in enumerate(self.shape))\n  x = self - mean.reshape(shape)\n  if weight is not None: x = x * weight.reshape(shape)\n  ret = x.mul(invstd.reshape(shape) if len(invstd.shape) == len(axis_) else invstd)\n  return (ret + bias.reshape(shape)) if bias is not None else ret\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.dropout","title":"dropout","text":"<pre><code>dropout(p=0.5) -&gt; Tensor\n</code></pre> <p>Applies dropout to <code>self</code>.</p> <p>Note</p> <p>dropout is only applied when <code>Tensor.training</code> is <code>True</code>.</p> <ul> <li>Paper: https://jmlr.org/papers/v15/srivastava14a.html</li> </ul> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 2)\nwith Tensor.train():\n  print(t.dropout().numpy())\n</code></pre> <pre><code>[[-1.0287  2.17  ]\n [ 1.8178  0.    ]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def dropout(self, p=0.5) -&gt; Tensor:\n  \"\"\"\n  Applies dropout to `self`.\n\n  NOTE: dropout is only applied when `Tensor.training` is `True`.\n\n  - Paper: https://jmlr.org/papers/v15/srivastava14a.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 2)\n  with Tensor.train():\n    print(t.dropout().numpy())\n  ```\n  \"\"\"\n  if not 0 &lt;= p &lt;= 1: raise ValueError(f\"{p=} is out of range [0, 1]\")\n  if not Tensor.training or p == 0: return self\n  if p == 1: return self.zeros_like()\n  return (Tensor.rand_like(self, requires_grad=False, dtype=dtypes.default_float, contiguous=False) &gt;= p).contiguous().where(self, 0) / (1.0 - p)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.one_hot","title":"one_hot","text":"<pre><code>one_hot(num_classes: int = -1) -&gt; Tensor\n</code></pre> <p>Converts <code>self</code> to a one-hot tensor.</p> <p><code>num_classes</code> defaults to -1, which means num_classes will be inferred as max(self) + 1.</p> <pre><code>t = Tensor([0, 1, 3, 3, 4])\nprint(t.one_hot(5).numpy())\n</code></pre> <pre><code>[[1 0 0 0 0]\n [0 1 0 0 0]\n [0 0 0 1 0]\n [0 0 0 1 0]\n [0 0 0 0 1]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def one_hot(self, num_classes:int=-1) -&gt; Tensor:\n  \"\"\"\n  Converts `self` to a one-hot tensor.\n\n  `num_classes` defaults to -1, which means num_classes will be inferred as max(self) + 1.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([0, 1, 3, 3, 4])\n  print(t.one_hot(5).numpy())\n  ```\n  \"\"\"\n  if not dtypes.is_int(self.dtype): raise RuntimeError(f\"expect integer dtype, getting {self.dtype=}\")\n  if num_classes == -1: num_classes = (self.max()+1).item()\n  return self[..., None]._one_hot_along_dim(num_classes).where(1, 0)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.scaled_dot_product_attention","title":"scaled_dot_product_attention","text":"<pre><code>scaled_dot_product_attention(\n    key: Tensor,\n    value: Tensor,\n    attn_mask: Tensor | None = None,\n    dropout_p: float = 0.0,\n    is_causal: bool = False,\n    enable_gqa: bool = False,\n) -&gt; Tensor\n</code></pre> <p>Computes scaled dot-product attention. <code>self</code> is the query tensor, <code>key</code> is the key tensor, and <code>value</code> is the value tensor.</p> <ul> <li>Paper: https://arxiv.org/abs/1706.03762v7</li> </ul> <pre><code>q = Tensor.randn(2, 4, 8)\nk = Tensor.randn(2, 4, 8)\nv = Tensor.randn(2, 4, 8)\nprint(q.scaled_dot_product_attention(k, v).numpy())\n</code></pre> <pre><code>[[[ 0.6408  0.3264  0.7317 -1.0943  0.5778 -0.0534 -0.0104 -0.0488]\n  [ 0.1243 -0.8259  1.6481 -0.8035 -0.3961  0.4269  0.1232  1.6462]\n  [ 0.9535  0.1068  0.8545 -0.5395  0.4692 -0.0548 -0.2274  0.6152]\n  [ 0.8891 -0.0411  0.7818 -0.3322  0.3931 -0.0202 -0.1101  0.8129]]\n\n [[-0.4273 -0.6085 -0.0465  0.5246  0.3641 -0.0381 -0.0106  0.8349]\n  [ 0.6321  0.3654  0.4137 -0.2327  0.2558  0.1418 -1.27   -0.802 ]\n  [ 0.1794  0.4616  0.1847 -0.1988  0.2123  0.1837 -0.9583 -0.5364]\n  [ 0.4408  0.6125  0.0811 -0.3886  0.3602  0.4987 -1.4414 -0.9565]]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def scaled_dot_product_attention(self, key:Tensor, value:Tensor, attn_mask:Tensor|None=None, dropout_p:float=0.0,\n                                 is_causal:bool=False, enable_gqa:bool=False) -&gt; Tensor:\n  \"\"\"\n  Computes scaled dot-product attention.\n  `self` is the query tensor, `key` is the key tensor, and `value` is the value tensor.\n\n  - Paper: https://arxiv.org/abs/1706.03762v7\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  q = Tensor.randn(2, 4, 8)\n  k = Tensor.randn(2, 4, 8)\n  v = Tensor.randn(2, 4, 8)\n  print(q.scaled_dot_product_attention(k, v).numpy())\n  ```\n  \"\"\"\n  # NOTE: it also works when `key` and `value` have symbolic shape.\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  # GQA: https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n  if enable_gqa:\n    key = key.repeat_interleave(self.shape[-3] // key.shape[-3], dim=-3)\n    value = value.repeat_interleave(self.shape[-3] // value.shape[-3], dim=-3)\n\n  if FUSE_ATTENTION: q, key, value = self.contiguous(), key.contiguous(), value.contiguous()\n  else: q = self\n\n  qk = q.matmul(key.transpose(-2,-1), dtype=least_upper_dtype(q.dtype, key.dtype, dtypes.float32)) / math.sqrt(q.shape[-1])\n  # handle attention mask\n  if is_causal:\n    if attn_mask is not None: raise RuntimeError(\"cannot set attn_mask when is_causal=True\")\n    attn_mask = qk.ones_like(requires_grad=False, device=self.device, dtype=dtypes.bool).tril()\n  if attn_mask is not None:\n    if attn_mask.dtype == dtypes.bool: attn_mask = attn_mask.where(0, -float(\"inf\"))\n    qk = qk + attn_mask\n  attn = qk.cast(self.dtype).softmax(-1).dropout(dropout_p) @ value\n  return attn.fuse() if FUSE_ATTENTION else attn\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.binary_crossentropy","title":"binary_crossentropy","text":"<pre><code>binary_crossentropy(\n    Y: Tensor, reduction: ReductionStr = \"mean\"\n) -&gt; Tensor\n</code></pre> <p>Computes the binary cross-entropy loss between <code>self</code> and <code>Y</code>.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html</p> <pre><code>t = Tensor([0.1, 0.9, 0.2])\nY = Tensor([0, 1, 0])\nprint(t.binary_crossentropy(Y).item())\n</code></pre> <pre><code>0.14462155103683472\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def binary_crossentropy(self, Y:Tensor, reduction:ReductionStr=\"mean\") -&gt; Tensor:\n  \"\"\"\n  Computes the binary cross-entropy loss between `self` and `Y`.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([0.1, 0.9, 0.2])\n  Y = Tensor([0, 1, 0])\n  print(t.binary_crossentropy(Y).item())\n  ```\n  \"\"\"\n  return (-Y*self.log() - (1-Y)*(1-self).log())._do_reduction(reduction)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.binary_crossentropy_logits","title":"binary_crossentropy_logits","text":"<pre><code>binary_crossentropy_logits(\n    Y: Tensor,\n    reduction: ReductionStr = \"mean\",\n    pos_weight: Tensor | None = None,\n) -&gt; Tensor\n</code></pre> <p>Computes the binary cross-entropy loss between <code>self</code> and <code>Y</code> where <code>self</code> is logits.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html</p> <pre><code>t = Tensor([-1, 2, -3])\nY = Tensor([0, 1, 0])\nprint(t.binary_crossentropy_logits(Y).item())\n</code></pre> <pre><code>0.16292566061019897\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def binary_crossentropy_logits(self, Y:Tensor, reduction:ReductionStr=\"mean\", pos_weight:Tensor|None=None) -&gt; Tensor:\n  \"\"\"\n  Computes the binary cross-entropy loss between `self` and `Y` where `self` is logits.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([-1, 2, -3])\n  Y = Tensor([0, 1, 0])\n  print(t.binary_crossentropy_logits(Y).item())\n  ```\n  \"\"\"\n  log_p, log_1_minus_p = self.logsigmoid(), (-self).logsigmoid()\n  return (-((1 if pos_weight is None else pos_weight) * Y * log_p + (1-Y) * log_1_minus_p))._do_reduction(reduction)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.sparse_categorical_crossentropy","title":"sparse_categorical_crossentropy","text":"<pre><code>sparse_categorical_crossentropy(\n    Y: Tensor,\n    ignore_index: int = -1,\n    label_smoothing=0.0,\n    reduction: ReductionStr = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Computes the sparse categorical cross-entropy loss between <code>self</code> and <code>Y</code>.</p> <p>Note</p> <p><code>self</code> is logits and <code>Y</code> is the target labels. NOTE: unlike PyTorch, this function expects the class axis to be -1</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html</p> <pre><code>t = Tensor([[-1, 2, -3], [1, -2, 3]])\nY = Tensor([1, 2])\nprint(t.sparse_categorical_crossentropy(Y).item())\n</code></pre> <pre><code>0.09391524642705917\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sparse_categorical_crossentropy(self, Y:Tensor, ignore_index:int=-1, label_smoothing=0.0, reduction:ReductionStr=\"mean\") -&gt; Tensor:\n  \"\"\"\n  Computes the sparse categorical cross-entropy loss between `self` and `Y`.\n\n  NOTE: `self` is logits and `Y` is the target labels.\n  NOTE: unlike PyTorch, this function expects the class axis to be -1\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[-1, 2, -3], [1, -2, 3]])\n  Y = Tensor([1, 2])\n  print(t.sparse_categorical_crossentropy(Y).item())\n  ```\n  \"\"\"\n  assert 0.0 &lt;= label_smoothing &lt;= 1.0, \"label_smoothing must be in [0.0, 1.0]\"\n  assert reduction in get_args(ReductionStr), f\"reduction must be one of {get_args(ReductionStr)}\"\n  log_probs = self.log_softmax()\n  loss_mask = (Y != ignore_index) if ignore_index != -1 else Y.ones_like(dtype=dtypes.bool)\n  y = Y.to(self.device).unsqueeze(-1)._one_hot_along_dim(self.shape[-1], dim=-1) * loss_mask.unsqueeze(-1)\n  smoothing = label_smoothing * (log_probs.mean(-1) * loss_mask)\n  unreduced = ((1 - label_smoothing) * (log_probs * y).sum(-1) + smoothing)\n  # NOTE: because of ignore_index, we can't use Tensor.mean (so can't use `_do_reduction` here)\n  return -(unreduced.sum() / loss_mask.sum() if reduction == \"mean\" else (unreduced.sum() if reduction == \"sum\" else unreduced))\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.cross_entropy","title":"cross_entropy","text":"<pre><code>cross_entropy(\n    Y: Tensor,\n    reduction: ReductionStr = \"mean\",\n    label_smoothing: float = 0.0,\n) -&gt; Tensor\n</code></pre> <p>Computes the cross entropy loss between input logits and target.</p> <p>Note</p> <p><code>self</code> are logits and <code>Y</code> are the target labels or class probabilities.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html</p> <p><pre><code>t = Tensor([[-1, 2, -3], [1, -2, 3]])\nY = Tensor([1, 2])\nprint(t.cross_entropy(Y).item())\n</code></pre> <pre><code>0.09391524642705917\n</code></pre> <pre><code>t = Tensor([[-1, 2, -3], [1, -2, 3]])\nY = Tensor([1, 2])\nprint(t.cross_entropy(Y, reduction='none').numpy())\n</code></pre> <pre><code>[0.055  0.1328]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cross_entropy(self, Y:Tensor, reduction:ReductionStr=\"mean\", label_smoothing:float=0.0) -&gt; Tensor:\n  \"\"\"\n  Computes the cross entropy loss between input logits and target.\n\n  NOTE: `self` are logits and `Y` are the target labels or class probabilities.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[-1, 2, -3], [1, -2, 3]])\n  Y = Tensor([1, 2])\n  print(t.cross_entropy(Y).item())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[-1, 2, -3], [1, -2, 3]])\n  Y = Tensor([1, 2])\n  print(t.cross_entropy(Y, reduction='none').numpy())\n  ```\n  \"\"\"\n  assert 0.0 &lt;= label_smoothing &lt;= 1.0, \"label_smoothing must be in [0.0, 1.0]\"\n  classes_dim = 0 if self.ndim == 1 else 1\n  if self.shape != Y.shape:\n    if self.max(classes_dim).shape != Y.shape: raise RuntimeError(f\"shape mismatch: {self.shape=}, {Y.shape=}\")\n    Y = Y.unsqueeze(classes_dim)._one_hot_along_dim(num_classes=self.shape[classes_dim], dim=classes_dim)\n  Y = (1 - label_smoothing)*Y + label_smoothing / int(Y.shape[classes_dim])\n  return -self.log_softmax(classes_dim).mul(Y).sum(classes_dim)._do_reduction(reduction)\n</code></pre>"},{"location":"tensor/ops/#tinygrad.Tensor.nll_loss","title":"nll_loss","text":"<pre><code>nll_loss(\n    Y: Tensor,\n    weight: Tensor | None = None,\n    ignore_index: int | None = None,\n    reduction: ReductionStr = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Computes the negative log likelihood loss between log-probabilities and target labels.</p> <p>Note</p> <p><code>self</code> is log-probabilities and <code>Y</code> is the Y labels or class probabilities.</p> <p>See: https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html</p> <p><pre><code>t = Tensor([[-1, 2, -3], [1, -2, 3]])\nY = Tensor([1, 2])\nprint(t.log_softmax().nll_loss(Y).item())\n</code></pre> <pre><code>0.09391524642705917\n</code></pre> <pre><code>t = Tensor([[-1, 2, -3], [1, -2, 3]])\nY = Tensor([1, 2])\nprint(t.log_softmax().nll_loss(Y, reduction='none').numpy())\n</code></pre> <pre><code>[0.055  0.1328]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def nll_loss(self, Y:Tensor, weight:Tensor|None=None, ignore_index:int|None=None, reduction:ReductionStr=\"mean\") -&gt; Tensor:\n  \"\"\"\n  Computes the negative log likelihood loss between log-probabilities and target labels.\n\n  NOTE: `self` is log-probabilities and `Y` is the Y labels or class probabilities.\n\n  See: https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[-1, 2, -3], [1, -2, 3]])\n  Y = Tensor([1, 2])\n  print(t.log_softmax().nll_loss(Y).item())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[-1, 2, -3], [1, -2, 3]])\n  Y = Tensor([1, 2])\n  print(t.log_softmax().nll_loss(Y, reduction='none').numpy())\n  ```\n  \"\"\"\n  weight = Tensor.ones_like(Y, requires_grad=False) if weight is None else weight[Y]\n  masked_weight = weight if ignore_index is None else weight * (Y != ignore_index)\n  nll = -self.gather(1, Y.unsqueeze(1)).squeeze(1) * masked_weight\n  return nll.sum() / masked_weight.sum() if reduction == \"mean\" else nll._do_reduction(reduction)\n</code></pre>"},{"location":"tensor/properties/","title":"Properties","text":""},{"location":"tensor/properties/#basic","title":"Basic","text":""},{"location":"tensor/properties/#tinygrad.Tensor.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[sint, ...]\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: DType\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.device","title":"device  <code>property</code>","text":"<pre><code>device: str | tuple[str, ...]\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>Returns the number of dimensions in the tensor.</p> <pre><code>t = Tensor([[1, 2], [3, 4]])\nprint(t.ndim)\n</code></pre> <pre><code>2\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.numel","title":"numel","text":"<pre><code>numel() -&gt; sint\n</code></pre> <p>Returns the total number of elements in the tensor.</p> <pre><code>t = Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nprint(t.numel())\n</code></pre> <pre><code>8\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def numel(self) -&gt; sint:\n  \"\"\"\n  Returns the total number of elements in the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n  print(t.numel())\n  ```\n  \"\"\"\n  return prod(self.shape)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.element_size","title":"element_size","text":"<pre><code>element_size() -&gt; int\n</code></pre> <p>Returns the size in bytes of an individual element in the tensor.</p> <pre><code>t = Tensor([5], dtype=dtypes.int16)\nprint(t.element_size())\n</code></pre> <pre><code>2\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def element_size(self) -&gt; int:\n  \"\"\"\n  Returns the size in bytes of an individual element in the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([5], dtype=dtypes.int16)\n  print(t.element_size())\n  ```\n  \"\"\"\n  return self.dtype.itemsize\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.nbytes","title":"nbytes","text":"<pre><code>nbytes() -&gt; int\n</code></pre> <p>Returns the total number of bytes of all elements in the tensor.</p> <pre><code>t = Tensor([8, 9], dtype=dtypes.float)\nprint(t.nbytes())\n</code></pre> <pre><code>8\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def nbytes(self) -&gt; int:\n  \"\"\"\n  Returns the total number of bytes of all elements in the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([8, 9], dtype=dtypes.float)\n  print(t.nbytes())\n  ```\n  \"\"\"\n  return self.numel() * self.element_size()\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.is_floating_point","title":"is_floating_point","text":"<pre><code>is_floating_point() -&gt; bool\n</code></pre> <p>Returns <code>True</code> if the tensor contains floating point types, i.e. is one of <code>dtypes.float64</code>, <code>dtypes.float32</code>, <code>dtypes.float16</code>, <code>dtypes.bfloat16</code>.</p> <pre><code>t = Tensor([8, 9], dtype=dtypes.float32)\nprint(t.is_floating_point())\n</code></pre> <pre><code>True\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def is_floating_point(self) -&gt; bool:\n  \"\"\"\n  Returns `True` if the tensor contains floating point types, i.e. is one of `dtypes.float64`, `dtypes.float32`,\n  `dtypes.float16`, `dtypes.bfloat16`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([8, 9], dtype=dtypes.float32)\n  print(t.is_floating_point())\n  ```\n  \"\"\"\n  return dtypes.is_float(self.dtype)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.size","title":"size","text":"<pre><code>size(dim: int | None = None) -&gt; sint | tuple[sint, ...]\n</code></pre> <p>Returns the size of the tensor. If <code>dim</code> is specified, return the length along dimension <code>dim</code>. Otherwise return the shape of the tensor.</p> <p><pre><code>t = Tensor([[4, 5, 6], [7, 8, 9]])\nprint(t.size())\n</code></pre> <pre><code>(2, 3)\n</code></pre> <pre><code>print(t.size(dim=1))\n</code></pre> <pre><code>3\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def size(self, dim:int|None=None) -&gt; sint|tuple[sint, ...]:\n  \"\"\"\n  Returns the size of the tensor. If `dim` is specified, return the length along dimension `dim`. Otherwise return the shape of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([[4, 5, 6], [7, 8, 9]])\n  print(t.size())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  print(t.size(dim=1))\n  ```\n  \"\"\"\n  return self.shape if dim is None else self.shape[dim]\n</code></pre>"},{"location":"tensor/properties/#data-access","title":"Data Access","text":""},{"location":"tensor/properties/#tinygrad.Tensor.data","title":"data","text":"<pre><code>data() -&gt; memoryview\n</code></pre> <p>Returns the data of this tensor as a memoryview.</p> <pre><code>t = Tensor([1, 2, 3, 4])\nprint(np.frombuffer(t.data(), dtype=np.int32))\n</code></pre> <pre><code>[1 2 3 4]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def data(self) -&gt; memoryview:\n  \"\"\"\n  Returns the data of this tensor as a memoryview.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(np.frombuffer(t.data(), dtype=np.int32))\n  ```\n  \"\"\"\n  if 0 in self.shape: return memoryview(bytearray(0)).cast(self.dtype.base.fmt)\n  assert all_int(self.shape), f\"no data if shape is symbolic, {self.shape=}\"\n  return self._buffer().as_typed_buffer(self.shape)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.item","title":"item","text":"<pre><code>item() -&gt; ConstType\n</code></pre> <p>Returns the value of this tensor as a standard Python number.</p> <pre><code>t = Tensor(42)\nprint(t.item())\n</code></pre> <pre><code>42\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def item(self) -&gt; ConstType:\n  \"\"\"\n  Returns the value of this tensor as a standard Python number.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor(42)\n  print(t.item())\n  ```\n  \"\"\"\n  assert self.numel() == 1, \"must have one element for item\"\n  return self.data()[(0,) * len(self.shape)]\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.tolist","title":"tolist","text":"<pre><code>tolist() -&gt; Sequence[ConstType] | ConstType\n</code></pre> <p>Returns the value of this tensor as a nested list. Returns single value for const tensor.</p> <p><pre><code>t = Tensor([1, 2, 3, 4])\nprint(t.tolist())\n</code></pre> <pre><code>[1, 2, 3, 4]\n</code></pre> <pre><code>t = Tensor(5)\nprint(t.tolist())\n</code></pre> <pre><code>5\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tolist(self) -&gt; Sequence[ConstType]|ConstType:\n  \"\"\"\n  Returns the value of this tensor as a nested list.\n  Returns single value for const tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(t.tolist())\n  ```\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor(5)\n  print(t.tolist())\n  ```\n  \"\"\"\n  # TODO: remove half once minimum python supports it\n  if self.dtype in (dtypes.half, dtypes.bfloat16, *dtypes.fp8s): return self.cast(dtypes.float32).tolist()\n  return self.data().tolist()\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.numpy","title":"numpy","text":"<pre><code>numpy() -&gt; 'np.ndarray'\n</code></pre> <p>Returns the value of this tensor as a <code>numpy.ndarray</code>.</p> <pre><code>t = Tensor([1, 2, 3, 4])\nprint(repr(t.numpy()))\n</code></pre> <pre><code>array([1, 2, 3, 4], dtype=int32)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def numpy(self) -&gt; 'np.ndarray':  # type: ignore [name-defined] # noqa: F821\n  \"\"\"\n  Returns the value of this tensor as a `numpy.ndarray`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(repr(t.numpy()))\n  ```\n  \"\"\"\n  assert all_int(self.shape), f\"no data if shape is symbolic, {self.shape=}\"\n  import numpy as np\n  if self.dtype.base in { dtypes.bfloat16, *dtypes.fp8s }: return self.float().numpy()\n  if 0 in self.shape: return np.empty(self.shape, dtype=_to_np_dtype(self.dtype.base))\n  return self._buffer().numpy().reshape(self.shape)\n</code></pre>"},{"location":"tensor/properties/#tinygrad-ops","title":"tinygrad ops","text":""},{"location":"tensor/properties/#tinygrad.Tensor.schedule_with_vars","title":"schedule_with_vars","text":"<pre><code>schedule_with_vars(\n    *lst: Tensor,\n) -&gt; tuple[list[ScheduleItem], dict[str, int]]\n</code></pre> <p>Creates the schedule needed to realize these Tensor(s), with Variables.</p> <p>Note</p> <p>A Tensor can only be scheduled once.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def schedule_with_vars(self, *lst:Tensor) -&gt; tuple[list[ScheduleItem], dict[str, int]]:\n  \"\"\"\n  Creates the schedule needed to realize these Tensor(s), with Variables.\n\n  NOTE: A Tensor can only be scheduled once.\n  \"\"\"\n  st = time.perf_counter()\n  self.kernelize(*lst)\n  sink = UOp.sink(*[x.uop for x in (self,)+lst])\n\n  # remove all AFTERs, after scheduling, the tensors are just buffers\n  remove_assign_map = {u:u.buf_uop for u in sink.toposort() if u.op is Ops.AFTER}\n  _apply_map_to_tensors(remove_assign_map, name=\"Remove After\")\n\n  # create the schedule\n  schedule, var_vals = create_schedule_with_vars(sink)\n  schedule = memory_planner(schedule)\n  if (DEBUG &gt;= 1 and len(schedule) &gt; 1) or DEBUG &gt;= 3: print(f\"scheduled {len(schedule)} kernels in {(time.perf_counter()-st)*1000:.2f} ms\")\n  return schedule, var_vals\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.schedule","title":"schedule","text":"<pre><code>schedule(*lst: Tensor) -&gt; list[ScheduleItem]\n</code></pre> <p>Creates the schedule needed to realize these Tensor(s).</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def schedule(self, *lst:Tensor) -&gt; list[ScheduleItem]:\n  \"\"\"Creates the schedule needed to realize these Tensor(s).\"\"\"\n  schedule, var_vals = self.schedule_with_vars(*lst)\n  assert len(var_vals) == 0\n  return schedule\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.realize","title":"realize","text":"<pre><code>realize(*lst: Tensor, do_update_stats=True) -&gt; Tensor\n</code></pre> <p>Triggers the computation needed to create these Tensor(s).</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def realize(self, *lst:Tensor, do_update_stats=True) -&gt; Tensor:\n  \"\"\"Triggers the computation needed to create these Tensor(s).\"\"\"\n  if len(to_realize:=[x for x in (self,)+lst if not x.uop.is_contiguous()]):\n    run_schedule(*Tensor.schedule_with_vars(*to_realize), do_update_stats=do_update_stats)\n  return self\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.replace","title":"replace","text":"<pre><code>replace(x: Tensor, allow_shape_mismatch=False) -&gt; Tensor\n</code></pre> <p>Replaces the data of this tensor with the data of another tensor. Only the shape of the tensors must match.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def replace(self, x:Tensor, allow_shape_mismatch=False) -&gt; Tensor:\n  \"\"\"\n  Replaces the data of this tensor with the data of another tensor. Only the shape of the tensors must match.\n  \"\"\"\n  # used for replacing a Tensor with a new version of it (potentially with a different device and dtype)\n  assert self.shape == x.shape or allow_shape_mismatch, f\"replace shape mismatch {self.shape} != {x.shape}\"\n  self.uop = x.uop\n  return self\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.assign","title":"assign","text":"<pre><code>assign(x) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def assign(self, x) -&gt; Tensor:\n  # TODO: this is a hack for writing to DISK. remove with working assign\n  if isinstance(self.device, str) and self.device.startswith(\"DISK\"):\n    if x.__class__ is not Tensor: x = Tensor(x, device=\"CPU\", dtype=self.dtype)\n    self._buffer().copyin(x._data())\n    return self\n  if x.__class__ is not Tensor: x = Tensor(x, device=self.device, dtype=self.dtype)\n  if self.uop is x.uop: return self  # a self assign is a NOOP\n  # NOTE: we allow cross device assign\n  # broadcast x\n  if least_upper_dtype(self.dtype, x.dtype) == self.dtype: x = x._broadcast_to(self.shape).cast(self.dtype)\n  assert self.shape == x.shape, f\"assign shape mismatch {self.shape} != {x.shape}\"\n  assert self.device == x.device, f\"assign device mismatch {self.device} != {x.device}\"\n  assert self.dtype == x.dtype, f\"assign dtype mismatch {self.dtype} != {x.dtype}\"\n  return self.replace(self._apply_uop(UOp.assign, x))\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.detach","title":"detach","text":"<pre><code>detach() -&gt; Tensor\n</code></pre> <p>Returns a new tensor with the same data as this tensor, but detached from the autograd graph.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def detach(self) -&gt; Tensor:\n  \"\"\"\n  Returns a new tensor with the same data as this tensor, but detached from the autograd graph.\n  \"\"\"\n  return Tensor(self.uop.detach(), device=self.device, requires_grad=False)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.clone","title":"clone","text":"<pre><code>clone() -&gt; Tensor\n</code></pre> <p>Creates a clone of this tensor allocating a separate buffer for the data.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def clone(self) -&gt; Tensor:\n  \"\"\"\n  Creates a clone of this tensor allocating a separate buffer for the data.\n  \"\"\"\n  ret = Tensor.empty(self.shape, device=self.device, dtype=self.dtype)\n  if self.grad is not None: ret.grad = self.grad.clone()\n  return ret.assign(self)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.to","title":"to","text":"<pre><code>to(device: str | tuple[str, ...] | None) -&gt; Tensor\n</code></pre> <p>Moves the tensor to the given device.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def to(self, device:str|tuple[str, ...]|None) -&gt; Tensor:\n  \"\"\"\n  Moves the tensor to the given device.\n  \"\"\"\n  device = tuple(canonicalize_device(x) for x in device) if isinstance(device, (tuple, list)) else canonicalize_device(device)\n  if device == self.device: return self\n  if not isinstance(device, str): return self.shard(device)\n  ret = Tensor(self.uop, device, requires_grad=self.requires_grad)\n  if self.grad is not None: ret.grad = self.grad.to(device)\n  return ret\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.to_","title":"to_","text":"<pre><code>to_(device: str | tuple[str, ...] | None) -&gt; Tensor\n</code></pre> <p>Moves the tensor to the given device in place.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def to_(self, device:str|tuple[str, ...]|None) -&gt; Tensor:\n  \"\"\"\n  Moves the tensor to the given device in place.\n  \"\"\"\n  real = self.to(device)\n  if self.grad is not None and real.grad is not None: self.grad.replace(real.grad)\n  return self.replace(real)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.shard","title":"shard","text":"<pre><code>shard(\n    devices: tuple[str, ...], axis: int | None = None\n) -&gt; Tensor\n</code></pre> <p>Shards the tensor across the given devices. Optionally specify which axis to shard on.</p> <pre><code>t = Tensor.empty(2, 4)\nprint(t.shard((t.device, t.device), axis=1).uop)\n</code></pre> <pre><code>UOp(Ops.MULTI, dtypes.float, arg=1, src=(\n  UOp(Ops.SHRINK, dtypes.float, arg=None, src=(\n    UOp(Ops.COPY, dtypes.float, arg=None, src=(\n      UOp(Ops.RESHAPE, dtypes.float, arg=None, src=(\n        UOp(Ops.BUFFER, dtypes.float, arg=8, src=(\n          UOp(Ops.UNIQUE, dtypes.void, arg=1419, src=()),\n          UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),)),\n        UOp(Ops.VCONST, dtypes.index.vec(2), arg=(2, 4), src=()),)),\n      UOp(Ops.DEVICE, dtypes.void, arg=('CPU', 'CPU'), src=()),)),\n    UOp(Ops.VECTORIZE, dtypes.index.vec(2), arg=None, src=(\n      UOp(Ops.CONST, dtypes.index, arg=0, src=()),\n      x10:=UOp(Ops.MUL, dtypes.index, arg=None, src=(\n        UOp(Ops.DEFINE_VAR, dtypes.index, arg=('_device_num', 0, 1), src=()),\n        x12:=UOp(Ops.CONST, dtypes.index, arg=2, src=()),)),)),\n    UOp(Ops.VECTORIZE, dtypes.index.vec(2), arg=None, src=(\n       x12,\n      UOp(Ops.ADD, dtypes.index, arg=None, src=(\n         x10,\n         x12,)),)),)),))\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def shard(self, devices:tuple[str, ...], axis:int|None=None) -&gt; Tensor:\n  \"\"\"\n  Shards the tensor across the given devices. Optionally specify which axis to shard on.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.empty(2, 4)\n  print(t.shard((t.device, t.device), axis=1).uop)\n  ```\n  \"\"\"\n  assert isinstance(self.device, str), \"can't shard a MultiLazyBuffer\"\n  devices = tuple(canonicalize_device(x) for x in devices)\n  mlb = self.uop.shard(devices, self._resolve_dim(axis)) if axis is not None else self.uop.copy_to_device(devices)\n  return Tensor(mlb, device=devices, requires_grad=self.requires_grad)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.shard_","title":"shard_","text":"<pre><code>shard_(\n    devices: tuple[str, ...], axis: int | None = None\n) -&gt; Tensor\n</code></pre> <p>Shards the tensor across the given devices in place.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def shard_(self, devices:tuple[str, ...], axis:int|None=None) -&gt; Tensor:\n  \"\"\"\n  Shards the tensor across the given devices in place.\n  \"\"\"\n  return self.replace(self.shard(devices, axis))\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.contiguous","title":"contiguous","text":"<pre><code>contiguous(*args, **kwargs) -&gt; Tensor\n</code></pre> <p>Returns a contiguous tensor.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def contiguous(self, *args, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Returns a contiguous tensor.\n  \"\"\"\n  return self._apply_uop(UOp.contiguous, extra_args=args, **kwargs)\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.contiguous_backward","title":"contiguous_backward","text":"<pre><code>contiguous_backward() -&gt; Tensor\n</code></pre> <p>Inserts a contiguous operation in the backward pass.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def contiguous_backward(self) -&gt; Tensor:\n  \"\"\"\n  Inserts a contiguous operation in the backward pass.\n  \"\"\"\n  return self._apply_uop(UOp.contiguous_backward)\n</code></pre>"},{"location":"tensor/properties/#gradient","title":"Gradient","text":""},{"location":"tensor/properties/#tinygrad.Tensor.gradient","title":"gradient","text":"<pre><code>gradient(\n    *targets: Tensor,\n    gradient: Tensor | None = None,\n    materialize_grads=False\n) -&gt; list[Tensor]\n</code></pre> <p>Computes the gradient of the targets with respect to self.</p> <pre><code>x = Tensor.eye(3)\ny = Tensor([[2.0,0,-2.0]])\nz = y.matmul(x).sum()\ndx, dy = z.gradient(x, y)\n\nprint(dx.tolist())  # dz/dx\nprint(dy.tolist())  # dz/dy\n</code></pre> <pre><code>[[2.0, 2.0, 2.0], [0.0, 0.0, 0.0], [-2.0, -2.0, -2.0]]\n[[1.0, 1.0, 1.0]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def gradient(self, *targets:Tensor, gradient:Tensor|None=None, materialize_grads=False) -&gt; list[Tensor]:\n  \"\"\"\n  Computes the gradient of the targets with respect to self.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  x = Tensor.eye(3)\n  y = Tensor([[2.0,0,-2.0]])\n  z = y.matmul(x).sum()\n  dx, dy = z.gradient(x, y)\n\n  print(dx.tolist())  # dz/dx\n  print(dy.tolist())  # dz/dy\n  ```\n  \"\"\"\n  assert gradient is not None or self.shape == tuple(), \"when no gradient is provided, backward must be called on a scalar tensor\"\n  if not (self.is_floating_point() and all(t.is_floating_point() for t in targets)): raise RuntimeError(\"only float Tensors have gradient\")\n  if gradient is None: gradient = Tensor(1.0, dtype=self.dtype, device=self.device, requires_grad=False)\n  target_uops = [x.uop for x in targets]\n  grads = compute_gradient(self.uop, gradient.uop, set(target_uops))\n  ret = []\n  for x in target_uops:\n    if (y:=grads.get(x)) is None:\n      if materialize_grads: y = x.const_like(0)\n      else: raise RuntimeError(f\"{x}\\n\\nnot found in\\n\\n{self.uop}\")\n    ret.append(y)\n  # create returned Tensors\n  return [Tensor(u, device=t.device) for t,u in zip(targets, ret)]\n</code></pre>"},{"location":"tensor/properties/#tinygrad.Tensor.backward","title":"backward","text":"<pre><code>backward(gradient: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Propagates the gradient of a tensor backwards through the computation graph. If the 'gradient' argument is not provided, the tensor must be a scalar, and the gradient is implicitly set to 1.0. <pre><code>t = Tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\nt.sum().backward()\nprint(t.grad.numpy())\n</code></pre> <pre><code>[1. 1. 1. 1.]\n</code></pre></p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def backward(self, gradient:Tensor|None=None) -&gt; Tensor:\n  \"\"\"\n  Propagates the gradient of a tensor backwards through the computation graph.\n  If the 'gradient' argument is not provided, the tensor must be a scalar, and the gradient is implicitly set to 1.0.\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n  t.sum().backward()\n  print(t.grad.numpy())\n  ```\n  \"\"\"\n  all_uops = self.uop.toposort()\n  tensors_need_grad: list[Tensor] = [t for tref in all_tensors if (t:=tref()) is not None and \\\n                                     t.uop in all_uops and t.requires_grad]\n  # clear contexts\n  for t,g in zip(tensors_need_grad, self.gradient(*tensors_need_grad, gradient=gradient, materialize_grads=True)):\n    assert g.shape == t.shape, f\"grad shape must match tensor shape, {g.shape!r} != {t.shape!r}\"\n    t.grad = g if t.grad is None else (t.grad + g)\n  return self\n</code></pre>"}]}