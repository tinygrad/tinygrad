#!/usr/bin/env python3
"""Tests for the RDNA assembly renderer"""
import unittest
import numpy as np
from tinygrad import Tensor, dtypes
from tinygrad.helpers import getenv

# Skip tests if not on AMD RDNA device
AMD_RDNA = getenv("AMD_RDNA", 0)

@unittest.skipUnless(AMD_RDNA, "AMD_RDNA=1 required")
class TestRDNABasic(unittest.TestCase):
  """Basic functionality tests"""

  def test_basic_half_load(self):
    """Test basic half-precision load and sum"""
    a = Tensor(np.arange(16, dtype=np.float16).reshape(1, 16))
    b = a.sum()
    b.realize()
    np.testing.assert_allclose(b.numpy(), np.arange(16).sum(), rtol=1e-2)

  def test_basic_float_load(self):
    """Test basic float load and sum"""
    a = Tensor(np.arange(16, dtype=np.float32).reshape(1, 16))
    b = a.sum()
    b.realize()
    np.testing.assert_allclose(b.numpy(), np.arange(16).sum(), rtol=1e-5)

  def test_elementwise_add(self):
    """Test elementwise addition"""
    a = Tensor([1.0, 2.0, 3.0, 4.0])
    b = Tensor([5.0, 6.0, 7.0, 8.0])
    c = a + b
    c.realize()
    np.testing.assert_allclose(c.numpy(), [6.0, 8.0, 10.0, 12.0])

  def test_elementwise_mul(self):
    """Test elementwise multiplication"""
    a = Tensor([1.0, 2.0, 3.0, 4.0])
    b = Tensor([2.0, 2.0, 2.0, 2.0])
    c = a * b
    c.realize()
    np.testing.assert_allclose(c.numpy(), [2.0, 4.0, 6.0, 8.0])

@unittest.skipUnless(AMD_RDNA, "AMD_RDNA=1 required")
class TestRDNAGatedLoads(unittest.TestCase):
  """Tests for gated loads (WHERE+LOAD patterns)"""

  def test_relu_backward_gated_load(self):
    """Test that relu backward correctly gates memory access"""
    Tensor.training = True
    x = Tensor(np.array([[-1, 2], [3, -4]], dtype=np.float32), device="AMD", requires_grad=True)
    y = x.relu()
    loss = y.sum()
    loss.backward()
    # Gradient should be 1 where x > 0, 0 elsewhere
    expected = np.array([[0, 1], [1, 0]], dtype=np.float32)
    np.testing.assert_allclose(x.grad.numpy(), expected)

  def test_max_pool2d_backward_gated_load(self):
    """Test that max_pool2d backward correctly gates memory access"""
    Tensor.training = True
    x = Tensor.rand(2, 4, 8, 8, device="AMD", requires_grad=True)
    y = x.max_pool2d(2)
    loss = y.sum()
    loss.backward()
    self.assertIsNotNone(x.grad)
    # Just verify it runs without GPU fault

  def test_sparse_categorical_crossentropy(self):
    """Test sparse_categorical_crossentropy with index-based gating"""
    Tensor.training = True
    logits = Tensor.rand(4, 10, device="AMD", requires_grad=True)
    targets = Tensor([0, 3, 5, 9], device="AMD")
    loss = logits.sparse_categorical_crossentropy(targets)
    loss.backward()
    self.assertIsNotNone(logits.grad)
    # Gradients for each row should sum to 0 (since exp/sum normalizes)
    grad_sum = logits.grad.numpy().sum(axis=1)
    np.testing.assert_allclose(grad_sum, np.zeros(4), atol=1e-5)

@unittest.skipUnless(AMD_RDNA, "AMD_RDNA=1 required")
class TestRDNA64BitOperations(unittest.TestCase):
  """Tests for 64-bit integer operations (used in division-by-multiplication pattern)"""

  @unittest.skip("Division by constant requires fix for quotient register preservation in RDNA renderer")
  def test_integer_division_optimization(self):
    """Test that integer division uses 64-bit MUL+SHR pattern correctly"""
    # Integer division by constants uses a mul+shift pattern that requires 64-bit ops
    a = Tensor([10, 20, 30, 40], dtype=dtypes.int32, device="AMD")
    b = a // 7
    expected = np.array([1, 2, 4, 5], dtype=np.int32)
    np.testing.assert_array_equal(b.numpy(), expected)

  @unittest.skip("Modulo by constant requires fix for quotient register preservation in RDNA renderer")
  def test_modulo_operation(self):
    """Test that modulo uses 64-bit intermediate operations correctly"""
    a = Tensor([10, 20, 30, 40], dtype=dtypes.int32, device="AMD")
    b = a % 7
    expected = np.array([3, 6, 2, 5], dtype=np.int32)
    np.testing.assert_array_equal(b.numpy(), expected)

  def test_integer_division_by_tensor(self):
    """Test that integer division by tensor works correctly (uses float conversion)"""
    a = Tensor([10, 20, 30, 40], dtype=dtypes.int32, device="AMD")
    b = Tensor([7, 7, 7, 7], dtype=dtypes.int32, device="AMD")
    c = a // b
    expected = np.array([1, 2, 4, 5], dtype=np.int32)
    np.testing.assert_array_equal(c.numpy(), expected)

@unittest.skipUnless(AMD_RDNA, "AMD_RDNA=1 required")
class TestRDNATraining(unittest.TestCase):
  """Tests for training/backward pass functionality"""

  def test_simple_mlp_training(self):
    """Test a simple 2-layer MLP training step"""
    Tensor.training = True
    np.random.seed(42)

    w1 = Tensor.scaled_uniform(16, 8, requires_grad=True)
    w2 = Tensor.scaled_uniform(8, 4, requires_grad=True)

    from tinygrad.nn import optim
    optimizer = optim.SGD([w1, w2], lr=0.01)

    x = Tensor.rand(4, 16, device="AMD")
    h = (x @ w1).relu()
    y = h @ w2
    loss = y.sum()

    optimizer.zero_grad()
    loss.backward()

    self.assertIsNotNone(w1.grad)
    self.assertIsNotNone(w2.grad)

    optimizer.step()
    # If we get here without GPU fault, the test passes

  def test_conv_backward(self):
    """Test conv2d backward pass"""
    Tensor.training = True
    x = Tensor.rand(2, 1, 8, 8, device="AMD", requires_grad=True)
    w = Tensor.rand(4, 1, 3, 3, device="AMD", requires_grad=True)
    y = x.conv2d(w)
    loss = y.sum()
    loss.backward()
    self.assertIsNotNone(x.grad)
    self.assertIsNotNone(w.grad)

  def test_matmul_backward(self):
    """Test matmul backward pass"""
    Tensor.training = True
    a = Tensor.rand(4, 8, device="AMD", requires_grad=True)
    b = Tensor.rand(8, 3, device="AMD", requires_grad=True)
    c = a @ b
    loss = c.sum()
    loss.backward()
    self.assertIsNotNone(a.grad)
    self.assertIsNotNone(b.grad)

@unittest.skipUnless(AMD_RDNA, "AMD_RDNA=1 required")
class TestRDNAWMMA(unittest.TestCase):
  """WMMA tensor core tests"""

  def test_wmma_16x16_correctness(self):
    """Test 16x16 WMMA correctness"""
    # Use uniform random values in [-0.5, 0.5) to avoid float16 overflow
    rng = np.random.default_rng(42)
    a = Tensor((rng.random((16, 16), dtype=np.float32) - 0.5).astype(np.float16)).realize()
    b = Tensor((rng.random((16, 16), dtype=np.float32) - 0.5).astype(np.float16)).realize()
    c = a.matmul(b, dtype=dtypes.float)
    c_np = a.numpy().astype(np.float32) @ b.numpy().astype(np.float32)
    np.testing.assert_allclose(c.numpy(), c_np, rtol=1e-2, atol=1e-2)

  def test_wmma_32x32_correctness(self):
    """Test 32x32 WMMA correctness"""
    # Use uniform random values in [-0.5, 0.5) to avoid float16 overflow
    rng = np.random.default_rng(42)
    a = Tensor((rng.random((32, 32), dtype=np.float32) - 0.5).astype(np.float16)).realize()
    b = Tensor((rng.random((32, 32), dtype=np.float32) - 0.5).astype(np.float16)).realize()
    c = a.matmul(b, dtype=dtypes.float)
    c_np = a.numpy().astype(np.float32) @ b.numpy().astype(np.float32)
    np.testing.assert_allclose(c.numpy(), c_np, rtol=1e-2, atol=1e-2)

@unittest.skipUnless(AMD_RDNA, "AMD_RDNA=1 required")
class TestRDNAKernelGeneration(unittest.TestCase):
  """Tests for kernel code generation"""

  def test_wmma_kernel_runs(self):
    """Test that WMMA kernel runs without error"""
    # If tensor cores are used, this matmul should run successfully
    a = Tensor(np.random.randn(16, 16).astype(np.float16))
    b = Tensor(np.random.randn(16, 16).astype(np.float16))
    c = a.matmul(b, dtype=dtypes.float)
    c.realize()
    # Check result is reasonable
    self.assertEqual(c.shape, (16, 16))
    self.assertFalse(np.any(np.isnan(c.numpy())))

  def test_larger_wmma_kernel_runs(self):
    """Test that larger WMMA kernel (multiple tiles) runs"""
    a = Tensor(np.random.randn(32, 32).astype(np.float16))
    b = Tensor(np.random.randn(32, 32).astype(np.float16))
    c = a.matmul(b, dtype=dtypes.float)
    c.realize()
    self.assertEqual(c.shape, (32, 32))
    self.assertFalse(np.any(np.isnan(c.numpy())))

@unittest.skipUnless(AMD_RDNA, "AMD_RDNA=1 required")
class TestRDNAVGPRLimits(unittest.TestCase):
  """Tests documenting VGPR limits"""

  @unittest.skip("64x64 WMMA exceeds VGPR limit (261 vs 256) - requires smaller tile size or improved register allocation")
  def test_wmma_64x64_vgpr_limit(self):
    """64x64 WMMA - tests if VGPR limit is respected
    Currently skipped: 64x64 tiles require 261 VGPRs but RDNA3 limit is 256.
    Use N=16 for working WMMA tests.
    """
    a = Tensor(np.random.randn(64, 64).astype(np.float16))
    b = Tensor(np.random.randn(64, 64).astype(np.float16))
    c = a.matmul(b, dtype=dtypes.float)
    c_np = a.numpy().astype(np.float32) @ b.numpy().astype(np.float32)
    c.realize()
    np.testing.assert_allclose(c.numpy(), c_np, rtol=1e-2, atol=1e-2)

class TestWMMAVGPRUsage(unittest.TestCase):
  """Tests for WMMA VGPR usage analysis - run without device"""

  @unittest.skip("64x64 WMMA exceeds VGPR limit (261 vs 256) - known limitation")
  def test_64x64_wmma_vgpr_count(self):
    """Test that 64x64 WMMA fits within 256 VGPR limit.

    KNOWN LIMITATION: 64x64 WMMA tiles require 261 VGPRs but RDNA3 limit is 256.

    Root cause analysis of VGPR usage:
    - 128 VGPRs for accumulators (16 output tiles x 8 floats each)
    - ~60 VGPRs for byte offset computation (SHL ops)
    - ~40 VGPRs for WMMA inputs (A and B matrices packed as half16)
    - Plus temps for index computation

    Current optimizations applied:
    - CAST reuse: float32â†’half conversions reuse accumulator VGPRs
    - Deferred INDEX allocation: store addresses computed just-in-time
    - REG INDEX/LOAD: accumulator arrays don't allocate extra VGPRs

    Future fix needed: defer SHL ops for byte offset computation to store time.
    """
    import re
    from tinygrad.codegen import full_rewrite
    from tinygrad.uop.ops import Ops
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.helpers import Context

    # Need TC=1 to enable tensor cores, TC_OPT=2 for padding support
    with Context(TC=1, TC_OPT=2):
      a = Tensor(np.random.randn(64, 64).astype(np.float16))
      b = Tensor(np.random.randn(64, 64).astype(np.float16))
      c = a.matmul(b, dtype=dtypes.float)

      sched = c.schedule()

      # Find the matmul kernel
      for item in sched:
        if hasattr(item, 'ast') and item.ast is not None:
          has_reduce = any(u.op == Ops.REDUCE for u in item.ast.toposort())
          if has_reduce:
            # Use RDNARenderer for both full_rewrite and render
            # (rdna_matcher needs to be applied during full_rewrite to handle pointer CASTs)
            renderer = RDNARenderer('gfx1100')
            uops = full_rewrite(item.ast, renderer)

            # Verify we have WMMA ops
            has_wmma = any(u.op == Ops.WMMA for u in uops)
            self.assertTrue(has_wmma, "Should have WMMA ops with USE_TC=1")

            # Count WMMA and stores
            wmma_count = sum(1 for u in uops if u.op == Ops.WMMA)
            self.assertEqual(wmma_count, 16, f"Should have 16 WMMA ops for 64x64, got {wmma_count}")

            # Render and check VGPR count
            asm = renderer.render(uops)

            match = re.search(r'\.amdhsa_next_free_vgpr (\d+)', asm)
            self.assertIsNotNone(match, "Should have .amdhsa_next_free_vgpr in metadata")
            vgpr_count = int(match.group(1))

            # The VGPR limit is 256. This test documents the current state
            # and will pass once the register allocator is fixed.
            self.assertLessEqual(vgpr_count, 256,
              f"64x64 WMMA needs {vgpr_count} VGPRs but limit is 256. "
              f"Main issue: 128 store addresses computed upfront. "
              f"Fix: compute addresses just-in-time or reuse address VGPRs.")
            return

      self.fail("Should find a kernel with REDUCE op")

  def test_32x32_wmma_fits_in_vgpr_limit(self):
    """Verify 32x32 WMMA fits within VGPR limit"""
    import re
    from tinygrad.codegen import full_rewrite
    from tinygrad.uop.ops import Ops
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.helpers import Context

    with Context(TC=1, TC_OPT=2):
      a = Tensor(np.random.randn(32, 32).astype(np.float16))
      b = Tensor(np.random.randn(32, 32).astype(np.float16))
      c = a.matmul(b, dtype=dtypes.float)

      sched = c.schedule()

      for item in sched:
        if hasattr(item, 'ast') and item.ast is not None:
          has_reduce = any(u.op == Ops.REDUCE for u in item.ast.toposort())
          if has_reduce:
            # Use RDNARenderer for both full_rewrite and render
            renderer = RDNARenderer('gfx1100')
            uops = full_rewrite(item.ast, renderer)

            has_wmma = any(u.op == Ops.WMMA for u in uops)
            self.assertTrue(has_wmma, "Should have WMMA ops")

            asm = renderer.render(uops)

            match = re.search(r'\.amdhsa_next_free_vgpr (\d+)', asm)
            self.assertIsNotNone(match)
            vgpr_count = int(match.group(1))

            # 32x32 should use fewer VGPRs (4 tiles instead of 16)
            # With perfect allocation: 4*8=32 accumulators + ~32 addresses + ~40 inputs = ~104
            self.assertLessEqual(vgpr_count, 256,
              f"32x32 WMMA needs {vgpr_count} VGPRs, should fit in 256")
            return

      self.fail("Should find a kernel with REDUCE op")

class TestLookAheadPacking(unittest.TestCase):
  """Tests for the look-ahead packing optimization - these run without device"""

  def test_half16_const_packing(self):
    """Test that half16 VECTORIZE with constants generates pack instructions"""
    from tinygrad.uop.ops import Ops, UOp
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.dtype import dtypes

    renderer = RDNARenderer("gfx1100")

    # Build a simple test: 16 scalar half constants -> half16 VECTORIZE
    # This tests the basic VECTORIZE packing logic (not look-ahead, but verifies packing works)
    half_vals = [UOp.const(dtypes.half, float(i)) for i in range(16)]
    vec = UOp(Ops.VECTORIZE, dtypes.half.vec(16), tuple(half_vals))
    sink = UOp(Ops.SINK, dtypes.void, (vec,))

    # Render and check for v_pack_b32_f16 instructions
    uops = list(sink.toposort())
    asm = renderer.render(uops)

    # Should have v_pack_b32_f16 instructions for packing pairs of halfs
    pack_count = asm.count('v_pack_b32_f16')
    self.assertGreater(pack_count, 0, "Should have v_pack_b32_f16 instructions for half16 packing")
    # Should have 8 pack instructions (16 halfs / 2 per pack)
    self.assertEqual(pack_count, 8, f"Should have exactly 8 pack instructions, got {pack_count}")

  def test_half16_load_packing_with_index(self):
    """Test that half16 VECTORIZE with LOADs generates pack instructions and uses look-ahead"""
    from tinygrad.uop.ops import Ops, UOp
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.dtype import dtypes

    renderer = RDNARenderer("gfx1100")

    # Create a minimal kernel with half LOADs feeding half16 VECTORIZE
    # This simulates what WMMA needs for input data

    # Create buffer argument using .ptr() method
    buf = UOp(Ops.DEFINE_GLOBAL, dtypes.half.ptr(), arg=0)

    # Create index (workitem ID)
    ridx = UOp.special(32, "ridx0", dtype=dtypes.int)

    # Create 16 LOADs at different offsets
    loads = []
    for i in range(16):
      offset = UOp.const(dtypes.int, i)
      idx = UOp(Ops.ADD, dtypes.int, (ridx, offset))
      index = UOp(Ops.INDEX, buf.dtype, (buf, idx))
      load = UOp(Ops.LOAD, dtypes.half, (index,))
      loads.append(load)

    # Create half16 VECTORIZE
    vec = UOp(Ops.VECTORIZE, dtypes.half.vec(16), tuple(loads))
    sink = UOp(Ops.SINK, dtypes.void, (vec,))

    uops = list(sink.toposort())
    asm = renderer.render(uops)

    # Should have v_pack_b32_f16 instructions
    pack_count = asm.count('v_pack_b32_f16')
    self.assertGreater(pack_count, 0, "Should have v_pack_b32_f16 instructions")

    # Should have 8 pack instructions total (16 halfs / 2 per pack)
    self.assertEqual(pack_count, 8, f"Should have exactly 8 pack instructions, got {pack_count}")

    # Check that LOADs are generated (global_load_u16 for half precision)
    load_count = asm.count('global_load_u16')
    self.assertGreater(load_count, 0, "Should have global_load_u16 instructions")
    # Should have 16 loads (one per half element)
    self.assertEqual(load_count, 16, f"Should have 16 load instructions, got {load_count}")

  def test_look_ahead_packing_pre_scan(self):
    """Test that the pre-scan correctly identifies half16 VECTORIZE sources"""
    from tinygrad.uop.ops import Ops, UOp
    from tinygrad.dtype import dtypes

    # Create a minimal kernel with half LOADs feeding half16 VECTORIZE
    buf = UOp(Ops.DEFINE_GLOBAL, dtypes.half.ptr(), arg=0)
    ridx = UOp.special(32, "ridx0", dtype=dtypes.int)

    # Create 16 LOADs
    loads = []
    for i in range(16):
      offset = UOp.const(dtypes.int, i)
      idx = UOp(Ops.ADD, dtypes.int, (ridx, offset))
      index = UOp(Ops.INDEX, buf.dtype, (buf, idx))
      load = UOp(Ops.LOAD, dtypes.half, (index,))
      loads.append(load)

    vec = UOp(Ops.VECTORIZE, dtypes.half.vec(16), tuple(loads))
    sink = UOp(Ops.SINK, dtypes.void, (vec,))

    uops = list(sink.toposort())

    # Simulate the pre-scan logic from the renderer
    half16_vectorize_sources = {}
    for u in uops:
      if u.op is Ops.VECTORIZE and u.dtype.scalar() == dtypes.half and u.dtype.count == 16:
        for pos, src in enumerate(u.src):
          half16_vectorize_sources[src] = (u, pos)

    # All 16 LOADs should be identified as half16 sources
    load_count = sum(1 for u in uops if u.op is Ops.LOAD and u in half16_vectorize_sources)
    self.assertEqual(load_count, 16, f"All 16 LOADs should be identified as half16 sources, got {load_count}")

  def test_look_ahead_packing_is_interleaved(self):
    """Test that pack instructions are interleaved with loads (not all at the end)"""
    from tinygrad.uop.ops import Ops, UOp
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.dtype import dtypes

    renderer = RDNARenderer("gfx1100")

    # Create a minimal kernel with half LOADs feeding half16 VECTORIZE
    buf = UOp(Ops.DEFINE_GLOBAL, dtypes.half.ptr(), arg=0)
    ridx = UOp.special(32, "ridx0", dtype=dtypes.int)

    # Create 16 LOADs
    loads = []
    for i in range(16):
      offset = UOp.const(dtypes.int, i)
      idx = UOp(Ops.ADD, dtypes.int, (ridx, offset))
      index = UOp(Ops.INDEX, buf.dtype, (buf, idx))
      load = UOp(Ops.LOAD, dtypes.half, (index,))
      loads.append(load)

    vec = UOp(Ops.VECTORIZE, dtypes.half.vec(16), tuple(loads))
    sink = UOp(Ops.SINK, dtypes.void, (vec,))

    uops = list(sink.toposort())
    asm = renderer.render(uops)
    lines = asm.split('\n')

    # Find line numbers of loads and packs
    load_lines = [i for i, l in enumerate(lines) if 'global_load_u16' in l]
    pack_lines = [i for i, l in enumerate(lines) if 'v_pack_b32_f16' in l]

    self.assertGreater(len(load_lines), 0, "Should have load instructions")
    self.assertGreater(len(pack_lines), 0, "Should have pack instructions")

    # The first pack should occur BEFORE the last load (interleaving)
    first_pack = min(pack_lines)
    last_load = max(load_lines)
    self.assertLess(first_pack, last_load,
      f"First pack (line {first_pack}) should be before last load (line {last_load}) for interleaving")

  def test_vgpr_reuse_in_look_ahead_packing(self):
    """Test that temp VGPRs are reused after packing (reducing register pressure)"""
    from tinygrad.uop.ops import Ops, UOp
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.dtype import dtypes
    import re

    renderer = RDNARenderer("gfx1100")

    # Create a minimal kernel with half LOADs feeding half16 VECTORIZE
    buf = UOp(Ops.DEFINE_GLOBAL, dtypes.half.ptr(), arg=0)
    ridx = UOp.special(32, "ridx0", dtype=dtypes.int)

    # Create 16 LOADs
    loads = []
    for i in range(16):
      offset = UOp.const(dtypes.int, i)
      idx = UOp(Ops.ADD, dtypes.int, (ridx, offset))
      index = UOp(Ops.INDEX, buf.dtype, (buf, idx))
      load = UOp(Ops.LOAD, dtypes.half, (index,))
      loads.append(load)

    vec = UOp(Ops.VECTORIZE, dtypes.half.vec(16), tuple(loads))
    sink = UOp(Ops.SINK, dtypes.void, (vec,))

    uops = list(sink.toposort())
    asm = renderer.render(uops)

    # Extract VGPR count from metadata
    vgpr_match = re.search(r'\.amdhsa_next_free_vgpr (\d+)', asm)
    self.assertIsNotNone(vgpr_match, "Should have .amdhsa_next_free_vgpr in metadata")
    vgpr_count = int(vgpr_match.group(1))

    # With look-ahead packing and VGPR reuse:
    # - 8 VGPRs for the half16 destination range
    # - A few temp VGPRs for loading (reused)
    # - A few VGPRs for address computation
    # - 32 scratch VGPRs are allocated for potential 64-bit ops
    # Without reuse, we'd need 8 + 16 + 32 = 56 VGPRs minimum
    # With reuse, we should need fewer than that
    self.assertLess(vgpr_count, 56, f"VGPR count should be < 56 with reuse, got {vgpr_count}")

  def test_multiple_half16_vectorizes(self):
    """Test look-ahead packing with multiple half16 VECTORIZEs (like WMMA A and B inputs)"""
    from tinygrad.uop.ops import Ops, UOp
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.dtype import dtypes
    import re

    renderer = RDNARenderer("gfx1100")

    # Create two buffers (like A and B matrices)
    buf_a = UOp(Ops.DEFINE_GLOBAL, dtypes.half.ptr(), arg=0)
    buf_b = UOp(Ops.DEFINE_GLOBAL, dtypes.half.ptr(), arg=1)
    ridx = UOp.special(32, "ridx0", dtype=dtypes.int)

    # Create 16 LOADs from buf_a
    loads_a = []
    for i in range(16):
      offset = UOp.const(dtypes.int, i)
      idx = UOp(Ops.ADD, dtypes.int, (ridx, offset))
      index = UOp(Ops.INDEX, buf_a.dtype, (buf_a, idx))
      load = UOp(Ops.LOAD, dtypes.half, (index,))
      loads_a.append(load)

    # Create 16 LOADs from buf_b
    loads_b = []
    for i in range(16):
      offset = UOp.const(dtypes.int, i + 100)  # Different offsets
      idx = UOp(Ops.ADD, dtypes.int, (ridx, offset))
      index = UOp(Ops.INDEX, buf_b.dtype, (buf_b, idx))
      load = UOp(Ops.LOAD, dtypes.half, (index,))
      loads_b.append(load)

    # Create two half16 VECTORIZEs
    vec_a = UOp(Ops.VECTORIZE, dtypes.half.vec(16), tuple(loads_a))
    vec_b = UOp(Ops.VECTORIZE, dtypes.half.vec(16), tuple(loads_b))
    sink = UOp(Ops.SINK, dtypes.void, (vec_a, vec_b))

    uops = list(sink.toposort())
    asm = renderer.render(uops)

    # Should have 16 pack instructions (8 for each half16)
    pack_count = asm.count('v_pack_b32_f16')
    self.assertEqual(pack_count, 16, f"Should have 16 pack instructions, got {pack_count}")

    # Should have 32 loads total
    load_count = asm.count('global_load_u16')
    self.assertEqual(load_count, 32, f"Should have 32 load instructions, got {load_count}")

    # Extract VGPR count
    vgpr_match = re.search(r'\.amdhsa_next_free_vgpr (\d+)', asm)
    self.assertIsNotNone(vgpr_match, "Should have .amdhsa_next_free_vgpr in metadata")
    vgpr_count = int(vgpr_match.group(1))

    # With look-ahead packing and VGPR reuse:
    # - 16 VGPRs for the two half16 destination ranges (8 each)
    # - A few temp VGPRs for loading (reused)
    # - 32 scratch VGPRs are allocated for potential 64-bit ops
    # Without reuse, we'd need 16 + 32 + 32 = 80 VGPRs minimum
    # With reuse, we should need fewer than that
    self.assertLess(vgpr_count, 80, f"VGPR count should be < 80 with reuse, got {vgpr_count}")

@unittest.skipUnless(AMD_RDNA, "AMD_RDNA=1 required")
class TestLookAheadPackingOnDevice(unittest.TestCase):
  """Tests for look-ahead packing that require actual device execution"""

  @unittest.skip("WMMA generation depends on scheduler decisions for matrix size")
  def test_wmma_generates_pack_instructions(self):
    """Test that WMMA kernel generates v_pack_b32_f16 instructions"""
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.uop.ops import Ops

    # Create matmul that uses WMMA
    a = Tensor(np.random.randn(16, 16).astype(np.float16))
    b = Tensor(np.random.randn(16, 16).astype(np.float16))
    c = a.matmul(b, dtype=dtypes.float)

    sched = c.schedule()
    renderer = RDNARenderer("gfx1100")

    # Find the WMMA kernel
    for item in sched:
      if hasattr(item, 'ast') and item.ast is not None:
        uops = list(item.ast.toposort())
        has_wmma = any(u.op == Ops.WMMA for u in uops)
        if has_wmma:
          asm = renderer.render(uops)
          pack_count = asm.count('v_pack_b32_f16')
          self.assertGreater(pack_count, 0, "WMMA kernel should have v_pack_b32_f16 instructions")
          return

    self.fail("Should find WMMA kernel in schedule")

  @unittest.skip("Test needs adjustment for full_rewrite spec requirements")
  def test_look_ahead_packing_reduces_temp_regs(self):
    """Test that look-ahead packing packs halfs immediately after load"""
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.codegen import full_rewrite
    from tinygrad.uop.ops import Ops
    from tinygrad.helpers import Context

    # Create matmul that uses WMMA - need TC=1 and TC_OPT=2
    with Context(TC=1, TC_OPT=2):
      a = Tensor(np.random.randn(16, 16).astype(np.float16))
      b = Tensor(np.random.randn(16, 16).astype(np.float16))
      c = a.matmul(b, dtype=dtypes.float)

      sched = c.schedule()
      renderer = RDNARenderer("gfx1100")

      # Find and render the WMMA kernel
      for item in sched:
        if hasattr(item, 'ast') and item.ast is not None:
          # Use full_rewrite to get lowered uops with WMMA
          uops = full_rewrite(item.ast, renderer)
          has_wmma = any(u.op == Ops.WMMA for u in uops)
          if has_wmma:
            asm = renderer.render(uops)
            lines = asm.split('\n')

            # Check that pack instructions appear after loads (look-ahead packing)
            # The pattern should be: global_load_d16_b16 ... then v_pack_b32_f16
            load_lines = [i for i, l in enumerate(lines) if 'global_load_d16_b16' in l]
            pack_lines = [i for i, l in enumerate(lines) if 'v_pack_b32_f16' in l]

            # Some pack instructions should appear interleaved with loads
            # (i.e., packing happens as soon as pairs are loaded, not all at the end)
            if load_lines and pack_lines:
              # Find first pack after a load
              first_pack = min(pack_lines)
              last_load = max(load_lines)
              # Pack should happen before all loads are done (interleaved)
              self.assertLess(first_pack, last_load,
                "Look-ahead packing should interleave pack instructions with loads")
            return

      self.fail("Should find WMMA kernel in schedule")

@unittest.skipUnless(AMD_RDNA, "AMD_RDNA=1 required")
class TestVGPRRegressions(unittest.TestCase):
  """Regression tests for VGPR allocation optimizations"""

  def test_16x16_wmma_on_device(self):
    """Verify 16x16 WMMA matmul works correctly on device - regression test"""
    rng = np.random.default_rng(42)
    a = Tensor((rng.random((16, 16), dtype=np.float32) - 0.5).astype(np.float16)).realize()
    b = Tensor((rng.random((16, 16), dtype=np.float32) - 0.5).astype(np.float16)).realize()
    c = a.matmul(b, dtype=dtypes.float)

    c_np = a.numpy().astype(np.float32) @ b.numpy().astype(np.float32)
    c.realize()
    np.testing.assert_allclose(c.numpy(), c_np, rtol=1e-2, atol=1e-2)

  def test_32x32_wmma_on_device(self):
    """Verify 32x32 WMMA matmul works correctly on device - regression test"""
    rng = np.random.default_rng(42)
    a = Tensor((rng.random((32, 32), dtype=np.float32) - 0.5).astype(np.float16)).realize()
    b = Tensor((rng.random((32, 32), dtype=np.float32) - 0.5).astype(np.float16)).realize()
    c = a.matmul(b, dtype=dtypes.float)

    c_np = a.numpy().astype(np.float32) @ b.numpy().astype(np.float32)
    c.realize()
    np.testing.assert_allclose(c.numpy(), c_np, rtol=1e-2, atol=1e-2)

  def test_deferred_store_index_detection(self):
    """Verify that store-only INDEX ops are detected for deferred allocation"""
    import re
    from tinygrad.codegen import full_rewrite
    from tinygrad.uop.ops import Ops
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.helpers import Context

    with Context(TC=1, TC_OPT=2):
      a = Tensor(np.random.randn(16, 16).astype(np.float16))
      b = Tensor(np.random.randn(16, 16).astype(np.float16))
      c = a.matmul(b, dtype=dtypes.float)

      sched = c.schedule()
      renderer = RDNARenderer('gfx1100')

      for item in sched:
        if hasattr(item, 'ast') and item.ast is not None:
          has_reduce = any(u.op == Ops.REDUCE for u in item.ast.toposort())
          if has_reduce:
            uops = full_rewrite(item.ast, renderer)

            # Count STORE operations - there should be some
            store_count = sum(1 for u in uops if u.op == Ops.STORE)
            self.assertGreater(store_count, 0, "Should have STORE operations")

            # Render and verify no register overflow
            asm = renderer.render(uops)
            match = re.search(r'\.amdhsa_next_free_vgpr (\d+)', asm)
            self.assertIsNotNone(match)
            vgpr_count = int(match.group(1))
            self.assertLessEqual(vgpr_count, 256, f"Should fit in 256 VGPRs, got {vgpr_count}")
            return

      self.fail("Should find kernel with REDUCE op")

  def test_16x16_wmma_vgpr_count(self):
    """Verify 16x16 WMMA uses reasonable VGPR count"""
    import re
    from tinygrad.codegen import full_rewrite
    from tinygrad.uop.ops import Ops
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.helpers import Context

    with Context(TC=1, TC_OPT=2):
      a = Tensor(np.random.randn(16, 16).astype(np.float16))
      b = Tensor(np.random.randn(16, 16).astype(np.float16))
      c = a.matmul(b, dtype=dtypes.float)

      sched = c.schedule()
      renderer = RDNARenderer('gfx1100')

      for item in sched:
        if hasattr(item, 'ast') and item.ast is not None:
          has_reduce = any(u.op == Ops.REDUCE for u in item.ast.toposort())
          if has_reduce:
            uops = full_rewrite(item.ast, renderer)
            has_wmma = any(u.op == Ops.WMMA for u in uops)
            if not has_wmma:
              continue

            asm = renderer.render(uops)
            match = re.search(r'\.amdhsa_next_free_vgpr (\d+)', asm)
            self.assertIsNotNone(match)
            vgpr_count = int(match.group(1))

            # 16x16 WMMA should use far fewer VGPRs than the limit
            # 1 WMMA tile = 8 accumulators, plus inputs/temps
            self.assertLess(vgpr_count, 128,
              f"16x16 WMMA should use <128 VGPRs, got {vgpr_count}")
            return

      # If no WMMA kernel found, that's fine - test passes
      pass

  def test_cast_reuse_optimization(self):
    """Verify that CAST operations reuse accumulator VGPRs"""
    import re
    from tinygrad.codegen import full_rewrite
    from tinygrad.uop.ops import Ops
    from tinygrad.renderer.rdna_new import RDNARenderer
    from tinygrad.helpers import Context

    with Context(TC=1, TC_OPT=2):
      a = Tensor(np.random.randn(32, 32).astype(np.float16))
      b = Tensor(np.random.randn(32, 32).astype(np.float16))
      # Output as half to trigger float32->half CAST
      c = a.matmul(b, dtype=dtypes.float).cast(dtypes.half)

      sched = c.schedule()
      renderer = RDNARenderer('gfx1100')

      for item in sched:
        if hasattr(item, 'ast') and item.ast is not None:
          has_reduce = any(u.op == Ops.REDUCE for u in item.ast.toposort())
          if has_reduce:
            uops = full_rewrite(item.ast, renderer)
            asm = renderer.render(uops)

            # Verify the kernel still fits in VGPR limit
            match = re.search(r'\.amdhsa_next_free_vgpr (\d+)', asm)
            self.assertIsNotNone(match)
            vgpr_count = int(match.group(1))
            self.assertLessEqual(vgpr_count, 256,
              f"32x32 WMMA with CAST should fit in 256 VGPRs, got {vgpr_count}")
            return

      # If no kernel found, that's fine
      pass

if __name__ == '__main__':
  unittest.main()
