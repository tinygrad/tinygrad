import pathlib
import numpy as np
from tinygrad import Device, dtypes
from tinygrad.uop.ops import UOp, Ops, KernelInfo

#from extra.assembly.amd.autogen.rdna4.ins import *
from extra.gemm.asm.rdna4.ins import *
from extra.assembly.amd.dsl import RawImm

from extra.gemm.amd_uop_matmul import N, test_matmul

# “2×2 wave-level tiling of a 128×128 workgroup C tile, where each wave computes a 64×64 sub-tile using WMMA.”
N = 4096
WAVE_BLOCK = 2
C_TILE = 64 * WAVE_BLOCK
TN = (N + C_TILE - 1) // C_TILE
NUM_WG = TN * TN
THREADS_PER_WG = 128
assert N % THREADS_PER_WG == 0, "N must be divisible by THREADS_PER_WG"

class Kernel:
  def __init__(self): self.insts:list[str] = []
  def __iadd__(self, inst):
    if isinstance(inst, list):
      for s in inst: self += s
    else: self.insts.append(inst if isinstance(inst, str) else "  "+inst.disasm());
    return self
  def __str__(self) -> str: return (pathlib.Path(__file__).parent/"template.s").read_text().replace("INSTRUCTIONS", "\n".join(self.insts))

def custom_gemm(dev:str) -> UOp:
  lidx = UOp.special(THREADS_PER_WG, "lidx0")
  gidx = UOp.special(NUM_WG, "gidx0")

  A = UOp.placeholder((N*N,), dtypes.half, slot=1)
  B = UOp.placeholder((N*N,), dtypes.half, slot=2)
  C = UOp.placeholder((N*N,), dtypes.half, slot=0)

  k = Kernel()
  k += "kernel_entry:"
  k += s_mov_b32(s[2], ttmp[9])
  k += s_and_b32(s[3], 0xffff, ttmp[7])
  k += s_lshr_b32(s[4], ttmp[7], 16)
  k += s_load_b64(sdata=s[28:29], sbase=s[0:1], ioffset=0x0, soffset=RawImm(124))
  k += s_load_b64(sdata=s[32:33], sbase=s[0:1], ioffset=0x10, soffset=RawImm(124))
  k += s_load_b64(sdata=s[34:35], sbase=s[0:1], ioffset=0x8, soffset=RawImm(124))
  k += s_waitcnt(simm16=64519)
  k += s_mov_b32(s[20], 1)
  k += s_mov_b32(s[21], 0)
  k += s_mov_b32(s[22], 0x2200001)
  k += s_mov_b32(s[11], 0x8010008)
  k += s_mov_b32(s[23], 0x400)
  k += s_mov_b32(s[24], N)
  k += s_mov_b32(s[25], N)
  k += s_mov_b32(s[26], 1)
  k += s_mov_b32(s[27], N)
  k += s_mov_b32(s[36], N)
  k += s_mov_b32(s[37], 0)
  k += s_mov_b32(s[38], N)
  k += s_mov_b32(s[39], 0)
  k += s_mov_b32(s[40], N)
  k += s_mov_b32(s[41], 0)
  k += s_mov_b32(s[42], N)
  k += s_and_b32(s[10], s[22], 0xffff0000)
  k += s_lshr_b32(s[10], s[10], 16)
  k += s_and_b32(s[46], s[22], 0xffff)
  k += s_mov_b32(s[5], s[21])
  k += s_mov_b32(M0, 0x4400)
  k += v_mov_b32_e32(v[254], v[0])
  k += s_mov_b32(VCC_HI, 0)
  k += s_lshr_b32(s[52], s[11], 16)
  k += s_ctz_i32_b32(s[52], s[52])
  k += s_lshr_b32(s[53], s[11], 22)
  k += s_cmp_gt_i32(s[52], 0)
  k += s_cmp_eq_u32(s[21], 0)
  k += v_and_b32_e32(v[1], 31, v[254])
  k += v_and_b32_e32(v[0], 15, v[1])
  k += v_lshlrev_b32_e32(v[0], 2, v[0])
  k += v_lshrrev_b32_e32(v[1], 4, v[1])
  k += v_lshl_add_u32(v[0], v[1], 10, v[0])
  k += v_lshrrev_b32_e32(v[4], 5, v[254])
  k += v_and_b32_e32(v[4], 1, v[4])
  k += v_lshl_add_u32(v[0], v[4], 6, v[0])
  k += v_and_b32_e32(v[2], 31, v[254])
  k += v_and_b32_e32(v[1], 15, v[2])
  k += v_lshlrev_b32_e32(v[1], 5, v[1])
  k += v_lshlrev_b32_e32(v[1], 2, v[1])
  k += v_lshrrev_b32_e32(v[2], 4, v[2])
  k += v_lshl_add_u32(v[1], v[2], 3, v[1])
  k += v_lshrrev_b32_e32(v[3], 6, v[254])
  k += v_and_b32_e32(v[3], 1, v[3])
  k += v_lshl_add_u32(v[1], v[3], 11, v[1])
  k += v_lshrrev_b32_e32(v[2], 5, v[254])
  k += v_lshrrev_b32_e32(v[2], 2, v[2])
  k += s_mov_b32(s[16], 0x1000)
  k += v_mul_lo_u32(v[2], s[16], v[2])
  k += v_add_lshl_u32(v[138], v[2], v[0], 1)
  k += v_lshrrev_b32_e32(v[0], 5, v[254])
  k += v_lshrrev_b32_e32(v[0], 2, v[0])
  k += s_mov_b32(s[16], 32)
  k += v_mul_lo_u32(v[0], s[16], v[0])
  k += v_add_lshl_u32(v[139], v[0], v[1], 1)
  k += v_lshrrev_b32_e32(v[2], 8, v[139])
  k += v_lshl_add_u32(v[139], v[2], 5, v[139])
  k += v_add_co_u32(v[139], VCC_LO, 0x2000, v[139])
  k += v_lshrrev_b32_e32(v[1], 4, v[254])
  k += v_and_b32_e32(v[0], 15, v[254])
  k += v_lshlrev_b32_e32(v[0], 3, v[0])
  k += v_mov_b32_e32(v[4], v[1])
  k += v_lshrrev_b32_e32(v[2], 2, v[254])
  k += v_and_b32_e32(v[3], 3, v[254])
  k += v_lshlrev_b32_e32(v[3], 3, v[3])
  k += v_mov_b32_e32(v[5], v[3])
  k += v_mul_u32_u24_e32(v[136], 0x80, v[4])
  k += v_add_lshl_u32(v[136], v[0], v[136], 1)
  k += v_mul_u32_u24_e32(v[137], 32, v[2])
  k += v_add_lshl_u32(v[137], v[5], v[137], 1)
  k += v_lshrrev_b32_e32(v[6], 8, v[137])
  k += v_lshl_add_u32(v[137], v[6], 5, v[137])
  k += v_add_co_u32(v[137], VCC_LO, 0x2000, v[137])
  k += s_wait_kmcnt(0x0)
  k += v_mov_b32_e32(v[8], 0x80)
  k += v_mov_b32_e32(v[7], s[24])
  k += v_cvt_f32_u32_e32(v[6], v[8])
  k += v_rcp_iflag_f32_e32(v[6], v[6])
  k += v_cvt_f32_u32_e32(v[9], v[7])
  k += v_mul_f32_e32(v[6], v[6], v[9])
  k += v_cvt_u32_f32_e32(v[6], v[6])
  k += v_mul_u32_u24_e32(v[9], v[6], v[8])
  k += v_sub_nc_u32_e32(v[9], v[7], v[9])
  k += v_cmp_ne_u32_e64(RawImm(106), v[9], 0)
  k += v_add_co_ci_u32(v[6], VCC_LO, v[6], 0, VCC_LO)
  k += v_mov_b32_e32(v[8], 0x80)
  k += v_mov_b32_e32(v[7], s[25])
  k += v_readfirstlane_b32_e32(RawImm(14), v[6])
  k += v_cvt_f32_u32_e32(v[6], v[8])
  k += v_rcp_iflag_f32_e32(v[6], v[6])
  k += v_cvt_f32_u32_e32(v[9], v[7])
  k += v_mul_f32_e32(v[6], v[6], v[9])
  k += v_cvt_u32_f32_e32(v[6], v[6])
  k += v_mul_u32_u24_e32(v[9], v[6], v[8])
  k += v_sub_nc_u32_e32(v[9], v[7], v[9])
  k += v_cmp_ne_u32_e64(RawImm(106), v[9], 0)
  k += v_add_co_ci_u32(v[6], VCC_LO, v[6], 0, VCC_LO)
  k += v_readfirstlane_b32_e32(RawImm(15), v[6])
  k += s_mul_i32(s[16], s[14], s[15])
  k += s_and_b32(s[17], s[46], 0x3fff)
  k += s_mul_i32(s[16], s[16], s[17])
  k += v_cvt_f32_u32_e32(v[6], s[16])
  k += v_rcp_iflag_f32_e32(v[6], v[6])
  k += v_cvt_f32_u32_e32(v[7], s[2])
  k += v_mul_f32_e32(v[6], v[6], v[7])
  k += v_cvt_u32_f32_e32(v[6], v[6])
  k += v_mul_u32_u24_e64(v[7], v[6], s[16])
  k += v_sub_nc_u32_e32(v[7], s[2], v[7])
  k += v_cmp_eq_u32_e64(RawImm(106), v[7], s[16])
  k += s_mov_b32(EXEC_LO, VCC_LO)
  k += v_add_nc_u32_e32(v[6], 1, v[6])
  k += s_mov_b32(EXEC_LO, -1)
  k += v_cmp_gt_u32_e64(RawImm(106), v[7], s[16])
  k += s_mov_b32(EXEC_LO, VCC_LO)
  k += v_sub_nc_u32_e64(v[6], v[6], 1)
  k += s_mov_b32(EXEC_LO, -1)
  k += v_readfirstlane_b32_e32(RawImm(16), v[6])
  k += s_mov_b32(s[4], s[16])
  k += s_mul_i32(s[16], s[15], s[14])
  k += s_mul_i32(s[16], s[16], s[4])
  k += s_mul_i32(s[16], s[16], s[17])
  k += s_sub_co_u32(s[2], s[2], s[16])
  k += v_cvt_f32_u32_e32(v[6], s[14])
  k += v_rcp_iflag_f32_e32(v[6], v[6])
  k += v_cvt_f32_u32_e32(v[7], s[2])
  k += v_mul_f32_e32(v[6], v[6], v[7])
  k += v_cvt_u32_f32_e32(v[6], v[6])
  k += v_mul_u32_u24_e64(v[7], v[6], s[14])
  k += v_sub_nc_u32_e32(v[7], s[2], v[7])
  k += v_cmp_eq_u32_e64(RawImm(106), v[7], s[14])
  k += s_mov_b32(EXEC_LO, VCC_LO)
  k += v_add_nc_u32_e32(v[6], 1, v[6])
  k += s_mov_b32(EXEC_LO, -1)
  k += v_cmp_gt_u32_e64(RawImm(106), v[7], s[14])
  k += s_mov_b32(EXEC_LO, VCC_LO)
  k += v_sub_nc_u32_e64(v[6], v[6], 1)
  k += s_mov_b32(EXEC_LO, -1)
  k += v_readfirstlane_b32_e32(RawImm(16), v[6])
  k += s_mov_b32(s[3], s[16])
  k += s_mul_i32(s[16], s[3], s[14])
  k += s_sub_co_u32(s[2], s[2], s[16])
  k += s_mov_b32(s[66], 0x5040100)
  k += s_mov_b32(s[67], 0x7060302)
  k += s_sub_co_u32(s[32], s[32], 16)
  k += s_sub_co_ci_u32(s[33], s[33], 0)
  k += s_sub_co_u32(s[34], s[34], 16)
  k += s_sub_co_ci_u32(s[35], s[35], 0)
  k += s_and_b32(s[16], s[46], 0x3fff)
  k += s_cmp_eq_u32(s[16], 1)
  k += s_mov_b64(s[6:7], 0)
  k += s_mov_b32(s[8], 1)
  k += s_mov_b32(s[9], 1)
  k += s_mov_b32(s[16], s[11])
  k += s_sext_i32_i16(s[16], s[16])
  k += s_cmp_gt_i32(s[16], 1)
  k += s_mov_b32(s[16], s[16])
  k += v_cvt_f64_u32_e32(v[6:7], s[16])
  k += v_rcp_f64_e32(v[6:7], v[6:7])
  k += v_cvt_f64_u32_e32(v[8:9], s[3])
  k += v_mul_f64_e32(v[6:7], v[6:7], v[8:9])
  k += v_cvt_u32_f64_e32(v[6], v[6:7])
  k += v_mul_lo_u32(v[7], v[6], s[16])
  k += v_sub_nc_u32_e32(v[8], s[3], v[7])
  k += v_cmp_ge_u32_e64(RawImm(106), v[8], s[16])
  k += s_mov_b32(EXEC_LO, VCC_LO)
  k += v_add_nc_u32_e64(v[6], v[6], 1)
  k += s_mov_b32(EXEC_LO, -1)
  k += v_readfirstlane_b32_e32(RawImm(17), v[6])
  k += s_mul_i32(s[20], s[17], s[16])
  k += s_sub_co_u32(s[20], s[3], s[20])
  k += s_mul_i32(s[20], s[20], s[14])
  k += s_add_co_u32(s[20], s[20], s[2])
  k += v_cvt_f64_u32_e32(v[6:7], s[16])
  k += v_rcp_f64_e32(v[6:7], v[6:7])
  k += v_cvt_f64_u32_e32(v[8:9], s[15])
  k += v_mul_f64_e32(v[6:7], v[6:7], v[8:9])
  k += v_cvt_u32_f64_e32(v[6], v[6:7])
  k += v_mul_lo_u32(v[7], v[6], s[16])
  k += v_sub_nc_u32_e32(v[8], s[15], v[7])
  k += v_cmp_ge_u32_e64(RawImm(106), v[8], s[16])
  k += s_mov_b32(EXEC_LO, VCC_LO)
  k += v_add_nc_u32_e64(v[6], v[6], 1)
  k += s_mov_b32(EXEC_LO, -1)
  k += v_readfirstlane_b32_e32(RawImm(18), v[6])
  k += s_mul_i32(s[19], s[16], s[18])
  k += s_sub_co_u32(s[19], s[15], s[19])
  k += s_cmp_eq_u32(s[19], 0)
  k += s_cmov_b32(s[19], s[16])
  k += s_cmp_ge_u32(s[17], s[18])
  k += s_cselect_b32(s[18], s[19], s[16])
  k += v_cvt_f64_u32_e32(v[6:7], s[18])
  k += v_rcp_f64_e32(v[6:7], v[6:7])
  k += v_cvt_f64_u32_e32(v[8:9], s[20])
  k += v_mul_f64_e32(v[6:7], v[6:7], v[8:9])
  k += v_cvt_u32_f64_e32(v[6], v[6:7])
  k += v_mul_lo_u32(v[7], v[6], s[18])
  k += v_sub_nc_u32_e32(v[8], s[20], v[7])
  k += v_cmp_ge_u32_e64(RawImm(106), v[8], s[18])
  k += s_mov_b32(EXEC_LO, VCC_LO)
  k += v_add_nc_u32_e64(v[6], v[6], 1)
  k += s_mov_b32(EXEC_LO, -1)
  k += v_mul_lo_u32(v[7], v[6], s[18])
  k += v_sub_nc_u32_e32(v[8], s[20], v[7])
  k += v_readfirstlane_b32_e32(RawImm(2), v[6])
  k += v_readfirstlane_b32_e32(RawImm(3), v[8])
  k += s_mul_i32(s[3], s[2], s[18])
  k += s_sub_co_u32(s[3], s[20], s[3])
  k += s_mul_i32(s[17], s[17], s[16])
  k += s_add_co_u32(s[3], s[3], s[17])
  k += v_mov_b32_e32(v[6], v[0])
  k += v_mov_b32_e32(v[7], v[2])
  k += v_add_co_u32(v[8], VCC_LO, 32, v[7])
  k += v_add_co_u32(v[9], VCC_LO, 32, v[8])
  k += v_add_co_u32(v[10], VCC_LO, 32, v[9])
  k += v_mov_b32_e32(v[11], v[1])
  k += v_add_co_u32(v[12], VCC_LO, 8, v[11])
  k += v_add_co_u32(v[13], VCC_LO, 8, v[12])
  k += v_add_co_u32(v[14], VCC_LO, 8, v[13])
  k += v_mov_b32_e32(v[15], v[3])
  k += s_mul_i32(s[16], s[2], 0x80)
  k += s_sub_co_u32(s[16], s[24], s[16])
  k += s_sub_co_u32(s[16], s[16], 8)
  k += v_mov_b32_e32(v[16], s[16])
  k += v_min_i32_e32(v[6], v[16], v[6])
  k += s_mul_hi_u32(s[19], s[2], 0x80)
  k += s_mul_i32(s[18], s[2], 0x80)
  k += s_and_b32(s[16], s[46], 0x8000)
  k += s_cbranch_scc1("skip_gsu_offset_A")
  k += s_mul_hi_u32(s[17], 32, s[6])
  k += s_mul_i32(s[16], 32, s[6])
  k += "skip_gsu_offset_A:"
  k += s_mul_hi_u32(s[17], s[16], s[40])
  k += s_mul_i32(s[16], s[16], s[40])
  k += s_add_co_u32(s[18], s[18], s[16])
  k += s_add_co_ci_u32(s[19], s[19], s[17])
  k += s_mov_b64(s[56:57], 1)
  k += s_sub_co_u32(s[16], s[24], 1)
  k += s_mul_hi_u32(s[17], 1, s[16])
  k += s_mul_i32(s[16], 1, s[16])
  k += s_add_co_u32(s[56], s[56], s[16])
  k += s_add_co_ci_u32(s[57], s[57], s[17])
  k += s_sub_co_u32(s[16], s[27], 1)
  k += s_mul_hi_u32(s[17], s[40], s[16])
  k += s_mul_i32(s[16], s[40], s[16])
  k += s_add_co_u32(s[56], s[56], s[16])
  k += s_add_co_ci_u32(s[57], s[57], s[17])
  k += s_sub_co_u32(s[56], s[56], s[18])
  k += s_sub_co_ci_u32(s[57], s[57], s[19])
  k += s_lshl_b64(s[56:57], s[56:57], 1)
  k += s_add_co_u32(s[56], s[56], 16)
  k += s_add_co_ci_u32(s[57], s[57], 0)
  k += s_cmp_eq_u32(s[57], 0)
  k += s_cselect_b32(s[50], s[56], -1)
  k += s_mul_hi_u32(s[17], s[41], s[4])
  k += s_mul_i32(s[16], s[41], s[4])
  k += s_add_co_u32(s[18], s[18], s[16])
  k += s_add_co_ci_u32(s[19], s[19], s[17])
  k += s_lshl_b64(s[18:19], s[18:19], 1)
  k += s_add_co_u32(s[48], s[32], s[18])
  k += s_add_co_ci_u32(s[49], s[33], s[19])
  k += s_mov_b32(s[51], 0x30020000)
  k += s_mul_hi_u32(s[19], s[3], 0x80)
  k += s_mul_i32(s[18], s[3], 0x80)
  k += s_mul_hi_u32(s[19], s[18], s[42])
  k += s_mul_i32(s[18], s[18], s[42])
  k += s_and_b32(s[16], s[46], 0x8000)
  k += s_cbranch_scc1("skip_gsu_offset_B")
  k += s_mul_hi_u32(s[17], 32, s[6])
  k += s_mul_i32(s[16], 32, s[6])
  k += "skip_gsu_offset_B:"
  k += s_add_co_u32(s[18], s[18], s[16])
  k += s_add_co_ci_u32(s[19], s[19], s[17])
  k += s_mov_b64(s[58:59], 1)
  k += s_sub_co_u32(s[16], s[27], 1)
  k += s_mul_hi_u32(s[17], 1, s[16])
  k += s_mul_i32(s[16], 1, s[16])
  k += s_add_co_u32(s[58], s[58], s[16])
  k += s_add_co_ci_u32(s[59], s[59], s[17])
  k += s_sub_co_u32(s[16], s[25], 1)
  k += s_mul_hi_u32(s[17], s[42], s[16])
  k += s_mul_i32(s[16], s[42], s[16])
  k += s_add_co_u32(s[58], s[58], s[16])
  k += s_add_co_ci_u32(s[59], s[59], s[17])
  k += s_sub_co_u32(s[58], s[58], s[18])
  k += s_sub_co_ci_u32(s[59], s[59], s[19])
  k += s_lshl_b64(s[58:59], s[58:59], 1)
  k += s_add_co_u32(s[58], s[58], 16)
  k += s_add_co_ci_u32(s[59], s[59], 0)
  k += s_cmp_eq_u32(s[59], 0)
  k += s_cselect_b32(s[54], s[58], -1)
  k += s_mov_b32(s[17], 0)
  k += s_mov_b32(s[16], 0)
  k += s_add_co_u32(s[18], s[18], s[16])
  k += s_add_co_ci_u32(s[19], s[19], s[17])
  k += s_lshl_b64(s[18:19], s[18:19], 1)
  k += s_add_co_u32(s[52], s[34], s[18])
  k += s_add_co_ci_u32(s[53], s[35], s[19])
  k += s_mov_b32(s[55], 0x30020000)
  k += v_mul_lo_u32(v[16], s[40], v[11])
  k += v_add_co_u32(v[128], VCC_LO, v[6], v[16])
  k += v_add_nc_u32_e32(v[128], 8, v[128])
  k += v_lshlrev_b32_e32(v[128], 1, v[128])
  k += v_mul_lo_u32(v[16], s[40], v[12])
  k += v_add_co_u32(v[129], VCC_LO, v[6], v[16])
  k += v_add_nc_u32_e32(v[129], 8, v[129])
  k += v_lshlrev_b32_e32(v[129], 1, v[129])
  k += v_mul_lo_u32(v[16], s[40], v[13])
  k += v_add_co_u32(v[130], VCC_LO, v[6], v[16])
  k += v_add_nc_u32_e32(v[130], 8, v[130])
  k += v_lshlrev_b32_e32(v[130], 1, v[130])
  k += v_mul_lo_u32(v[16], s[40], v[14])
  k += v_add_co_u32(v[131], VCC_LO, v[6], v[16])
  k += v_add_nc_u32_e32(v[131], 8, v[131])
  k += v_lshlrev_b32_e32(v[131], 1, v[131])
  k += v_mul_lo_u32(v[11], s[42], v[7])
  k += v_add_co_u32(v[132], VCC_LO, v[15], v[11])
  k += v_add_nc_u32_e32(v[132], 8, v[132])
  k += v_lshlrev_b32_e32(v[132], 1, v[132])
  k += v_mul_lo_u32(v[11], s[42], v[8])
  k += v_add_co_u32(v[133], VCC_LO, v[15], v[11])
  k += v_add_nc_u32_e32(v[133], 8, v[133])
  k += v_lshlrev_b32_e32(v[133], 1, v[133])
  k += v_mul_lo_u32(v[11], s[42], v[9])
  k += v_add_co_u32(v[134], VCC_LO, v[15], v[11])
  k += v_add_nc_u32_e32(v[134], 8, v[134])
  k += v_lshlrev_b32_e32(v[134], 1, v[134])
  k += v_mul_lo_u32(v[11], s[42], v[10])
  k += v_add_co_u32(v[135], VCC_LO, v[15], v[11])
  k += v_add_nc_u32_e32(v[135], 8, v[135])
  k += v_lshlrev_b32_e32(v[135], 1, v[135])
  k += s_and_b32(s[17], s[46], 0x3fff)
  k += s_mul_i32(s[17], s[17], 64)
  k += s_and_b32(s[16], s[46], 0x8000)
  k += s_cmov_b32(s[17], 64)
  k += s_mul_i32(s[64], s[17], s[40])
  k += s_and_b32(s[17], s[46], 0x3fff)
  k += s_mul_i32(s[17], s[17], 64)
  k += s_and_b32(s[16], s[46], 0x8000)
  k += s_cselect_b32(s[65], 64, s[17])
  k += s_lshr_b32(s[12], s[27], 5)
  k += s_and_b32(s[16], s[46], 0x3fff)
  k += s_cmp_eq_u32(s[16], 1)
  k += s_mov_b32(s[13], s[12])
  k += s_and_b32(s[18], s[10], 0x1f00)
  k += s_lshr_b32(s[18], s[18], 8)
  k += s_and_b32(s[19], s[10], 0xe000)
  k += s_and_b32(s[10], s[10], 0xff)
  k += s_mov_b32(s[16], s[10])
  k += s_lshl_b32(s[17], s[16], s[18])
  k += s_cmp_ge_u32(s[13], s[17])
  k += s_sub_co_u32(s[17], s[16], 1)
  k += s_cmp_ge_u32(s[16], 1)
  k += s_cselect_b32(s[47], s[17], 0)
  k += s_cmp_eq_u32(s[19], 0)
  k += s_cmp_eq_u32(s[19], 0x2000)
  k += s_cbranch_scc1("stagger_done")
  k += s_mov_b32(s[16], s[3])
  k += "stagger_done:"
  k += s_and_b32(s[47], s[47], s[16])
  k += s_lshl_b32(s[47], s[47], s[18])
  k += s_mul_hi_i32(s[17], s[47], s[64])
  k += s_mul_i32(s[16], s[47], s[64])
  k += s_mul_hi_i32(s[61], s[12], s[64])
  k += s_mul_i32(s[60], s[12], s[64])
  k += s_sub_co_u32(s[60], s[64], s[60])
  k += s_sub_co_ci_u32(s[61], 0, s[61])
  k += s_add_co_u32(s[48], s[48], s[16])
  k += s_add_co_ci_u32(s[49], s[49], s[17])
  k += s_sub_co_u32(s[56], s[56], s[16])
  k += s_sub_co_ci_u32(s[57], s[57], s[17])
  k += s_cmp_eq_u32(s[57], 0)
  k += s_cselect_b32(s[50], s[56], -1)
  k += s_mul_hi_i32(s[17], s[47], s[65])
  k += s_mul_i32(s[16], s[47], s[65])
  k += s_mul_hi_i32(s[63], s[12], s[65])
  k += s_mul_i32(s[62], s[12], s[65])
  k += s_sub_co_u32(s[62], s[65], s[62])
  k += s_sub_co_ci_u32(s[63], 0, s[63])
  k += s_add_co_u32(s[52], s[52], s[16])
  k += s_add_co_ci_u32(s[53], s[53], s[17])
  k += s_sub_co_u32(s[58], s[58], s[16])
  k += s_sub_co_ci_u32(s[59], s[59], s[17])
  k += s_cmp_eq_u32(s[59], 0)
  k += s_cselect_b32(s[54], s[58], -1)
  k += s_add_co_u32(s[47], s[47], 2)
  k += s_cmp_eq_u32(s[12], 0)
  k += s_cbranch_scc1("skip_first_prefetch")
  # Prefetch A matrix tiles (4 loads from rsrc=48)
  k += [buffer_load_b128(v[222+i*4:226+i*4], v[128+i], 48) for i in range(4)]
  # Prefetch B matrix tiles (4 loads from rsrc=52)
  k += [buffer_load_b128(v[238+i*4:242+i*4], v[132+i], 52) for i in range(4)]
  k += s_add_co_u32(s[18], s[12], 1)
  k += s_cmp_eq_u32(s[47], s[18])
  k += s_cselect_b32(s[16], s[60], s[64])
  k += s_cselect_b32(s[17], s[61], 0)
  k += s_add_co_u32(s[48], s[48], s[16])
  k += s_add_co_ci_u32(s[49], s[49], s[17])
  k += s_sub_co_u32(s[56], s[56], s[16])
  k += s_sub_co_ci_u32(s[57], s[57], s[17])
  k += s_cmp_eq_u32(s[57], 0)
  k += s_cselect_b32(s[50], s[56], -1)
  k += s_add_co_u32(s[18], s[12], 1)
  k += s_cmp_eq_u32(s[47], s[18])
  k += s_cselect_b32(s[16], s[62], s[65])
  k += s_cselect_b32(s[17], s[63], 0)
  k += s_add_co_u32(s[52], s[52], s[16])
  k += s_add_co_ci_u32(s[53], s[53], s[17])
  k += s_sub_co_u32(s[58], s[58], s[16])
  k += s_sub_co_ci_u32(s[59], s[59], s[17])
  k += s_cmp_eq_u32(s[59], 0)
  k += s_cselect_b32(s[54], s[58], -1)
  k += "skip_first_prefetch:"
  k += s_mov_b64(s[16:17], s[28:29])
  k += s_mov_b32(s[18], 0x80000000)
  k += s_mov_b32(s[19], 0x30020000)
  k += s_mov_b64(s[20:21], s[30:31])
  k += s_mov_b32(s[22], 0x80000000)
  k += s_mov_b32(s[23], 0x30020000)
  k += s_mul_i32(s[70], 0x80, s[3])
  k += s_mul_hi_u32(s[69], s[70], s[38])
  k += s_mul_i32(s[68], s[70], s[38])
  k += s_lshl_b64(s[68:69], s[68:69], s[8])
  k += s_add_co_u32(s[20], s[30], s[68])
  k += s_add_co_ci_u32(s[21], s[31], s[69])
  k += s_mul_hi_u32(s[69], s[70], s[36])
  k += s_mul_i32(s[68], s[70], s[36])
  k += s_lshl_b64(s[68:69], s[68:69], s[9])
  k += s_add_co_u32(s[16], s[28], s[68])
  k += s_add_co_ci_u32(s[17], s[29], s[69])
  k += s_mul_hi_u32(s[69], s[4], s[39])
  k += s_mul_i32(s[68], s[4], s[39])
  k += s_lshl_b64(s[68:69], s[68:69], s[8])
  k += s_add_co_u32(s[20], s[20], s[68])
  k += s_add_co_ci_u32(s[21], s[21], s[69])
  k += s_mul_hi_u32(s[69], s[4], s[37])
  k += s_mul_i32(s[68], s[4], s[37])
  k += s_lshl_b64(s[68:69], s[68:69], s[9])
  k += s_add_co_u32(s[16], s[16], s[68])
  k += s_add_co_ci_u32(s[17], s[17], s[69])
  k += s_and_b32(s[68], s[46], 0x3fff)
  k += s_cmp_eq_u32(s[68], 1)
  # Zero initialize accumulators v[0:127]
  k += [v_mov_b32_e32(v[i], 0) for i in range(128)]
  k += s_wait_loadcnt(0x0)
  k += ds_store_b128(v[136], v[222:225], offset1=0)
  k += ds_store_b128(v[136], v[226:229], offset1=8)
  k += ds_store_b128(v[136], v[230:233], offset1=16)
  k += ds_store_b128(v[136], v[234:237], offset1=24)
  k += ds_store_b128(v[137], v[238:241], offset1=0)
  k += ds_store_b128(v[137], v[242:245], offset1=9)
  k += ds_store_b128(v[137], v[246:249], offset1=18)
  k += ds_store_b128(v[137], v[250:253], offset1=27)
  k += s_cmp_eq_u32(s[12], 1)
  k += s_cbranch_scc1("skip_prefetch_2")
  # Prefetch A matrix tiles (4 loads from rsrc=48)
  k += [buffer_load_b128(v[222+i*4:226+i*4], v[128+i], 48) for i in range(4)]
  # Prefetch B matrix tiles (4 loads from rsrc=52)
  k += [buffer_load_b128(v[238+i*4:242+i*4], v[132+i], 52) for i in range(4)]
  k += "skip_prefetch_2:"
  k += s_wait_dscnt(0x0)
  k += barrier_signal()
  k += s_barrier_wait(0xffff)
  # Load A tile from LDS: 8x b64 loads
  k += [ds_load_b64(v[156+i*2:158+i*2], v[138], offset1=i) for i in range(8)]
  # Load B tile from LDS: 4x b128 loads
  k += [ds_load_b128(v[189+i*4:193+i*4], v[139], offset0=i*64) for i in range(4)]
  k += s_cmp_eq_u32(s[12], 1)
  k += s_cbranch_scc1("final_wmma")
  k += s_cmp_le_u32(s[12], 2)
  k += s_cbranch_scc1("loop_epilogue")
  k += s_nop(0)
  k += "main_loop:"
  k += s_wait_dscnt(0x3)
  k += v_perm_b32(v[140], v[158], v[156], s[66])
  k += v_perm_b32(v[141], v[162], v[160], s[66])
  k += v_perm_b32(v[142], v[166], v[164], s[66])
  k += v_perm_b32(v[143], v[170], v[168], s[66])
  k += v_perm_b32(v[144], v[158], v[156], s[67])
  k += v_perm_b32(v[145], v[162], v[160], s[67])
  k += v_wmma_f32_16x16x16_f16(v[0:7], v[189:192], v[140:143], v[0:7])
  k += ds_load_b64(v[172:173], v[138], offset1=16)
  k += ds_load_b64(v[174:175], v[138], offset1=17)
  k += s_cmp_eq_u32(s[12], s[47])
  k += s_cselect_b32(s[68], s[60], s[64])
  k += s_cselect_b32(s[69], s[61], 0)
  k += v_perm_b32(v[146], v[166], v[164], s[67])
  k += v_perm_b32(v[147], v[170], v[168], s[67])
  k += v_perm_b32(v[148], v[159], v[157], s[66])
  k += v_perm_b32(v[149], v[163], v[161], s[66])
  k += v_wmma_f32_16x16x16_f16(v[8:15], v[189:192], v[144:147], v[8:15])
  k += ds_load_b64(v[176:177], v[138], offset1=18)
  k += ds_load_b64(v[178:179], v[138], offset1=19)
  k += s_add_co_u32(s[48], s[48], s[68])
  k += s_add_co_ci_u32(s[49], s[49], s[69])
  k += s_sub_co_u32(s[56], s[56], s[68])
  k += v_perm_b32(v[150], v[167], v[165], s[66])
  k += v_perm_b32(v[151], v[171], v[169], s[66])
  k += v_perm_b32(v[152], v[159], v[157], s[67])
  k += v_perm_b32(v[153], v[163], v[161], s[67])
  k += v_wmma_f32_16x16x16_f16(v[16:23], v[189:192], v[148:151], v[16:23])
  k += ds_load_b64(v[180:181], v[138], offset1=20)
  k += ds_load_b64(v[182:183], v[138], offset1=21)
  k += s_sub_co_ci_u32(s[57], s[57], s[69])
  k += s_cmp_eq_u32(s[57], 0)
  k += s_cselect_b32(s[50], s[56], -1)
  k += v_perm_b32(v[154], v[167], v[165], s[67])
  k += v_perm_b32(v[155], v[171], v[169], s[67])
  k += s_nop(1)
  k += v_wmma_f32_16x16x16_f16(v[24:31], v[189:192], v[152:155], v[24:31])
  k += ds_load_b64(v[184:185], v[138], offset1=22)
  k += ds_load_b64(v[186:187], v[138], offset1=23)
  k += s_cmp_eq_u32(s[12], s[47])
  k += s_cselect_b32(s[68], s[62], s[65])
  k += s_cselect_b32(s[69], s[63], 0)
  k += s_wait_dscnt(0x8)
  k += v_wmma_f32_16x16x16_f16(v[32:39], v[193:196], v[140:143], v[32:39])
  k += ds_load_b128(v[205:208], v[139], offset0=32)
  k += s_add_co_u32(s[52], s[52], s[68])
  k += s_add_co_ci_u32(s[53], s[53], s[69])
  k += s_sub_co_u32(s[58], s[58], s[68])
  k += v_wmma_f32_16x16x16_f16(v[40:47], v[193:196], v[144:147], v[40:47])
  k += ds_load_b128(v[209:212], v[139], offset0=96)
  k += s_sub_co_ci_u32(s[59], s[59], s[69])
  k += s_cmp_eq_u32(s[59], 0)
  k += s_cselect_b32(s[54], s[58], -1)
  k += v_wmma_f32_16x16x16_f16(v[48:55], v[193:196], v[148:151], v[48:55])
  k += ds_load_b128(v[213:216], v[139], offset0=160)
  k += v_wmma_f32_16x16x16_f16(v[56:63], v[193:196], v[152:155], v[56:63])
  k += ds_load_b128(v[217:220], v[139], offset0=224)
  k += v_wmma_f32_16x16x16_f16(v[64:71], v[197:200], v[140:143], v[64:71])
  k += v_wmma_f32_16x16x16_f16(v[72:79], v[197:200], v[144:147], v[72:79])
  k += v_wmma_f32_16x16x16_f16(v[80:87], v[197:200], v[148:151], v[80:87])
  k += v_wmma_f32_16x16x16_f16(v[88:95], v[197:200], v[152:155], v[88:95])
  k += v_wmma_f32_16x16x16_f16(v[96:103], v[201:204], v[140:143], v[96:103])
  k += s_wait_dscnt(0x0)
  k += barrier_signal()
  k += s_barrier_wait(0xffff)
  k += v_wmma_f32_16x16x16_f16(v[104:111], v[201:204], v[144:147], v[104:111])
  k += s_wait_loadcnt(0x7)
  k += ds_store_b128(v[136], v[222:225], offset1=0)
  k += buffer_load_b128(v[222:225], v[128], 48)
  k += s_wait_loadcnt(0x7)
  k += ds_store_b128(v[136], v[226:229], offset1=8)
  k += v_wmma_f32_16x16x16_f16(v[112:119], v[201:204], v[148:151], v[112:119])
  k += buffer_load_b128(v[226:229], v[129], 48)
  k += s_wait_loadcnt(0x7)
  k += ds_store_b128(v[136], v[230:233], offset1=16)
  k += buffer_load_b128(v[230:233], v[130], 48)
  k += v_wmma_f32_16x16x16_f16(v[120:127], v[201:204], v[152:155], v[120:127])
  k += s_wait_loadcnt(0x7)
  k += ds_store_b128(v[136], v[234:237], offset1=24)
  k += buffer_load_b128(v[234:237], v[131], 48)
  k += s_wait_loadcnt(0x7)
  k += ds_store_b128(v[137], v[238:241], offset1=0)
  k += s_wait_dscnt(0x5)
  k += v_perm_b32(v[140], v[174], v[172], s[66])
  k += v_perm_b32(v[141], v[178], v[176], s[66])
  k += v_perm_b32(v[142], v[182], v[180], s[66])
  k += v_perm_b32(v[143], v[186], v[184], s[66])
  k += v_perm_b32(v[144], v[174], v[172], s[67])
  k += v_perm_b32(v[145], v[178], v[176], s[67])
  k += v_wmma_f32_16x16x16_f16(v[0:7], v[205:208], v[140:143], v[0:7])
  k += buffer_load_b128(v[238:241], v[132], 52)
  k += s_wait_loadcnt(0x7)
  k += ds_store_b128(v[137], v[242:245], offset1=9)
  k += buffer_load_b128(v[242:245], v[133], 52)
  k += v_perm_b32(v[146], v[182], v[180], s[67])
  k += v_perm_b32(v[147], v[186], v[184], s[67])
  k += v_perm_b32(v[148], v[175], v[173], s[66])
  k += v_perm_b32(v[149], v[179], v[177], s[66])
  k += v_wmma_f32_16x16x16_f16(v[8:15], v[205:208], v[144:147], v[8:15])
  k += s_wait_loadcnt(0x7)
  k += ds_store_b128(v[137], v[246:249], offset1=18)
  k += buffer_load_b128(v[246:249], v[134], 52)
  k += s_wait_loadcnt(0x7)
  k += ds_store_b128(v[137], v[250:253], offset1=27)
  k += v_perm_b32(v[150], v[183], v[181], s[66])
  k += v_perm_b32(v[151], v[187], v[185], s[66])
  k += v_perm_b32(v[152], v[175], v[173], s[67])
  k += v_perm_b32(v[153], v[179], v[177], s[67])
  k += v_wmma_f32_16x16x16_f16(v[16:23], v[205:208], v[148:151], v[16:23])
  k += buffer_load_b128(v[250:253], v[135], 52)
  k += v_perm_b32(v[154], v[183], v[181], s[67])
  k += v_perm_b32(v[155], v[187], v[185], s[67])
  k += s_nop(1)
  k += v_wmma_f32_16x16x16_f16(v[24:31], v[205:208], v[152:155], v[24:31])
  k += v_wmma_f32_16x16x16_f16(v[32:39], v[209:212], v[140:143], v[32:39])
  k += s_wait_dscnt(0x0)
  k += barrier_signal()
  k += s_barrier_wait(0xffff)
  k += v_wmma_f32_16x16x16_f16(v[40:47], v[209:212], v[144:147], v[40:47])
  k += ds_load_b64(v[156:157], v[138], offset1=0)
  k += ds_load_b64(v[158:159], v[138], offset1=1)
  k += v_wmma_f32_16x16x16_f16(v[48:55], v[209:212], v[148:151], v[48:55])
  k += ds_load_b64(v[160:161], v[138], offset1=2)
  k += ds_load_b64(v[162:163], v[138], offset1=3)
  k += v_wmma_f32_16x16x16_f16(v[56:63], v[209:212], v[152:155], v[56:63])
  k += ds_load_b64(v[164:165], v[138], offset1=4)
  k += ds_load_b64(v[166:167], v[138], offset1=5)
  k += v_wmma_f32_16x16x16_f16(v[64:71], v[213:216], v[140:143], v[64:71])
  k += ds_load_b64(v[168:169], v[138], offset1=6)
  k += ds_load_b64(v[170:171], v[138], offset1=7)
  k += v_wmma_f32_16x16x16_f16(v[72:79], v[213:216], v[144:147], v[72:79])
  k += ds_load_b128(v[189:192], v[139], offset0=0)
  k += v_wmma_f32_16x16x16_f16(v[80:87], v[213:216], v[148:151], v[80:87])
  k += ds_load_b128(v[193:196], v[139], offset0=64)
  k += v_wmma_f32_16x16x16_f16(v[88:95], v[213:216], v[152:155], v[88:95])
  k += ds_load_b128(v[197:200], v[139], offset0=128)
  k += v_wmma_f32_16x16x16_f16(v[96:103], v[217:220], v[140:143], v[96:103])
  k += ds_load_b128(v[201:204], v[139], offset0=192)
  k += v_wmma_f32_16x16x16_f16(v[104:111], v[217:220], v[144:147], v[104:111])
  k += v_wmma_f32_16x16x16_f16(v[112:119], v[217:220], v[148:151], v[112:119])
  k += v_wmma_f32_16x16x16_f16(v[120:127], v[217:220], v[152:155], v[120:127])
  k += s_sub_co_u32(s[12], s[12], 1)
  k += s_cmp_eq_i32(s[12], 2)
  k += s_cbranch_scc0("main_loop")
  k += "loop_epilogue:"
  k += s_wait_dscnt(0x3)
  k += v_perm_b32(v[140], v[158], v[156], s[66])
  k += v_perm_b32(v[141], v[162], v[160], s[66])
  k += v_perm_b32(v[142], v[166], v[164], s[66])
  k += v_perm_b32(v[143], v[170], v[168], s[66])
  k += v_perm_b32(v[144], v[158], v[156], s[67])
  k += v_perm_b32(v[145], v[162], v[160], s[67])
  k += v_wmma_f32_16x16x16_f16(v[0:7], v[189:192], v[140:143], v[0:7])
  k += ds_load_b64(v[172:173], v[138], offset1=16)
  k += ds_load_b64(v[174:175], v[138], offset1=17)
  k += s_cmp_eq_u32(s[12], s[47])
  k += s_cselect_b32(s[68], s[60], s[64])
  k += s_cselect_b32(s[69], s[61], 0)
  k += v_perm_b32(v[146], v[166], v[164], s[67])
  k += v_perm_b32(v[147], v[170], v[168], s[67])
  k += v_perm_b32(v[148], v[159], v[157], s[66])
  k += v_perm_b32(v[149], v[163], v[161], s[66])
  k += v_wmma_f32_16x16x16_f16(v[8:15], v[189:192], v[144:147], v[8:15])
  k += ds_load_b64(v[176:177], v[138], offset1=18)
  k += ds_load_b64(v[178:179], v[138], offset1=19)
  k += s_add_co_u32(s[48], s[48], s[68])
  k += s_add_co_ci_u32(s[49], s[49], s[69])
  k += s_sub_co_u32(s[56], s[56], s[68])
  k += v_perm_b32(v[150], v[167], v[165], s[66])
  k += v_perm_b32(v[151], v[171], v[169], s[66])
  k += v_perm_b32(v[152], v[159], v[157], s[67])
  k += v_perm_b32(v[153], v[163], v[161], s[67])
  k += v_wmma_f32_16x16x16_f16(v[16:23], v[189:192], v[148:151], v[16:23])
  k += ds_load_b64(v[180:181], v[138], offset1=20)
  k += ds_load_b64(v[182:183], v[138], offset1=21)
  k += s_sub_co_ci_u32(s[57], s[57], s[69])
  k += s_cmp_eq_u32(s[57], 0)
  k += s_cselect_b32(s[50], s[56], -1)
  k += v_perm_b32(v[154], v[167], v[165], s[67])
  k += v_perm_b32(v[155], v[171], v[169], s[67])
  k += s_nop(1)
  k += v_wmma_f32_16x16x16_f16(v[24:31], v[189:192], v[152:155], v[24:31])
  k += ds_load_b64(v[184:185], v[138], offset1=22)
  k += ds_load_b64(v[186:187], v[138], offset1=23)
  k += s_cmp_eq_u32(s[12], s[47])
  k += s_cselect_b32(s[68], s[62], s[65])
  k += s_cselect_b32(s[69], s[63], 0)
  k += s_wait_dscnt(0x8)
  k += v_wmma_f32_16x16x16_f16(v[32:39], v[193:196], v[140:143], v[32:39])
  k += ds_load_b128(v[205:208], v[139], offset0=32)
  k += s_add_co_u32(s[52], s[52], s[68])
  k += s_add_co_ci_u32(s[53], s[53], s[69])
  k += s_sub_co_u32(s[58], s[58], s[68])
  k += v_wmma_f32_16x16x16_f16(v[40:47], v[193:196], v[144:147], v[40:47])
  k += ds_load_b128(v[209:212], v[139], offset0=96)
  k += s_sub_co_ci_u32(s[59], s[59], s[69])
  k += s_cmp_eq_u32(s[59], 0)
  k += s_cselect_b32(s[54], s[58], -1)
  k += v_wmma_f32_16x16x16_f16(v[48:55], v[193:196], v[148:151], v[48:55])
  k += ds_load_b128(v[213:216], v[139], offset0=160)
  k += v_wmma_f32_16x16x16_f16(v[56:63], v[193:196], v[152:155], v[56:63])
  k += ds_load_b128(v[217:220], v[139], offset0=224)
  k += v_wmma_f32_16x16x16_f16(v[64:71], v[197:200], v[140:143], v[64:71])
  k += v_wmma_f32_16x16x16_f16(v[72:79], v[197:200], v[144:147], v[72:79])
  k += v_wmma_f32_16x16x16_f16(v[80:87], v[197:200], v[148:151], v[80:87])
  k += v_wmma_f32_16x16x16_f16(v[88:95], v[197:200], v[152:155], v[88:95])
  k += v_wmma_f32_16x16x16_f16(v[96:103], v[201:204], v[140:143], v[96:103])
  k += s_wait_dscnt(0x0)
  k += barrier_signal()
  k += s_barrier_wait(0xffff)
  k += v_wmma_f32_16x16x16_f16(v[104:111], v[201:204], v[144:147], v[104:111])
  k += s_wait_loadcnt(0x7)
  k += ds_store_b128(v[136], v[222:225], offset1=0)
  k += s_wait_loadcnt(0x6)
  k += ds_store_b128(v[136], v[226:229], offset1=8)
  k += v_wmma_f32_16x16x16_f16(v[112:119], v[201:204], v[148:151], v[112:119])
  k += s_wait_loadcnt(0x5)
  k += ds_store_b128(v[136], v[230:233], offset1=16)
  k += v_wmma_f32_16x16x16_f16(v[120:127], v[201:204], v[152:155], v[120:127])
  k += s_wait_loadcnt(0x4)
  k += ds_store_b128(v[136], v[234:237], offset1=24)
  k += s_wait_loadcnt(0x3)
  k += ds_store_b128(v[137], v[238:241], offset1=0)
  k += s_wait_dscnt(0x5)
  k += v_perm_b32(v[140], v[174], v[172], s[66])
  k += v_perm_b32(v[141], v[178], v[176], s[66])
  k += v_perm_b32(v[142], v[182], v[180], s[66])
  k += v_perm_b32(v[143], v[186], v[184], s[66])
  k += v_perm_b32(v[144], v[174], v[172], s[67])
  k += v_perm_b32(v[145], v[178], v[176], s[67])
  k += v_wmma_f32_16x16x16_f16(v[0:7], v[205:208], v[140:143], v[0:7])
  k += s_wait_loadcnt(0x2)
  k += ds_store_b128(v[137], v[242:245], offset1=9)
  k += v_perm_b32(v[146], v[182], v[180], s[67])
  k += v_perm_b32(v[147], v[186], v[184], s[67])
  k += v_perm_b32(v[148], v[175], v[173], s[66])
  k += v_perm_b32(v[149], v[179], v[177], s[66])
  k += v_wmma_f32_16x16x16_f16(v[8:15], v[205:208], v[144:147], v[8:15])
  k += s_wait_loadcnt(0x1)
  k += ds_store_b128(v[137], v[246:249], offset1=18)
  k += s_wait_loadcnt(0x0)
  k += ds_store_b128(v[137], v[250:253], offset1=27)
  k += v_perm_b32(v[150], v[183], v[181], s[66])
  k += v_perm_b32(v[151], v[187], v[185], s[66])
  k += v_perm_b32(v[152], v[175], v[173], s[67])
  k += v_perm_b32(v[153], v[179], v[177], s[67])
  k += v_wmma_f32_16x16x16_f16(v[16:23], v[205:208], v[148:151], v[16:23])
  k += v_perm_b32(v[154], v[183], v[181], s[67])
  k += v_perm_b32(v[155], v[187], v[185], s[67])
  k += s_nop(1)
  k += v_wmma_f32_16x16x16_f16(v[24:31], v[205:208], v[152:155], v[24:31])
  k += v_wmma_f32_16x16x16_f16(v[32:39], v[209:212], v[140:143], v[32:39])
  k += s_wait_dscnt(0x0)
  k += barrier_signal()
  k += s_barrier_wait(0xffff)
  k += v_wmma_f32_16x16x16_f16(v[40:47], v[209:212], v[144:147], v[40:47])
  k += ds_load_b64(v[156:157], v[138], offset1=0)
  k += ds_load_b64(v[158:159], v[138], offset1=1)
  k += v_wmma_f32_16x16x16_f16(v[48:55], v[209:212], v[148:151], v[48:55])
  k += ds_load_b64(v[160:161], v[138], offset1=2)
  k += ds_load_b64(v[162:163], v[138], offset1=3)
  k += v_wmma_f32_16x16x16_f16(v[56:63], v[209:212], v[152:155], v[56:63])
  k += ds_load_b64(v[164:165], v[138], offset1=4)
  k += ds_load_b64(v[166:167], v[138], offset1=5)
  k += v_wmma_f32_16x16x16_f16(v[64:71], v[213:216], v[140:143], v[64:71])
  k += ds_load_b64(v[168:169], v[138], offset1=6)
  k += ds_load_b64(v[170:171], v[138], offset1=7)
  k += v_wmma_f32_16x16x16_f16(v[72:79], v[213:216], v[144:147], v[72:79])
  k += ds_load_b128(v[189:192], v[139], offset0=0)
  k += v_wmma_f32_16x16x16_f16(v[80:87], v[213:216], v[148:151], v[80:87])
  k += ds_load_b128(v[193:196], v[139], offset0=64)
  k += v_wmma_f32_16x16x16_f16(v[88:95], v[213:216], v[152:155], v[88:95])
  k += ds_load_b128(v[197:200], v[139], offset0=128)
  k += v_wmma_f32_16x16x16_f16(v[96:103], v[217:220], v[140:143], v[96:103])
  k += ds_load_b128(v[201:204], v[139], offset0=192)
  k += v_wmma_f32_16x16x16_f16(v[104:111], v[217:220], v[144:147], v[104:111])
  k += v_wmma_f32_16x16x16_f16(v[112:119], v[217:220], v[148:151], v[112:119])
  k += v_wmma_f32_16x16x16_f16(v[120:127], v[217:220], v[152:155], v[120:127])
  k += "final_wmma:"
  k += s_wait_dscnt(0x3)
  k += v_perm_b32(v[140], v[158], v[156], s[66])
  k += v_perm_b32(v[141], v[162], v[160], s[66])
  k += v_perm_b32(v[142], v[166], v[164], s[66])
  k += v_perm_b32(v[143], v[170], v[168], s[66])
  k += v_perm_b32(v[144], v[158], v[156], s[67])
  k += v_perm_b32(v[145], v[162], v[160], s[67])
  k += v_wmma_f32_16x16x16_f16(v[0:7], v[189:192], v[140:143], v[0:7])
  k += ds_load_b64(v[172:173], v[138], offset1=16)
  k += ds_load_b64(v[174:175], v[138], offset1=17)
  k += v_perm_b32(v[146], v[166], v[164], s[67])
  k += v_perm_b32(v[147], v[170], v[168], s[67])
  k += v_perm_b32(v[148], v[159], v[157], s[66])
  k += v_perm_b32(v[149], v[163], v[161], s[66])
  k += v_wmma_f32_16x16x16_f16(v[8:15], v[189:192], v[144:147], v[8:15])
  k += ds_load_b64(v[176:177], v[138], offset1=18)
  k += ds_load_b64(v[178:179], v[138], offset1=19)
  k += v_perm_b32(v[150], v[167], v[165], s[66])
  k += v_perm_b32(v[151], v[171], v[169], s[66])
  k += v_perm_b32(v[152], v[159], v[157], s[67])
  k += v_perm_b32(v[153], v[163], v[161], s[67])
  k += v_wmma_f32_16x16x16_f16(v[16:23], v[189:192], v[148:151], v[16:23])
  k += ds_load_b64(v[180:181], v[138], offset1=20)
  k += ds_load_b64(v[182:183], v[138], offset1=21)
  k += v_perm_b32(v[154], v[167], v[165], s[67])
  k += v_perm_b32(v[155], v[171], v[169], s[67])
  k += s_nop(1)
  k += v_wmma_f32_16x16x16_f16(v[24:31], v[189:192], v[152:155], v[24:31])
  k += ds_load_b64(v[184:185], v[138], offset1=22)
  k += ds_load_b64(v[186:187], v[138], offset1=23)
  k += s_wait_dscnt(0x8)
  k += v_wmma_f32_16x16x16_f16(v[32:39], v[193:196], v[140:143], v[32:39])
  k += ds_load_b128(v[205:208], v[139], offset0=32)
  k += v_wmma_f32_16x16x16_f16(v[40:47], v[193:196], v[144:147], v[40:47])
  k += ds_load_b128(v[209:212], v[139], offset0=96)
  k += v_wmma_f32_16x16x16_f16(v[48:55], v[193:196], v[148:151], v[48:55])
  k += ds_load_b128(v[213:216], v[139], offset0=160)
  k += v_wmma_f32_16x16x16_f16(v[56:63], v[193:196], v[152:155], v[56:63])
  k += ds_load_b128(v[217:220], v[139], offset0=224)
  k += v_wmma_f32_16x16x16_f16(v[64:71], v[197:200], v[140:143], v[64:71])
  k += v_wmma_f32_16x16x16_f16(v[72:79], v[197:200], v[144:147], v[72:79])
  k += v_wmma_f32_16x16x16_f16(v[80:87], v[197:200], v[148:151], v[80:87])
  k += v_wmma_f32_16x16x16_f16(v[88:95], v[197:200], v[152:155], v[88:95])
  k += v_wmma_f32_16x16x16_f16(v[96:103], v[201:204], v[140:143], v[96:103])
  k += v_wmma_f32_16x16x16_f16(v[104:111], v[201:204], v[144:147], v[104:111])
  k += s_wait_dscnt(0x0)
  k += barrier_signal()
  k += s_barrier_wait(0xffff)
  k += v_wmma_f32_16x16x16_f16(v[112:119], v[201:204], v[148:151], v[112:119])
  k += v_wmma_f32_16x16x16_f16(v[120:127], v[201:204], v[152:155], v[120:127])
  k += s_wait_dscnt(0x0)
  k += v_perm_b32(v[140], v[174], v[172], s[66])
  k += v_perm_b32(v[141], v[178], v[176], s[66])
  k += v_perm_b32(v[142], v[182], v[180], s[66])
  k += v_perm_b32(v[143], v[186], v[184], s[66])
  k += v_perm_b32(v[144], v[174], v[172], s[67])
  k += v_perm_b32(v[145], v[178], v[176], s[67])
  k += v_wmma_f32_16x16x16_f16(v[0:7], v[205:208], v[140:143], v[0:7])
  k += v_perm_b32(v[146], v[182], v[180], s[67])
  k += v_perm_b32(v[147], v[186], v[184], s[67])
  k += v_perm_b32(v[148], v[175], v[173], s[66])
  k += v_perm_b32(v[149], v[179], v[177], s[66])
  k += v_wmma_f32_16x16x16_f16(v[8:15], v[205:208], v[144:147], v[8:15])
  k += v_perm_b32(v[150], v[183], v[181], s[66])
  k += v_perm_b32(v[151], v[187], v[185], s[66])
  k += v_perm_b32(v[152], v[175], v[173], s[67])
  k += v_perm_b32(v[153], v[179], v[177], s[67])
  k += v_wmma_f32_16x16x16_f16(v[16:23], v[205:208], v[148:151], v[16:23])
  k += v_perm_b32(v[154], v[183], v[181], s[67])
  k += v_perm_b32(v[155], v[187], v[185], s[67])
  k += s_nop(1)
  k += v_wmma_f32_16x16x16_f16(v[24:31], v[205:208], v[152:155], v[24:31])
  k += v_wmma_f32_16x16x16_f16(v[32:39], v[209:212], v[140:143], v[32:39])
  k += v_wmma_f32_16x16x16_f16(v[40:47], v[209:212], v[144:147], v[40:47])
  k += v_wmma_f32_16x16x16_f16(v[48:55], v[209:212], v[148:151], v[48:55])
  k += v_wmma_f32_16x16x16_f16(v[56:63], v[209:212], v[152:155], v[56:63])
  k += v_wmma_f32_16x16x16_f16(v[64:71], v[213:216], v[140:143], v[64:71])
  k += v_wmma_f32_16x16x16_f16(v[72:79], v[213:216], v[144:147], v[72:79])
  k += v_wmma_f32_16x16x16_f16(v[80:87], v[213:216], v[148:151], v[80:87])
  k += v_wmma_f32_16x16x16_f16(v[88:95], v[213:216], v[152:155], v[88:95])
  k += v_wmma_f32_16x16x16_f16(v[96:103], v[217:220], v[140:143], v[96:103])
  k += v_wmma_f32_16x16x16_f16(v[104:111], v[217:220], v[144:147], v[104:111])
  k += v_wmma_f32_16x16x16_f16(v[112:119], v[217:220], v[148:151], v[112:119])
  k += v_wmma_f32_16x16x16_f16(v[120:127], v[217:220], v[152:155], v[120:127])
  k += s_load_b256(sdata=s[48:55], sbase=s[0:1], ioffset=0x58, soffset=RawImm(124))
  k += s_load_b32(sdata=s[56], sbase=s[0:1], ioffset=0x78, soffset=RawImm(124))
  # calculate address
  k += v_lshrrev_b32_e32(v[132], 5, v[254])
  k += v_lshrrev_b32_e32(v[133], 1, v[132])
  k += v_mul_lo_u32(v[133], 16, v[133])
  k += v_and_b32_e32(v[129], 31, v[254])
  k += v_lshrrev_b32_e32(v[129], 4, v[129])
  k += v_lshlrev_b32_e32(v[129], 3, v[129])
  k += v_add_lshl_u32(v[129], v[133], v[129], 2)
  k += v_mul_lo_u32(v[130], v[129], s[38])
  k += v_mul_lo_u32(v[131], v[129], s[36])
  k += v_and_b32_e32(v[128], 1, v[132])
  k += v_mul_lo_u32(v[128], 16, v[128])
  k += v_and_b32_e32(v[133], 15, v[254])
  k += s_mul_i32(s[8], 0x80, s[2])
  k += v_add_lshl_u32(v[128], v[133], v[128], 2)
  k += v_add_nc_u32_e32(v[128], s[8], v[128])
  k += v_add_lshl_u32(v[137], v[131], v[128], 1)
  k += barrier_signal()
  k += s_barrier_wait(0xffff)
  # final stores
  ptr, tmpv = 16, 140
  vaddr = v[137]
  bases = (0, 32, 64, 96)
  row_pairs = ((0, 1), (2, 3), (4, 5), (6, 7))
  first = True
  for a, b in row_pairs:
    k += [s_nop(0), s_wait_dscnt(0x0)]
    # even row then odd row, for each base
    for r in (a, b):
      for base in bases:
        # advance the ptr SGPR
        if not first: k += [s_add_co_u32(s[ptr], s[ptr], N << 1), s_add_co_ci_u32(s[ptr+1], s[ptr+1], 0)]
        first = False
        # pack 4 fp16s
        acc4 = [v[base+r], v[base+r+8], v[base+r+16], v[base+r+24]]
        k += v_cvt_f16_f32_e64(acc4[0], acc4[0])
        k += v_cvt_f16_f32_e64(acc4[1], acc4[1])
        k += v_pack_b32_f16(v[tmpv+0], acc4[0], acc4[1])
        k += v_cvt_f16_f32_e64(acc4[2], acc4[2])
        k += v_cvt_f16_f32_e64(acc4[3], acc4[3])
        k += v_pack_b32_f16(v[tmpv+1], acc4[2], acc4[3])
        k += buffer_store_b64(v[tmpv:tmpv+1], vaddr, ptr)
  k += s_endpgm()
  sink = UOp.sink(A, B, C, lidx, gidx, arg=KernelInfo(name="gemm"))
  return UOp(Ops.PROGRAM, src=(sink, UOp(Ops.DEVICE, arg=dev), UOp(Ops.LINEAR, src=(*sink.src, sink)), UOp(Ops.SOURCE, arg=str(k))), arg=())

if __name__ == "__main__":
  test_matmul(custom_gemm(Device.DEFAULT), dtype=dtypes.half, N=N)
